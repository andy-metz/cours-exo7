
\input{../preamb-texte.tex}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\begin{document}

\debuttexte


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\diapo

\change

On continue notre étude des applications linéaires avec cette fois des exemples.

\change

Des exemples géométriques pour commencer

\change

Puis des exemples variés provenant de tou[s] horizon.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\diapo


Soit $E$ un $\Kk$-espace vectoriel. 
On définit l'application  
$f$ par :
$$\begin{array}{rcl}
f : E & \to & E \\
u & \mapsto & - u
  \end{array}$$
$f$ est une application linéaire et s'appelle la \defi{symétrie centrale} 
  par rapport à l'origine $0_E$.

  
\change


Soit toujours $E$ un $\Kk$-espace vectoriel et soit $\lambda$ un scalaire.

On définit l'application  
$f_{\lambda}$ par :
$$\begin{array}{rcl}
f_{\lambda} : E & \to & E \\
u & \mapsto & \lambda u
  \end{array}$$
$f_{\lambda}$ est une application linéaire. 
$f_{\lambda}$ est appelée \defi{homothétie} de rapport $\lambda$.


Il y a quelques cas particuliers notables :
\begin{itemize}
  \item $\lambda = 1$, $f_{\lambda}$ est l'application identité ;
  \item $\lambda = 0$, $f_{\lambda}$ est l'application nulle ;
  \item $\lambda = -1$, on retrouve la symétrie centrale.
\end{itemize}

\change

Voici la preuve que $f_{\lambda}$ est une application linéaire :
$$
f_{\lambda}(\alpha u + \beta v) 
= \lambda (\alpha u + \beta v) 
= \alpha (\lambda u)+ \beta (\lambda v) 
= \alpha f_{\lambda}(u) +\beta f_{\lambda}(v).
$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\diapo

Voyons un autre exemple d'application linéaire : les projections.

\change


On part d'un  espace vectoriel $E$

et on suppose que $F$ et $G$ sont deux sous-espaces
vectoriels supplémentaires dans $E$, c'est-à-dire 
$F \oplus G = E$.

\change

Que $F$ et $G$ soient en somme directe implique que tout vecteur $u$ de $E$ 
se décompose de façon unique  $u=v+w$ avec $v \in F$ et $w \in G$. 

\change

La \defi{projection} sur $F$ parallèlement à $G$ est par définition l'application $p : E \to E$
définie par $p(u)=v$.  

[Sur la figure]

A un vecteur $u$ on le projette parallèlement à $G$ pour lui associer
ce vecteur $v$ de $F$.



\change

Une projection est un exemple important d'application linéaire. 

Cela se prouve simplement grâce à l'unicité de la décomposition.

\change

En plus une projection $p$ vérifie l'égalité $p^2=p$. 
C'est même une caractérisation des projections.

$p^2=p$ signifie $p\circ p = p$, c'est-à-dire pour tout vecteur $u$ :
$p\big(p(u)\big) = p(u)$.

Il s'agit juste de remarquer que si $v \in F$ alors $p(v) = v$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\diapo

Reprenons l'exemple des sous-espaces vectoriels 
$$F=\big\{ (x,y,z) \in \Rr^3\mid x-y-z=0\big\} \qquad  \text{ et } \qquad 
G=\big\{(x,y,z) \in \Rr^3 \mid y=z=0 \big\}$$
vu auparavant.


\change

Nous avons vu que les sous-espaces vectoriels $F$ et $G$ 
sont supplémentaires dans $\Rr^3$ : $ F \oplus G = \Rr^3$.

[Sur la figure]
Calculons la projection d'un vecteur $(x,y,z)$ sur $F$ parallèlement à $G$.

\change

Nous avions vu que la décomposition s'écrivait :
$(x,y,z)=$ un élément de $F$ + un élément de $G$.

\change


Si on note $p$ la projection sur $F$ parallèlement à $G$, alors  
on a $p(x,y,z)=(y+z,y,z)$. 

[[FIG ??]]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\diapo

Passons à un exemple de projections dans l'espace vectoriel des fonctions de $\Rr$ dans $\Rr$.

Nous avions vu  que l'ensemble des fonctions paires $\mathcal{P}$ et l'ensemble des fonctions 
impaires $\mathcal{I}$ 
 
\change
 
sont des sous-espaces vectoriels supplémentaires dans $\mathcal{F}(\Rr ,\Rr)$.

\change

Notons $p$ la projection sur $\mathcal{P}$ parallèlement à $\mathcal{I}$. 

\change

Pour une fonction quelconque $f$ 

alors $p(f)=g$ où $g$ est la fonction (paire) définie par $g(x)=\frac{f(x)+f(-x)}{2}$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\diapo

Voyons quelques opérations que vous connaissez bien et qui sont en fait des applications linéaires.

Tout d'abord la \textbf{dérivation}.

Soient $E  = \mathcal{C}^1 (\Rr,\Rr)$ l'espace vectoriel des fonctions 
$f$ dérivables avec $f'$ continue et 
$F = \mathcal{C}^0 (\Rr,\Rr)$ l'espace vectoriel des fonctions continues.
Soit 
$$\begin{array}{rcl}
d : \mathcal{C}^1 (\Rr,\Rr) & \longrightarrow & \mathcal{C}^0 (\Rr,\Rr)  \\
f & \longmapsto & f'     
  \end{array}$$
Alors $d$, l'opération de dérivation, est une application linéaire.


En effet on sait que $(\lambda f + \mu g)' = \lambda f' + \mu g'$ ce qui est exactement dire
$d(\lambda f + \mu g)=\lambda d(f) + \mu d(g)$.

\change

C'est pareil pour l'intégration, cette fois 
 $E = \mathcal{C}^0(\Rr,\Rr)$ et $F = \mathcal{C}^1(\Rr,\Rr)$. 

et $I$ est l'opération qui à une fonction $f$ associe
la primitive $\int_0^x f(t) \; dt$.


L'application $I$ est linéaire car 
$\int_0^x \big(\lambda f(t) + \mu g(t)\big) \; dt 
= \lambda \int_0^x f(t) \; dt + \mu \int_0^x g(t) \; dt$  
pour toutes fonctions $f$ et $g$ et pour tous $\lambda,\mu \in \Rr$.
 
La propriété que l'on vient d'utiliser s'appelle justement la linéarité de l'intégrale !

\change

Voyons un exemple avec les polynômes.



Soit $E=\Rr_n[X]$ l'espace vectoriel des polynômes de degré $\le n$.
Soit $F = \Rr_{n+1}[X]$ et soit 
$$\begin{array}{rcl}
f : \quad E & \longrightarrow & F \\
P & \longmapsto & X P
  \end{array}$$
  
Autrement dit, si $P = a_n X^n + \dots + a_1 X + a_0$,
alors $f(P) =  a_n X^{n+1} + \dots + a_1 X^2 + a_0 X$.

C'est une application linéaire : car on vérifie facilement que 
$f(\lambda P+ \mu Q) = \lambda X P + \mu X Q = \lambda f(P) + \mu f(Q)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\diapo

On continue notre liste non exhaustive d'exemples.


La \textbf{transposition}, qui a une matrice associe sa transposée est une application linéaire
de l'espace vectoriel des matrices dans lui même.

En effet la transposition vérifie que 
$$(\lambda A + \mu B)^{T}=\lambda A^{T} + \mu B^{T}.$$

\change


Il en va de même pour la trace. 

Je vous rappelle que la trace d'une matrice est la somme des éléments sur sa diagonale.

Comme $tr(\lambda A + \mu B) = \lambda tr A + \mu tr B$
alors la trace est une application linéaire de l'espace vectoriel des matrices vers l'espace vectoriel $\Rr$.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\diapo

Il vous reste encore beaucoup de travail pour bien comprendre les applications linéaires.

Commencez par ces exercices !



\end{document}
