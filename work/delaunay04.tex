
\documentclass[class=report,crop=false]{standalone}
\usepackage[screen]{../exo7book}

\begin{document}

%====================================================================
\chapitre{Algèbre linéaire -- 2ème année -- Sandra Delaunay}
%====================================================================


\tableofcontents

Dans tout ce cours, le corps $K$ désignera soit le corps $\Rr$ des réels, 
soit le corps $\Cc$ des complexes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Rappels d'algèbre linéaire}

Nous allons commencer par quelques rappels d'algèbre linéaire 
indispensables, sans démonstrations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Espaces vectoriels}


Un espace vectoriel est un ensemble stable par combinaison linéaire.

\begin{definition}
Un ensemble $E$ muni d'une opération interne notée $+$ 
et d'une opération externe est un espace vectoriel sur un corps $K$, 
s'il vérifie les propriétés suivantes :
\begin{enumerate}
  \item L'ensemble $(E,+)$ est un groupe commutatif :
  \begin{itemize}
    \item l'opération $+$ est associative et commutative,
    \item il existe un élément neutre noté $0_E$ qui vérifie pour tout $u\in E$, $u+0_E=u$,    \item
    \item tout élément de $E$ admet un symétrique (ou opposé), pour tout $u\in E$, il existe $v\in E$ tel que $u+v=0_E$ on notera cet opposé
$-u$.
  \end{itemize}

  \item Pour tout $u,v\in E$ et pour tout $\lambda,\mu\in K$, on a
  \begin{itemize}
    \item $1_K . u=u$,
    \item $\lambda.(u+v)=\lambda.u+\lambda.v$,
    \item $(\lambda+\mu).u=\lambda.u+\mu.u$,
    \item $\lambda.(\mu.v)=(\lambda\mu).v.$
  \end{itemize}  
\end{enumerate}


Les éléments de $E$ sont appelés \defi{vecteurs}, 
les éléments de $K$ sont appelés \defi{scalaires}.
  
\end{definition}


[[vérifier ordre avec exo7]]

\begin{definition}
Un sous-ensemble non vide $E'$ de $E$ est un 
\defi{sous-espace vectoriel} de $E$, 
s'il est stable par combinaisons linéaires.
[[cad...]] 
\end{definition}

\begin{exemple}
\begin{itemize}
  \item Le corps $\Cc$ des nombres complexes est un espace 
  vectoriel sur $\Cc$ et sur $\Rr$,  
  l'ensemble $\Rr^n$, $n\in\Nn$, est un espace vectoriel sur $\Rr$.
  
  \item $\Rr^2$ est un sous-espace vectoriel de $\Rr^3$. 
  
\end{itemize} 
\end{exemple}

[[ex1 : bof]]

\begin{proposition}
Soient $E_1,\dots,E_p$ des sous-espaces vectoriels de $E$, l'ensemble 
$$F=E_1+\dots+E_p=\{u_1+\dots+u_p,\ u_1\in E_1,\dots,x_p\in E_p\}$$
somme des sous-espaces vectoriels $E_1,\dots,E_p$, est un sous-espace vectoriel de $E$. 
C'est le plus petit sous-espace vectoriel de $E$ contenant la réunion $E_1\cup\dots\cup E_p$. 
\end{proposition}

\begin{definition}
Deux sous-espaces vectoriels, $E_1$ et $E_2$, de $E$ sont dits 
\defi{supplémentaires} si $E_1\cap E_2=\{0\}$ et $E=E_1+E_2$, 
on note alors $E=E_1\oplus E_2$ et tout vecteur de $E$ 
se décompose de manière unique en somme d'un vecteur de $E_1$ 
et d'un vecteur de $E_2$.
\end{definition}

[[somme directe]]

%---------------------------------------------------------------
\subsection{Familles libres, génératrices. Bases}


\begin{definition}
\begin{itemize}
  \item Soient $(u_1,u_2,\dots,u_n)$ des vecteurs de $E$, 
  on dit que la famille $(u_1,u_2,\dots,u_n)$ \defi{engendre} 
  l'espace vectoriel $E$ si tout vecteur de $E$ s'écrit comme 
  combinaison linéaire des vecteurs $(u_1,u_2,\dots,u_n)$, 
  c'est-à-dire si, pour tout $v\in E$, il existe des scalaires 
  $(\lambda_1,\lambda_2,\dots,\lambda_n)$ tel que 
  $v=\lambda_1u_1+\lambda_2u_2+\dots+\lambda_nu_n.$

  
  \item Soient $(u_1,u_2,\dots,u_n)$ des vecteurs de $E$, 
  on dit que la famille $(u_1,u_2,\dots,u_n)$ est \defi{libre}, 
  si pour tout $(\lambda_1,\dots,\lambda_n)\in K^n$, 
  l'égalité $\lambda_1u_1+\lambda_2u_2+\dots+\lambda_nu_n=0$ 
  implique $\lambda_1=\lambda_2=\dots=\lambda_n=0$.

  
  \item Une famille de vecteurs, génératrice et libre, 
  est appelée une \defi{base} de $E$. Si $E$ admet une base 
  de $n$ vecteurs, toutes les bases de $E$ ont le même nombre, 
  $n$, de vecteurs et $n$ est la \defi{dimension} de $E$.
\end{itemize} 
\end{definition}

[[génératrice]]

Dans ce cours, tous les espaces vectoriels seront de dimension finie.

Si $(u_1,u_2,\dots,u_n)$ est une base de $E$, alors 
pour tout $v\in E$, il existe un unique $n$-uplet 
$(\lambda_1,\dots,\lambda_n)\in K^n$ tel que
$$v=\lambda_1u_1+\lambda_2u_2+\dots+\lambda_nu_n.$$

[[coordonnées]]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications linéaires}

\begin{definition}
Soient $E$ et $F$ deux espaces vectoriels, 
une application $f: E\rightarrow F$ est dite $K$-linéaire ou 
\defi{linéaire} si pour tout $(u,v)\in E^2$ et 
pour tout $(\lambda,\mu)\in K^2$, 
$$f(\lambda. u+\mu. v)=\lambda. f(u)+\mu. f(v).$$  
\end{definition}

\begin{definition}
\begin{itemize}
  \item Les applications linéaires sont encore appelées \defi{homomorphismes} 
  d'espaces vectoriels, on note $\mathcal{L}(E,F)$ 
  l'ensemble des applications linéaires de $E$ dans $F$.
  
  \item Les applications linéaires de $E$ dans lui-même sont appelées 
  \defi{endomorphismes} de $E$, on note $\mathcal{L}(E)$ leur ensemble.
 
  \item Les applications linéaires \emph{bijectives} de $E$ dans $F$ sont appelées \defi{isomorphismes}.
 
  \item Les applications linéaires \emph{bijectives} de $E$ dans lui-même sont les \defi{automorphismes} de $E$.

  \item Les applications linéaires de $E$ dans le corps de base, $K$, sont appelées \defi{formes linéaires} de $E$.
\end{itemize}
  
\end{definition}

[[virer 1, verfier notation $\mathcal{L}$]]

On définit sur $\mathcal{L}(E,F)$ une addition, pour $f$ et $g\in\mathcal{L}(E,F)$ et pour $u\in E$, par
$$(f+g)(u)=f(u)+g(u),$$
et une multiplication externe, pour $\lambda\in K$, par
$$(\lambda f)(u)=\lambda.f(u).$$



Nous allons maintenant rappeler quelques propositions évidentes.

\begin{proposition}
L'ensemble $\mathcal{L}(E,F)$ muni de ces deux opérations a une structure d'espace vectoriel.
\end{proposition} 

Par ailleurs, la composée de deux applications linéaires est encore une application linéaire.

\begin{proposition}
Soient $E,F,G$ trois espaces vectoriels sur un corps $K$, soient 
$f\in \mathcal{L}(E,F)$ et $g\in \mathcal{L}(F,G)$, alors $f\circ g\in\mathcal{L}(E,G)$.
\end{proposition} 


\begin{proposition}
Soit $E$ un espace vectoriel, l'ensemble $\mathcal{L}(E)$ muni de la somme et de 
la composition des applications est un anneau.
\end{proposition}


\begin{proposition}
Si $f$ est une application linéaire bijective de $E$ dans $F$, 
alors, son application inverse $f^{-1}$ est une application linéaire 
bijective de $F$ dans $E$, c'est-à-dire un isomorphisme entre $E$ et $F$. 
Si une telle application existe, on dit que les espaces vectoriels $E$ et $F$ 
sont isomorphes. Ils sont alors de même dimension.
\end{proposition} 


%---------------------------------------------------------------
\subsection{Noyau, image}

[[proposition-définition]]

\begin{proposition}
Soient $E$ et $F$ deux espaces vectoriel et $f\in\mathcal{L}(E,F)$. 
Notons $E'$ un sous-espace vectoriel de $E$ et $F'$ un sous-espace vectoriel de $F$.

\begin{enumerate}
  \item L'ensemble $f(E')$ est un sous-espace vectoriel de $F$. En particulier l'ensemble suivant appelé image de $f$ :
$$\Im f=f(E)=\{f(u),\ u\in E\}=\{y\in F,\ \exists u\in E,\ v=f(u)\}.$$
L'application $f$ est surjective si et seulement si $\Im f=F$.

  \item L'ensemble $f^{-1}(F')=\{u\in E,\ f(u)\in F'\}$ est un sous-espace vectoriel de $E$. En particulier l'ensemble suivant appelé noyau de $f$ :
$$\Ker f=\{u\in E,\ f(u)=0_F\}.$$
L'application $f$ est injective si et seulement si $\Ker f=\{0_E\}$.
  
\end{enumerate}

  
\end{proposition}

\begin{theoreme}[Théorème du rang]
Soient $E$ et $F$ deux espaces vectoriels (de dimensions finies) et $f\in\mathcal{L}(E,F)$, on a
$$\dim E=\dim \Ker f+\dim \Im f.$$  
\end{theoreme}

\begin{proof}
Supposons $E$ de dimension $n$, et notons $m$ la dimension du sous-espace vectoriel $\Ker f$ de $E$. Soit $(e_1,e_2,\dots,e_m)$ une base de $\Ker f$, on la complète par des vecteurs $e_{m+1},\dots,e_n$ en une base de $E$. Les vecteurs $f(e_1),\dots,f(e_n)$ engendrent $\Im f$, mais comme les vecteurs $e_1,\dots,e_m$ sont dans le noyau, le sous-espace $\Im f$ est engendré par les vecteurs $f(e_{m+1}),\dots,f(e_n)$. Montrons que ces vecteurs forment une base de $\Im f$, pour cela il suffit de démontrer qu'ils sont linéairement indépendants.

Supposons qu'il existe des scalaires $\lambda_{m+1},\dots, \lambda_n$ tels que 
$\lambda_{m+1}f(e_{m+1})+\dots+\lambda_nf(e_n)=0,$ on a alors 
$$f(\lambda_{m+1}e_{m+1}+\dots+\lambda_ne_n)=0,$$
c'est-à-dire $\lambda_{m+1}e_{m+1}+\dots+\lambda_ne_n\in \Ker f$, par conséquent, il existe des scalaires $\mu_1,\dots,\mu_m$ tels que 
$$\lambda_{m+1}e_{m+1}+\dots+\lambda_ne_n=\mu_1e_1+\dots+\mu_me_m.$$
Mais, comme les vecteurs $e_1,\dots,e_m,e_{m+1},\dots,e_n$ forment une base de $E$, ceci implique la nullité de tous les coefficients. Ainsi les vecteurs $f(e_{m+1}),\dots,f(e_n)$ forment bien une base de $\Im f$. Le sous-espace vectoriel de $\Im f$ de $F$ est donc de dimension $n-m$ et, on a bien,
$$n=\dim E=m+(n-m)=\dim \Ker f+\dim \Im f.$$  
\end{proof}

   


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrices}


Nous allons maintenant, avant d'aborder les déterminants, terminer ce 
chapitre de rappels par quelques mots sur les matrices.


\begin{definition}
On appelle \defi{matrice} à $n$ lignes et $m$ colonnes, à coefficients 
 dans $K$, le tableau suivant :
$$M=(a_{ij})_{1\leq i\leq n,1\leq j\leq m}=\left(\begin{matrix}a_{11}&a_{12}&\ldots&a_{1m}\cr
a_{21}&a_{22}&\ldots&a_{2m}\cr\vdots&\vdots& &\vdots\cr a_{n1}&a_{n2}&\ldots&a_{nm}\end{matrix}\right)$$ 
On note $M_{n,m}(K)$, l'ensemble de ces matrices et $M_n(K)$ cet ensemble si $m=n$.  
\end{definition}


On définit sur $M_{n,m}(K)$ une addition, composante par composante et une 
multiplication externe par un élément de $K$ sur chaque composante. 
Muni de ces deux opérations, l'ensemble $M_{n,m}(K)$ a une structure d'espace vectoriel.

\begin{definition}
Soit $M\in M_n(K)$ une matrice carrée. La matrice $M=(a_{ij})_{1\leq i,j\leq n}$ est dite  
\begin{itemize}
  \item triangulaire supérieure si $a_{ij}=0$ pour tout $(i,j)$ tel que $i>j$,
  \item triangulaire inférieure si $a_{ij}=0$ pour tout $(i,j)$ tel que $i<j$,
  \item diagonale si $a_{ij}=0$ pour tout $(i,j)$ tel que $i\neq j$,
  \item symétrique si $a_{ij}=a_{ji}$ pour tout $(i,j)$.
\end{itemize}
  
\end{definition}

\begin{definition}
Soit $M=(a_{ij})_{1\leq i\leq n,1\leq j\leq m}\in M_{n,m}(K)$, on 
appelle transposée de $M$, la matrice $M^T=(b_{ij})_{1\leq i\leq n,1\leq j\leq m}$ 
où $b_{ij}=a_{ji}$ pour tout $(i,j)$. 
\end{definition}

[[changement notation $M^T$]]

%---------------------------------------------------------------
\subsection{Produit de matrices}


On définit également un \defi{produit} de matrices pour 
$A=(a_{ik})_{1\leq i\leq n,1\leq k\leq m}\in M_{n,m}(K)$ et 
$B=(b_{kj})_{1\leq k\leq m,1\leq j\leq p}\in M_{m,p}(K)$ par 
$$A.B=(c_{ij})_{1\leq i\leq n,1\leq j\leq p}\in M_{n,p}(K),\ {\hbox{où}}\ c_{ij}=\sum_{k=1}^n a_{ik}b_{kj}.$$
A la place $(ij)$, le coefficient $c_{ij}$ est le produit scalaire de la $i$-ème ligne de la matrice $A$ et de la $j$-ème colonne de la matrice $B$.
Remarquons que ce produit n'est possible que si le nombre de colonnes de la matrice $A$ est égal au nombre de lignes de la matrice $B$ et qu'il n'est évidemment pas commutatif, même dans le cas des matrices carrées.


\begin{definition}
Une matrice $A\in M_n(K)$ est dite \defi{inversible} 
s'il existe une matrice $B\in M_n(K)$ telle que $A.B=B.A=I_n$ 
où $I_n$ désigne la matrice diagonale telle que $a_{ii}=1$ pour tout $i$. 
La matrice $I_n$ est appelée \defi{matrice identité}, elle est élément neutre 
pour le produit des matrices.  
\end{definition}
 

\begin{proposition}
L'ensemble $M_n(K)$ muni de l'addition et du produit des matrices est un anneau (non commutatif). 
L'ensemble des matrices inversibles de $M_n(K)$ est un groupe pour le produit des matrices, 
c'est le groupe linéaire noté $GL_n(K)$.
\end{proposition} 


Nous allons maintenant relier matrices et applications linéaires.

Soient $E$ et $F$ deux espaces vectoriels, on note $m$ la dimension de $E$ et $n$ la dimension de $F$. Soient $\mathcal{B}=(e_1,\dots,e_m)$ une base de $E$ et $\mathcal{B}'=(f_1,\dots,f_n)$ une base de $F$. Soit $\varphi\in \mathcal{L}(E,F)$.

\begin{definition}
On appelle \defi{matrice de $\varphi$} dans les 
bases $\mathcal{B}$ et $\mathcal{B}'$, la matrice dont les colonnes 
sont les coordonnées des vecteurs $\varphi(e_j)_{1\leq j\leq m}$ exprimées 
dans la base $\mathcal{B}'$.
\end{definition}


Si pour tout $j$, $1\leq j\leq m$, on a $$\varphi(e_j)=\sum_{i=1}^n a_{ij}f_i,$$
la matrice de $\varphi$, relativement aux bases $\mathcal{B}$ et $\mathcal{B}'$ 
est la matrice $A=(a_{ij})_{1\leq i\leq n,1\leq j\leq m}$. 

\begin{proposition}
L'application de $\mathcal{L}(E,F)$ dans $M_{n,m}(K)$ qui à 
une application linéaire $\varphi$ associe sa matrice dans des bases données de $E$ et $F$ est 
un isomorphisme d'espaces vectoriels.
\end{proposition} 

Ainsi, la matrice d'une application linéaire dépend des bases dans 
lesquelles elle est exprimée. Néanmoins, certaines propriétés de 
l'application linéaire, et par là de sa matrice, sont conservées 
quelques soient les bases dans lesquelles on exprime cette matrice. 
C'est là un point essentiel puisque, dans la suite de ce cours, 
il s'agira de déterminer des bases dans lesquelles la matrice 
d'une application linéaire donnée soit la plus simple possible, 
triangulaire ou diagonale par exemple. C'est pourquoi, il faut 
bien comprendre les changements de bases et leurs matrices.


%---------------------------------------------------------------
\subsection{Changements de bases}


Soit $E$ un espace vectoriel de dimension $n$ et soient 
$\mathcal{B}=(e_1,\dots,e_n)$ et $\mathcal{B}'=(e'_1,\dots,e'_n)$ 
deux bases de $E$. Pour $j$, $1\leq j\leq n$, on note $(a_{ij})_{1\leq i\leq n}$ 
les coordonnées du vecteur $e'j$ dans la base $\mathcal{B}$, on a
$$e'_j=\sum_{i=1}^n a_{ij}e_i.$$

[[Définition et propriétés.]]
 
\begin{proposition}
La matrice $P=(a_{ij})_{1\leq i\leq n}$ est appelée 
\defi{matrice de passage} de la base $\mathcal{B}$ à la base $\mathcal{B}'$. 
Elle est inversible et la matrice de passage de la base $\mathcal{B}'$ 
à la base $\mathcal{B}$  est la matrice $P^{-1}$.
\end{proposition}

 
Si $u$ est un vecteur de $E$, on peut exprimer ses coordonnées dans la base $\mathcal{B}$ ou dans la base $\mathcal{B}'$, on note 
$$u=\sum_{i=1}^n x_ie_i=\sum_{i=1}^n x'_ie'_i.$$
Si on note 
$$X=\left(\begin{matrix}x_1\cr\vdots\cr x_n\end{matrix}\right)\quad {\hbox{et}}\quad X'=\left(\begin{matrix}x'_1\cr\vdots\cr x'_n\end{matrix}\right),$$
on a 
$$X=P.X'.$$

\begin{proposition}
Soient $E$ un espace vectoriel, $\mathcal{B}=(e_1,\dots,e_n)$ et 
$\mathcal{B}'=(e'_1,\dots,e'_n)$ deux bases de $E$ . Si $\varphi\in\mathcal{L}(E)$, 
notons $M$ la matrice de $\varphi$ dans la base $\mathcal{B}$ et $M'$ la matrice de 
$\varphi$ dans la base $\mathcal{B}'$, alors on a
$$M'=P^{-1}.M.P.$$
On dit alors que les matrices $M$ et $M'$ sont \defi{semblables}.
\end{proposition}   



Avant d'aborder les déterminants, nous allons tenter d'illustrer ce qui 
précède par quelques exemples en dimension $2$ et $3$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exemples : homothéties, projections, symétries}

%---------------------------------------------------------------
\subsection{Homothéties vectorielles}


Soit $E$ un espace vectoriel et $\lambda\in K$, on note $h_\lambda$ l'application :
$$\begin{array}{cc}h_\lambda : E &\longrightarrow E\cr u & \longmapsto \lambda.u\end{array}$$
Cette application est linéaire et bijective, on a $h_\lambda=\lambda\id_E$, $\Ker h_\lambda=\{0_E\}$ et $\Im h_\lambda=E$. Quelque soit le base choisie, la matrice de $h_\lambda$ s'écrit
$$M_\lambda=\left(\begin{matrix}\lambda&0&\dots&0\cr 0&\lambda& &\vdots\cr\vdots& &\ddots&0\cr0&\dots&\dots&\lambda\end{matrix}\right).$$

%---------------------------------------------------------------
\subsection{Projections vectorielles} 



[[Définition et propriétés]]

\begin{proposition}
Soient $E_1$ et $E_2$ deux sous-espaces supplémentaires d'un espace vectoriel $E$. 
Tout vecteur $u$ de $E$ s'écrit de manière unique $u=u_1+u_2$ avec 
$u_1\in E_1$ et $u_2\in E_2$, on appelle \defi{projection vectorielle} (ou \defi{projecteur}) 
de $E$ sur $E_1$ parallèlement à $E_2$ l'application
$$\begin{array}{cc}p:E&\longrightarrow E\cr u&\longmapsto u_1.\end{array}$$
Cette application est linéaire et vérifie les propriétés suivantes :
\begin{enumerate}
  \item $p\circ p=p$.
  \item $\Im p =E_1$ et $\Ker p=E_2$.
  \item $E_1$ est l'ensemble des vecteurs invariants par $p$.  
\end{enumerate}
\end{proposition}


Si $E$ est de dimension $2$, les espaces $E_1$ et $E_2$ 
sont des droites vectorielles, notons $e_1$ une base de $E_1$ et $e_2$ 
une base de $E_2$, les vecteurs $e_1$ et $e_2$ forment une base de $E$ dans cette base, 
la matrice de la projection $p$ sur $E_1$ parallèlement à $E_2$ s'écrit
$$\left(\begin{matrix}1&0\cr0&0\end{matrix}\right).$$

[[exclure cas $p=id$, $p=0$]]

Si $E$ est de dimension $3$, supposons $E_1$ de dimension $2$ engendré par $e_1$ et $e_2$, et $E_2$ de dimension $1$ engendré par $e_3$. Dans la base $(e_1,e_2,e_3)$ de $E$ la matrice de la projection $p$ sur $E_1$ parallèlement à $E_2$ s'écrit
$$\left(\begin{matrix}1&0&0\cr0&1&0\cr0&0&0\end{matrix}\right).$$


%---------------------------------------------------------------
\subsection{Symétries vectorielles}

[[Définition et propriétés]]

\begin{proposition}
Soient $E_1$ et $E_2$ deux sous-espaces supplémentaires d'un espace vectoriel $E$. Tout vecteur $u$ de $E$ s'écrit de manière unique $u=u_1+u_2$ avec $u_1\in E_1$ et $u_2\in E_2$, on appelle \emph{symétrie vectorielle}  de $E$ par rapport à $E_1$ parallèlement à $E_2$ l'application
$$\begin{array}{cc}s:E&\longrightarrow E\cr u&\longmapsto u_1-u_2.\end{array}$$
Cette application est linéaire et vérifie les propriétés suivantes :
\begin{enumerate}
  \item $s\circ s=\id_E$.
  \item $\Im s =E$ et $\Ker s=\{0\}$.
  \item $E_1$ est l'ensemble des vecteurs invariants par $s$.
  \item $E_2$ est l'ensemble des vecteurs tels que $s(u)=-u$.  
\end{enumerate}
\end{proposition}

Si $E$ est de dimension $2$, les espaces $E_1$ et $E_2$ sont des droites vectorielles, 
notons $e_1$ une base de $E_1$ et $e_2$ une base de $E_2$, 
les vecteurs $e_1$ et $e_2$ forment une base de $E$ dans cette base, 
la matrice de la symétrie $s$ par rapport à $E_1$ parallèlement à $E_2$ s'écrit
$$\left(\begin{matrix}1&0\cr0&-1\end{matrix}\right).$$

Si $E$ est de dimension $3$, supposons $E_1$ de dimension $2$ engendré 
par $e_1$ et $e_2$, et $E_2$ de dimension $1$ engendré par $e_3$. 
Dans la base $(e_1,e_2,e_3)$ de $E$ la matrice de la symétrie $s$ par 
rapport à $E_1$ parallèlement à $E_2$ s'écrit
$$\left(\begin{matrix}1&0&0\cr0&1&0\cr0&0&-1\end{matrix}\right).$$



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Les Déterminants}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Formes multilinéaires alternées}

%---------------------------------------------------------------
\subsection{Définitions et premières propriétés}

\begin{definition}
Soient $E_1,\dots E_p,F$ des espaces vectoriels sur $K$, une application
$$\begin{array}{cc}\varphi : E_1\times\dots\times E_p&\longrightarrow F\cr
(x_1,\dots,x_p)&\longmapsto \varphi(x_1,\dots,x_p)\cr\end{array} $$
est dite \defi{$p$-linéaire}, si en tout point, 
les $p$ applications partielles sont linéaires.

Ce qui revient à dire que $\varphi$ est linéaire par rapport à chacune de ses variables.
Si l'on se fixe $i\in\{1,\dots,p\}$ et des vecteurs $x_j\in E_j$ pour $j\in\{1,\dots,p\}\setminus\{i\}$ alors, l'application 
$$\begin{array}{cc}\varphi_i: E_i&\longrightarrow F\cr
x&\longmapsto \varphi(x_1,\dots,x_{i-1},x,x_{i+1},\dots,x_p)\cr\end{array} $$
est linéaire.

Dans le cas particulier où $E_1=\dots=E_p=E$ et $F=K$, on parle 
alors de formes $p$-linéaires et on note 
${\cal{L}}_p(E)$ leur ensemble. Si $p=1$, ce sont les \defi{formes linéaires}, 
si $p=2$ les \defi{formes bilinéaires}.
\end{definition}


\begin{exemple}
On note $a=(a_1,a_2)$ et $b=(b_1,b_2)$ des vecteurs de $\Rr^2$ ,  et on définit
$$\varphi(a,b)=a_1b_2-a_2b_1 $$
L'application $\varphi$ est bilinéaire alternée, en effet si $x$ et $y$ sont des vecteurs de $\Rr^2$, $\alpha$ et $\beta$ des réels, on vérifie que
$$\varphi(a, \alpha x+\beta y)=\alpha\varphi(a,x)+\beta\varphi(a,y)$$ et
$$\varphi(\alpha x+\beta y,b)=\alpha\varphi(x,b)+\beta\varphi(y,b).$$  
\end{exemple}



\begin{definition}
Une forme $p$-linéaire, $\varphi$ sur $E$ est dite \defi{alternée}, si
$$\varphi(x_1,\dots,x_p)=0$$
dès que deux vecteurs parmi les $x_i$, $1\leq i\leq p$, sont égaux.  
\end{definition}

Une conséquence de cette définition est la propriété suivante :

\begin{proposition}
Soit $\varphi$ une forme $p$-linéaire alternée sur $E$,
on ne change pas la valeur de $\varphi(x_1,\dots,x_p)$, 
en ajoutant à un des vecteurs une combinaison
linéaire des autres.  
\end{proposition}


\begin{proof}
Soient $x_1,\dots,x_p$ des vecteurs de $E$ et $\lambda_1,\dots,\lambda_p$ des éléments de $K$.
On a
$$\varphi(x_1,\dots,\underbrace{x_i+\sum_{k=1,k\neq i}^n\lambda_kx_k}_{i^{\rm eme}{\rm place}},\dots,x_p)=
\varphi(x_1,\dots,x_p)+\sum_{k=1,k\neq i}^n\lambda_k\varphi(x_1,\dots,\underbrace{x_k}_{i^{\rm eme}{\rm place}},\dots,x_p).$$
La somme qui apparaît dans le membre de droite est nulle, en effet la forme multilinéaire $\varphi$ étant alternée, pour tout $k\neq i$, $\varphi(x_1,\dots,x_{i-1},x_k,x_{i+1},\dots,x_p)=0$
car $x_k$ apparaît deux fois. Remarquons que cette propriété implique que $\varphi(x_1,\dots,x_p)=0$ dès que les vecteurs 
$x_1,\dots,x_p$ sont linéairement dépendants.  
\end{proof}

Afin de pouvoir définir le déterminant, nous allons faire quelques 
rappels sur le groupe des permutations.



%---------------------------------------------------------------
\subsection{Permutations}

\begin{definition}
Pour tout $n\in\Nn^*$, on appelle \defi{groupe des permutations} de $\{1,\dots,n\}$, noté
$S_n$ le groupe des bijections de $\{1,\dots,n\}$ dans $\{1,\dots,n\}$. On a $\Card S_n=n!\ .$
\end{definition}


\begin{definition}
On appelle \defi{transposition} sur $i$ et $j$, la permutation notée $\tau_{i,j}$ qui
échange $i$ et $j$.  
\end{definition}

\begin{theoreme}
Toute permutation se décompose en produit de transpositions. Ce produit n'est pas
unique mais, pour $\sigma\in S_n$ fixé, la parité du nombre de ces transpositions est fixé, c'est
la signature de la permutation $\sigma$.  
\end{theoreme}

La démonstration de ce théorème se fait par récurrence sur $n$, on ne la fera pas ici. 
Cette décomposition n'est pas unique, mais, pour $\sigma$ fixée, la parité du nombre de transpositions entrant dans la décomposition de $\sigma$ est fixe.
C'est ce qui définit la signature de la permutation $\sigma$.

\begin{definition}
Soit $\sigma\in S_n$, on appelle \defi{signature} de $\sigma$, notée $\epsilon(\sigma)$,
le nombre appartenant à $\{-1,1\}$ défini par
$$\epsilon(\sigma)={{\prod_{1\leq i<j\leq n}^{} (\sigma(i)-\sigma(j)) }\over{\prod_{1\leq i<j\leq n}^{}(i-j)}}$$
\end{definition}

Il est clair que la signature d'une transposition est égale à $-1$.
Lorsque $\epsilon(\sigma)=1$, on dit que la permutation est paire, lorsque $\epsilon(\sigma)=-1$ on dit qu'elle est impaire.
Le sous-groupe des permutations paires est appelé groupe alterné et noté $A_n$.


\begin{proposition}
Soient $\sigma$ et $\sigma'$ deux éléments de $S_n$, on a 
$$\epsilon(\sigma\sigma')=\epsilon(\sigma)\epsilon(\sigma').$$
\end{proposition} 

Ce qui signifie que la signature est un morphisme de groupes de $S_n$ 
sur le groupe multiplicatif $\{-1,1\}$.

\begin{proposition}
Soit $\varphi$ une forme $p$-linéaire sur $E$. Alors, $\varphi$ est alternée si et
seulement si pour toute permutation $\sigma$ de $S_p$ et pour tout $(x_1,\dots,x_p)\in E^p$ on a 
$$\varphi(x_{\sigma(1)},\dots,x_{\sigma(p)})=\epsilon(\sigma)\varphi(x_1,\dots,x_p).$$  
\end{proposition}


Compte tenu de la décomposition de $\sigma$ en produit de transpositions 
et de la propriété de morphisme de la signature, nous allons démontrer 
la proposition suivante, dont on peut déduire la Proposition 1 :
\begin{proposition}
Soit $\varphi$ une forme $p$-linéaire sur $E$. Alors, $\varphi$ est alternée si et
seulement si pour toute transposition $\tau$, on a
$$\varphi(x_{\tau(1)},\dots,x_{\tau(p)})=-\varphi(x_1,\dots,x_p).$$  
\end{proposition}

Démonstration de la proposition 2.
\begin{proof}
Soient $i,k\in\{1,\dots,p\}$, $i<k$. On suppose $\varphi$ alternée, on note $\tau=\tau_{ik}$ la transposition sur $\{i,k\}$. On a
$$\begin{array}{cc}0&=\varphi(x_1,\dots,x_{i-1},\underbrace{x_i+x_k}_{i^{\rm{eme}}{\rm place}},x_{i+1},\dots,x_{k-1},\underbrace{x_k+x_i}_{k^{\rm{eme}}{\rm place}},x_{k+1},\dots,x_p)\cr
&=\varphi(x_1,\dots,x_p)+
\varphi(x_{\tau(1)},\dots,x_{\tau(p)})\end{array}$$
car $x_{\tau(j)}=x_j$ pour $j\neq i $ et $j\neq k$, et $x_{\tau(i)}=x_k$, $x_{\tau(k)}=x_i$.
D'où le résultat.

Réciproquement, si pour toute transposition $\tau$, on a
$\varphi(x_{\tau(1)},\dots,x_{\tau(p)})=-\varphi(x_1,\dots,x_p)$, alors, si $x_i=x_k$ on a
d'une part,
$$\varphi(x_{\tau(1)},\dots,x_{\tau(p)})=\varphi(x_1,\dots,x_p)$$ et d'autre part,
$$\varphi(x_{\tau(1)},\dots,x_{\tau(p)})=-\varphi(x_1,\dots,x_p)$$
d'où $\varphi(x_1,\dots,x_p)=-\varphi(x_1,\dots,x_p)$, c'est-à-dire $\varphi(x_1,\dots,x_p)=0$,
ce qui prouve que la forme est alternée.  
\end{proof}







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Déterminants}

%---------------------------------------------------------------
\subsection{Définition et propriétés}


\begin{theoreme}
Soit $E$ un $K$-espace vectoriel de dimension $n$. L'ensemble des formes $n$-linéaires
alternées sur $E$ est un $K$-espace vectoriel de dimension $1$. De plus, si $\mathcal{B}=(e_1,\dots,e_n)$ est une base de $E$, il
existe une unique forme $n$-linéaire alternée $\Delta$ vérifiant
$$\Delta(e_1,\dots,e_n)=1\,.$$
\end{theoreme} 

\begin{definition}
Avec les notations ci-dessus, le \defi{déterminant} des vecteurs $x_1,\dots,x_n$ dans la base 
$\mathcal{B}=(e_1,\dots,e_n)$ est
$${\det}_{\mathcal{B}}(x_1,\dots,x_n)=\Delta(x_1,\dots,x_n)\,.$$
\end{definition} 

\begin{proof}
On se fixe une base $\mathcal{B}=(e_1,\dots,e_n)$ de $E$.
Soient $x_1,\dots,x_n$, $n$ vecteurs de $E$. Pour $1\leq j\leq n$, on note $(x_{ij})_{1\leq i\leq n}$ les coordonnées du vecteur $x_j$ dans la base $\mathcal{B}=(e_1,\dots,e_n)$, on a $x_j=\sum_{i=1}^n x_{ij}e_i$ et,
$$\varphi(x_1,\dots,x_n)=\varphi\left(\sum_{i=1}^n x_{i1}e_i,\dots,\sum_{i=1}^n x_{in}e_i\right),$$
en utilisant le fait que $\varphi$ est, d'une part $n$-linéaire, d'autre part alternée, on obtient
$$\begin{array}{cc}\varphi(x_1,\dots,x_n)&=
\sum_{\sigma\in S_n}x_{\sigma(1)1}\cdots x_{\sigma(n)n}\varphi(e_{\sigma(1)},\dots,e_{\sigma(n)})\cr
&=\sum_{\sigma\in S_n}x_{\sigma(1)1}\cdots x_{\sigma(n)n}\epsilon(\sigma)\varphi(e_1,\dots, e_n)
\cr&=\left(\sum_{\sigma\in S_n}\epsilon(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(n)n}\right)
\varphi(e_1,\dots, e_n),\end{array}$$ d'après la proposition 1. 
Notons $\Delta(x_1,\dots,x_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(n)n}$. 
Comme $\varphi(e_1,\dots, e_n)$ est une constante qui ne dépend ni des vecteurs $x_i$, ni des permutations $\sigma$ mais uniquement de la base $\mathcal{B}$, en démontrant que $\Delta$ est une forme $n$-linéaire alternée non nulle nous aurons démontré que toute forme $n$-linéaire est un multiple de $\Delta$ et ainsi prouvé que l'ensemble des formes $n$-linéaires
alternées sur $E$ est un $K$-espace vectoriel de dimension $1$.

Démontrons donc que $\Delta$ est une forme $n$-linéaire alternée non nulle. Soit $(x_1,\dots,x_n)\in E^n$, on suppose qu'il existe $i$ et $k\in\{1,\dots,n\}$ tels que $x_i=x_k$. On a alors
$$\begin{array}{cc}\Delta(x_1,\dots,x_n)&=\sum_{\sigma\in S_n}\epsilon(\sigma) x_{\sigma(1)1}\cdots x_{\sigma(n)n}\cr
&=\sum_{\sigma\in A_n}\epsilon(\sigma) x_{\sigma(1)1}\cdots x_{\sigma(n)n}+\sum_{\sigma\notin A_n} \epsilon(\sigma)x_{\sigma(1)1}\cdots x_{\sigma(n)n}.\end{array}$$


Notons $\tau$ la transposition qui échange $i$ et $k$, l'application
$$\begin{array}{cc}A_n &\longrightarrow S_n\setminus A_n\cr \sigma &\longmapsto\sigma\circ\tau\end{array}$$
est une bijection. On peut donc écrire
$$\begin{array}{cc}\Delta(x_1,\dots,x_n)&=
\sum_{\sigma\in A_n}\epsilon(\sigma) x_{\sigma(1)1}\cdots x_{\sigma(n)n}+\sum_{\sigma\in A_n} \epsilon(\sigma\circ\tau)x_{\sigma\circ\tau(1)1}\cdots x_{\sigma\circ\tau(n)n}\cr
&=\sum_{\sigma\in A_n} x_{\sigma(1)1}\cdots x_{\sigma(n)n}-\sum_{\sigma\in A_n} x_{\sigma(1)1}\cdots x_{\sigma(n)n}=0,\end{array}$$ car $x_i=x_k$.
Ce qui prouve que $\Delta$ est alternée. Par ailleurs, $\Delta$ est clairement $n$-linéaire et non nulle et, pour $1\leq j\leq n$, on a 
$$e_j=\sum_{i=1}^n \delta_{ij}e_j\ \ {\hbox{où}}\ \ \delta_{ij}=
\begin{cases}1 & \text{ si } i=j\cr 0& \text{ si } i\neq j\end{cases}$$
d'où
$$\Delta(e_1,\dots,e_n)=\sum_{\sigma\in S_n}\epsilon(\sigma)\delta_{\sigma(1)1}\cdots\delta_{\sigma(n)n}
=\delta_{11}\cdots\delta_{nn}=1.$$


L'espace des formes $n$-linéaires alternées étant de dimension $1$, 
la forme $\Delta$ est unique.
\end{proof}




\begin{theoreme}
Soient $x_1,\dots,x_n$ des vecteurs de $E$, 
les propriétés suivantes sont équivalentes :
\begin{itemize}
  \item[(i)] les vecteurs $x_1,\dots,x_n$ sont linéairement dépendants ;
  \item[(ii)] pour toute base $B$ de $E$, ${\det_B}(x_1,\dots,x_n)=0$ ;
  \item[(iii)] il existe une base $B$ de $E$ telle que ${\det_B}(x_1,\dots,x_n)=0$
\end{itemize}
\end{theoreme} 

\begin{proof}
\begin{itemize}
  \item (i)$\Rrightarrow$(ii) Quelque soit la base, le déterminant est une 
forme $n$-linéaire alternée, il prend donc la valeur $0$ dès que les 
vecteurs sont linéairement dépendants.
  
  \item (ii)$\Rrightarrow$(iii) évident
  
  \item (iii)$\Rrightarrow$(i) On a vu que pour toute forme $n$-linéaire alternée 
$\varphi$, si $\mathcal{B}=(e_1,\dots,e_n)$, pour tout $(x_1,\dots,x_n)\in E^n$ on a
$$\varphi(x_1,\dots,x_n)=\varphi(e_1,\dots,e_n){\det} _\mathcal{B}(x_1,\dots,x_n).$$


Donc, s'il existe une base dans laquelle le déterminant des $x_1,\dots,x_n$ 
est nul, alors, pour toute forme $n$-linéaire alternée $\varphi$, on a 
$\varphi(x_1,\dots,x_n)=0$. Or, si les $x_i$ étaient linéairement indépendants, 
ils formeraient une base de $E$ et il existerait une forme $n$-linéaire alternée 
prenant la valeur $1$ sur cette base, ce qui est contradictoire. Les $x_i$ sont 
donc linéairement dépendants.
\end{itemize}

\end{proof}


Nous venons de définir le déterminant d'un système de $n$ vecteurs, 
nous allons maintenant relier cette notion à celle de déterminant 
d'une matrice carrée ou d'un endomorphisme de $E$.


%---------------------------------------------------------------
\subsection{Déterminant d'une matrice}


\begin{definition}
Soit $A=(a_{i,j})_{1\leq i,j\leq n}\in M_n(K)$. On appelle \defi{déterminant} de $A$, noté
 $\det A$, le déterminant des vecteurs colonnes de la matrice $A$. On a
$$\det A=\left|\begin{matrix}a_{1,1} &\dots &a_{1,n}\cr
\vdots & &\vdots\cr a_{n,1} &\dots & a_{n,n}\end{matrix}\right|=
\sum_{\sigma\in S_n}\epsilon(\sigma)a_{\sigma(1),1}\dots a_{\sigma(n),n}.$$
Le déterminant de $A$ est le déterminant des vecteurs
$c_j=\sum_{i=1}^{n}a_{i,j}e_i$
dans la base $(e_1,\dots,e_n).$  
\end{definition} 

Avant d'en déduire les propriétés du déterminant, voyons ce que signifie cette définition 
si $n=3$. Le groupe symétrique $S_3$ contient $6$ éléments que nous noterons :
$$\id, \tau_1=(23), \tau_2=(13), \tau_3=(12), \sigma_1=(123), \sigma_2=(132),$$
les permutations $\tau_1,\tau_2,\tau_3$ sont des transpositions et $\sigma_1,\sigma_2$ 
sont des $3$-cycles. Calculons le déterminant suivant :
$$\begin{array}{cc}\left|\begin{matrix}a_{1,1} & a_{1,2} & a_{1,3}\cr
a_{2,1} & a_{2,2} & a_{2,3}\cr a_{3,1} & a_{3,2} & a_{3,3}\end{matrix}\right|=
&\epsilon(\id) a_{\id(1),1}a_{\id(2),2}a_{\id(3),3}
+\epsilon(\tau_1)a_{\tau_1(1),1}a_{\tau_1(2),2}a_{\tau_1(3),3}\cr
&+\epsilon(\tau_2)a_{\tau_2(1),1}a_{\tau_2(2),2}a_{\tau_2(3),3}
+\epsilon(\tau_3)a_{\tau_3(1),1}a_{\tau_3(2),2}a_{\tau_3(3),3}\cr
&+\epsilon(\sigma_1)a_{\sigma_1(1),1}a_{\sigma_1(2),2}a_{\sigma_1(3),3}
+\epsilon(\sigma_2)a_{\sigma_2(1),1}a_{\sigma_2(2),2}a_{\sigma_(3),3}\cr
&=a_{1,1}a_{2,2}a_{3,3}
+(-1)a_{1,1}a_{3,2}a_{2,3}
+(-1)a_{3,1}a_{2,2}a_{1,3}\cr
&+(-1)a_{2,1}a_{1,2}a_{3,3}
 +a_{2,1}a_{3,2}a_{1,3}+a_{3,1}a_{1,2}a_{2,3}.\end{array}$$
On retrouve bien un développement du déterminant $3\times 3$.


\begin{proposition}
Soient $A, B\in M_n(K)$, on a
\begin{enumerate}
  \item $\det A=\det^t\!\!A$
  \item $\det A$ dépend linéairement des colonnes (resp. des lignes) de $A$.
  \item Pour tout $\lambda\in K$, $\det (\lambda A)=\lambda^n\det A$.
  \item $\det A\neq 0\iff A$ inversible.
  \item $\det A.B=\det A.\det B$
  \item $\det A^{-1}=(\det A)^{-1}$  
\end{enumerate}  
\end{proposition}

\begin{proof}
\begin{enumerate}
  \item $\det A=\sum_{\sigma\in S_n}\epsilon(\sigma)a_{\sigma(1)1}\cdots a_{\sigma(n)n}$ et 
$\det^t\!\!A=\sum_{\rho\in S_n}\epsilon(\rho)a_{1\rho(1)}\cdots a_{n\rho(n)}$
or, le produit $a_{1\rho(1)}\cdots a_{n\rho(n)}$ reste invariant si l'on fait subir à ses facteurs une
permutation quelconque. On a donc pour $\sigma\in S_n$, 
$$a_{1\rho(1)}\cdots a_{n\rho(n)}=a_{\sigma(1)(\rho\sigma)(1)}\cdots a_{\sigma(n)(\rho\sigma)(n)}$$
ainsi, en prenant $\sigma=\rho^{-1}$, on obtient
$$a_{1\rho(1)}\cdots a_{n\rho(n)}=a_{\rho^{-1}(1)1}\cdots a_{\rho^{-1}(n)n}$$
or, l'application
$$\begin{array}{cc}S_n&\longrightarrow S_n\cr \rho&\longmapsto \rho^{-1}\end{array}$$ 
est une bijection, on a donc
$$\sum_{\sigma\in S_n}\epsilon(\sigma)a_{\sigma(1)1}\cdots a_{\sigma(n)n}=
\sum_{\rho\in S_n}\epsilon(\rho)a_{1\rho(1)}\cdots a_{n\rho(n)},$$
c'est-à-dire
$\det A=\det^t\!\!A$.

  \item Le déterminant est une forme linéaire alternée, il dépend donc linéairement des colonnes de $A$. En raison de la propriété $1$, il dépend également linéairement des lignes de $A$.

  \item Soit $\lambda\in K$, par $n$-linéarité du déterminant, on a
$$\det(\lambda A)=\det(\lambda c_1,\dots,\lambda c_n)=\lambda^n\det(c_1,\dots,c_n)$$

  \item On a $\det A\neq 0\iff$ les vecteurs $c_1,\dots,c_n$ sont des vecteurs linéairements indépendants.
Ce qui revient à dire que ces vecteurs forment une base de $E$, ce qui équivaut à dire que la matrice $A$ est inversible.

  \item  Notons $A=(a_{ij})$ et $B=(b_{ij})$, rappelons que si $C=AB$, alors $C=(c_{ij})$ où
$$c_{ij}=\sum_{k=1}^na_{ik}b_{kj}$$
on a alors 
$$\det C=\sum_{\sigma\in S_n}\epsilon(\sigma)\left(\sum_{k=1}^n a_{\sigma(1)k}b_{k1}\right)
\cdots\left(\sum_{k=1}^n a_{\sigma(n)k}b_{kn}\right)$$ et 
$$\det A\det B=\sum_{\sigma,\rho\in S_n}\epsilon(\sigma)\epsilon(\rho)a_{\sigma(1)1}\cdots 
a_{\sigma(n)n}b_{\rho(1)1}\cdots b_{\rho(n)n}.$$
Un peu de gymnastique sur les permutations permet de se convaincre de l'égalité, mais nous verrons comment obtenir une autre démonstration au paragraphe suivant.

  \item Si $\det A\neq 0$, on a 
$$\det A A^{-1}=\det I_n=1=(\det A)(\det A^{-1})$$
d'où l'égalité $\det (A^{-1})=(\det A)^{-1}$. 
\end{enumerate}
\end{proof}


%---------------------------------------------------------------
\subsection{Déterminant d'un endomorphisme}

\begin{proposition}
Soit $u\in \mathcal{L}(E)$ et $B=(e_1,\dots,e_n)$ une base de $E$,  
le déterminant $\det_B(u(e_1),\dots,u(e_n))$ ne dépend pas de la 
base $B$, on le note $\det u$.
\end{proposition}

\begin{proof}
Soient $B=(e_1,\dots,e_n)$ et $B'=(e'_1,\dots,e'_n)$ deux bases de $E$ 
et $P$ la matrice de passage de $B$ à $B'$ qui exprime les coordonnées 
des vecteurs $(e'_{j})_{1\leq j\leq n}$ dans la base $B$.
Si $A$ est la matrice de $u$ dans la base $B$ et $A'$ la matrice de $u$ 
dans la base $B'$, on a 
$A'=P^{-1}AP$ d'où
$$\det A'=\det P^{-1}AP=(\det P^{-1})(\det A)(\det P)
=(\det P)^{-1}(\det A)(\det P)=\det A.$$
\end{proof}


\begin{proposition}
Pour tout $(x_1,\dots, x_n)\in E^n$, et pour toute forme $n$-linéaire alternée
$\varphi$, on a
$$\varphi(u(x_1),\dots,u(x_n))= (\det u)\varphi(x_1,\dots, x_n)$$  
\end{proposition}


\begin{remarque*}
\begin{enumerate}
  \item Cette proposition pourrait aussi être prise comme définition 
  du déterminant de l'endomorphisme $u$.

  \item Si on prend pour $\varphi$ la forme $n$-linéaire alternée $\det_B$, alors on retrouve 
$$\det u=\det_B(u(e_1),\dots,u(e_n))$$ où $B=(e_1,\dots,e_n)$ est une base de $E$.

  \item L'application $\varphi\circ u$ est une forme $n$-linéaire alternée.
\end{enumerate}  
\end{remarque*}


\begin{proof}
Nous allons examiner trois cas.

$1^{er} cas$ : On suppose le système $(x_1,\dots, x_n)$ lié, alors 
le système $(u(x_1),\dots,u(x_n))$
est également lié, on a donc pour toute forme $n$-linéaire alternée 
$\varphi$, $\varphi(u(x_1),\dots,u(x_n))=0$ et $\varphi(x_1,\dots,x_n)=0$ d'où l'égalité.

$2^{ieme} cas$ : On suppose cette fois $(x_1,\dots,x_n)$ libre mais 
$(u(x_1),\dots,u(x_n))$ lié. Le système de $n$ vecteurs $(x_1,\dots,x_n)$ 
étant libre, c'est une base de $E$ et on peut écrire
$$\det u=\det_{(x_1,\dots,x_n)}(u(x_1),\dots,u(x_n))$$
mais, comme le système $(u(x_1),\dots,u(x_n))$ est supposé lié, on a, 
d'une part, $\det u=0$ et, d'autre part $\varphi(u(x_1),\dots,u(x_n))=0$, d'où l'égalité.

$3^{ieme} cas$ : On suppose les deux systèmes $(x_1,\dots,x_n)$ et $(u(x_1),\dots,u(x_n))$ 
libres, ce qui signifie que $u$ est un isomorphisme. Si $\Delta$ est l'unique 
forme $n$-linéaire alternée prenant la valeur $1$ sur la base $(x_1,\dots,x_n)$ 
on a pour toute $\varphi$,
$$\varphi(u(x_1),\dots,u(x_n))=\Delta(u(x_1),\dots,u(x_n))\varphi(x_1,\dots,x_n),$$
or, $\Delta(u(x_1),\dots,u(x_n))=\det_{(x_1,\dots,x_n)}(u(x_1),\dots,u(x_n))$, 
d'où le résultat.
\end{proof}


Nous remarquons que
si $x_1,\dots,x_n$ sont $n$ vecteurs quelconques de $E$, le déterminant 
de $(x_1,\dots,x_n)$ dans la base $(e_1,\dots,e_n)$ est égal au déterminant 
de l'endomorphisme défini par 
$u(e_i)=x_i$ pour $1\leq i\leq n$. Par ailleurs, en définissant le déterminant 
d'une matrice comme le déterminant de l'endomorphisme qu'elle représente dans 
une base donnée et en utilisant la proposition $2$ pour définir le déterminant 
d'un endomorphisme, les propriétés  du déterminant énoncées au 
paragraphe précédent sont faciles à démontrer (exercice).

\begin{proposition}
Soient $u,v\in\mathcal{L}(E)$, on a
\begin{enumerate}
  \item $\det (u\circ v)=(\det u)(\det v)$
  \item $\det\id_E=1$
  \item Si $u\in\mathcal{L}(E)$, on a $u\in GL(E)\iff\det u\neq 0$, 
  dans ce cas $\det u^{-1}=(\det u)^{-1}$ 
\end{enumerate} 
\end{proposition}

\begin{proof}
(i) En utilisant la proposition $2$, on a d'une part,
$$\varphi(u\circ v(x_1),\dots,u\circ v(x_n))=\det(u\circ v)\varphi(x_1,\dots,x_n),$$
et, d'autre part
$$\varphi(u\circ v(x_1),\dots,u\circ v(x_n))=(\det u)\varphi(v(x_1),\dots,v(x_n))=(\det u)(\det v)\varphi(x_1,\dots,x_n).$$
ce qui prouve que $\det(u\circ v)=\det u\det v$. Remarquons que si $A$ est la matrice de $u$ dans une base $B$ et $A'$ celle de $v$ on retrouve 
$$\det(u\circ v)=\det AA'=\det A\det A'=\det u\det v.$$

(ii) Pour tout $u\in \mathcal{L}(E)$, on a $\id_E\circ u=u$ d'où
$$\det u\det\id_E=\det u$$
d'où $\det \id_E =1$.

(iii) immédiat en utilisant (i) et (ii).
\end{proof}





%---------------------------------------------------------------
\subsection{Méthodes de calcul}

La formule définissant le déterminant ne rend pas très aisé le 
calcul de celui-ci. Nous allons donc étudier quelques méthodes pratiques de calcul.

\begin{proposition}
\begin{enumerate}
  \item On ne change pas la valeur du déterminant d'une matrice 
  en ajoutant à une colonne (resp. ligne) 
une combinaison linéaire des autres colonnes (resp.lignes).
  
  \item Si on effectue une permutation sur les colonnes (ou les lignes), 
  le déterminant est multiplié 
par la signature de la permutation.
\end{enumerate}
  
\end{proposition}

Ce sont de simples rappels des propriétés des formes $n$-linéaires alternées, 
elles ont déjà été démontrées mais elles seront utiles pour les calculs de déterminants.

\begin{theoreme}[Développement par blocs]
Soit $M$ une matrice carrée d'ordre $n$ telle que pour
$i>p$ et $j<p+1$, on ait $a_{i,j}=0$, $M$ est de la forme 
$$M=\left(\begin{matrix}A&C\cr O&B\cr\end{matrix}\right)$$
où $A$ est une matrice carrée d'ordre $p$, $B$ une matrice carrée d'ordre $q=n-p$ et $O$ désigne la 
matrice $q\times p$ dont tous les termes sont nuls. On a alors
$$\det M=(\det A)(\det B)$$
\end{theoreme}  

\begin{proof}
Notons $\mathcal{B}=(\underbrace{e_1,\dots,e_p}_{\mathcal{B}'},\underbrace{e_{p+1},\dots,e_n}_{\mathcal{B}''})$ une base de $E$, $E'$ le sous-espace vectoriel de $E$ engendré par le système $\mathcal{B}'$ et $E''$ le sous-espace vectoriel de $E$ engendré par le système $\mathcal{B}''$, on a $\dim E'=p$ et $\dim E''=q=n-p$.

Considérons les vecteurs colonnes de la matrice $M$ :
$$a_1,\dots,a_p,c_{p+1}+b_{p+1},\dots,c_n+b_n$$ 
avec pour $1\leq i\leq p$, $a_i\in E'$ et pour $p+1\leq j\leq n$, $c_j\in E'$, $b_j\in E''$.

Soit $$\begin{array}{cc}f:(E')^p&\longrightarrow K\cr (x_1,\dots,x_p)&\longmapsto\det_\mathcal{B}(x_1,\dots,x_p,c_{p+1}+b_{p+1},\dots,c_n+b_n),\end{array}$$
l'application $f$ est une forme $p$-linéaire alternée sur $E'$, on a donc $\forall(x_1,\dots,x_p)\in (E')^p$,
$$f(x_1,\dots,x_p)=f(e_1,\dots,e_p)\det_{\mathcal{B}'}(x_1,\dots,x_p),$$
or,
$$f(a_1,\dots,a_p)=\det M=f(e_1,\dots,e_p)\det A$$ et
$$f(e_1,\dots,e_p)=\det_\mathcal{B}(e_1,\dots,e_p,c_{p+1}+b_{p+1},\dots,c_n+b_n).$$
Les $(c_j)_{p+1\leq j\leq n}$ étant combinaison linéaire des $(e_i)_{1\leq i\leq p}$ on a :
$$f(e_1,\dots,e_p)=\det_\mathcal{B}(e_1,\dots,e_p,b_{p+1},\dots,b_n).$$
Or, l'application
$$\begin{array}{cc}g:(E'')^q&\longrightarrow K\cr (x_{p+1},\dots,x_n)&\longmapsto\det_\mathcal{B}(e_1,\dots,e_p,x_{p+1},\dots,x_n)\end{array}$$
est $q$-linéaire alternée donc
$$g(x_{p+1},\dots,x_n)=g(e_{p+1},\dots,e_n)\det_{\mathcal{B}''}(x_{p+1},\dots,x_n).$$
Mais,
$$g(b_{p+1},\dots,b_n)=g(e_{p+1},\dots,e_n)\det B$$
et
$$g(e_{p+1},\dots,e_n)=\det_\mathcal{B}(e_1,\dots,e_p,e_{p+1},\dots,e_n)=1.$$
D'où 
$$\det M= f(e_1,\dots,e_p)\det A=g(b_{p+1},\dots,b_n)\det A=\det A\det B.$$
\end{proof}



Nous allons maintenant étudier la méthode consistant à développer 
le déterminant suivant une ligne où une colonne.

\begin{definition}
Soit $A=(a_{i,j})_{1\leq i,j\leq n}$ une matrice carrée d'ordre $n$. Pour tout $(i,j)$
on appelle {\rm mineur} de l'élément $a_{i,j}$, le déterminant $\Delta_{i,j}$ 
de la matrice carrée d'ordre
$n-1$ obtenue en supprimant la $i$-ième ligne et la $j$-ième colonne de $A$. Le scalaire 
$A_{i,j}=(-1)^{i+j}\Delta_{i,j}$ s'appelle le \defi{cofacteur} de $a_{i,j}$  
\end{definition}
 



\begin{theoreme}[Développement suivant une ligne ou une colonne]
Soit 
$A=(a_{i,j})_{1\leq i,j\leq n}$ une matrice carrée d'ordre $n$, $A_{i,j}$ les cofacteurs des éléments de $A$, 
alors pour tout $(i,j)$,
$$\det A= \sum_{k=1}^{n}a_{k,j}A_{k,j}=\sum_{k=1}^{n}a_{i,k}A_{i,k}.$$
\end{theoreme}    

\begin{proof}
Soit $\mathcal{B}=(e_1,\dots,e_n)$ une base de $E$. Pour $1\leq j\leq n$ notons $a_j$ les vecteurs colonnes de $A$. On a $a_j=\sum_{k=1}^n a_{kj}e_k$.
Pour $j$, $1\leq j\leq n$ fixé, la linéarité du déterminant nous donne
$$\begin{array}{cc}\det A &=\det_\mathcal{B}(a_1,\dots,a_j,\dots,a_n)\cr
&=\det_\mathcal{B}(a_1,\dots,\sum_{k=1}^n a_{kj}e_k,\dots,a_n)\cr
&=\sum_{k=1}^n a_{kj}\underbrace{\det_\mathcal{B}(a_1,\dots,a_{j-1},e_k,a_j\dots,a_n)}_{D_{kj}}\end{array}.$$
En utilisant les propriétés des formes $n$-linéaires alternées, on obtient :
$$\begin{array}{cc}D_{kj}&=(-1)^{j-1}\det_\mathcal{B}(e_k,a_1,\dots,a_{j-1},a_{j+1},\dots,a_n)\cr
&=(-1)^{k-1}(-1)^{j-1}\det_{\mathcal{B}'}(e_k,a_1,\dots,a_{j-1},a_{j+1}\dots,a_n)\end{array}$$
où
$${\mathcal{B}'}=(e_k,e_1,\dots,e_{k-1},e_{k+1},\dots,e_n).$$
On considère la matrice suivante qui exprime les vecteurs $e_k,a_1,\dots,a_n$ dans la base $\mathcal{B}'$ :
$$\left[\begin{matrix}1 & & & \cr 0& & & \cr \vdots&  &M_{kj}& \cr 0& & & \cr\end{matrix}\right]$$
On a donc $D_{kj}=(-1)^{j+k}\det M_{kj}=A_{kj}$ et $\det A=\sum_{k=1}^n a_{kj}A_{kj}$.
\end{proof}




\begin{definition}
La matrice $(A_{i,j})_{1\leq i,j\leq n}$ des cofacteurs de $A$ est appelée 
\defi{comatrice} de $A$, on la note $\tilde A$.
\end{definition} 

\begin{proposition}
Soit $A\in M_n(K)$, alors
$$^t\!\!\tilde A.A=A.^t\!\!\tilde A=(\det A)I_n$$
d'où, si $\det A\neq 0$,
$$A^{-1}={{1}\over{\det A}}\  ^t\!\!\tilde A$$
\end{proposition} 

\begin{proof}
Par définition de $\tilde A$ et $^t\!\!\tilde A$ on a 
$A^t\!\!\tilde A=(\gamma_{ij})_{1\leq i,j\leq n}$ où $\gamma_{ij}=\sum_{k=1}^na_{ik}A_{jk}$.

Remarquons la chose suivante, si l'on note $M$ la matrice obtenue à partir de $A$ en substituant à la $j$-ème ligne une ligne $(\beta_1,\dots,\beta_n)$, $\beta_j\in K$, on a $\det M=\sum_{k=1}^n \beta_k A_{jk}$, en développant par rapport à la $j$-ème ligne. En particulier, si dans $A$ on remplace la $j$-ième ligne par la $l$-ième alors, si $l\neq j$, le déterminant de cette matrice est nul et si $l=j$ c'est le déterminant de $A$. On a donc
$$\sum_{k=1}^na_{lk}A_{jk}=\delta_{lj}\det A.$$
Où $\delta_{lj}$ est le symbole de Kronecker 
$$\delta_{lj}=\begin{cases}1 & \text{ si } l=j\cr 0& \text{ si } l\neq j\end{cases}$$
de même
$$\sum_{k=1}^n a_{kl}A_{kj}=\delta_{lj}\det A.$$
Ainsi les coefficients de $A^t\!\!\tilde A$ sont
$$\gamma_{ij}=\sum_{k=1}^n a_{ik}A_{jk}=\delta_{ij}\det A.$$
D'où $^t\!\!\tilde A.A=A.^t\!\!\tilde A=(\det A)I_n.$
\end{proof}



\begin{exemple}
Notons $A=\left(\begin{matrix}1&2&-1\cr 0&1&1\cr1&1&1\end{matrix}\right)$, on a 
$\tilde A=\left(\begin{matrix}0&1&-1\cr-3&2&1\cr3&-1&1\end{matrix}\right)$ et 
$^t\!\!\tilde A=\left(\begin{matrix}0&-3&3\cr1&2&-1\cr-1&1&1\end{matrix}\right).$

Par ailleurs, $\det A=3$ et on a bien 
$$A^t\!\!\tilde A=\left(\begin{matrix}1&2&-1\cr 0&1&1\cr1&1&1\end{matrix}\right)
\left(\begin{matrix}0&-3&3\cr1&2&-1\cr-1&1&1\end{matrix}\right)=
\left(\begin{matrix}3&0&0\cr0&3&0\cr0&0&3\end{matrix}\right)=3I_n=(\det A)I_n.$$
\end{exemple}


%---------------------------------------------------------------
\subsection{Rang}

\begin{definition}
Soient $E$ et $F$ des espaces vectoriels sur $K$.
\begin{enumerate}
  \item Si $u : E\longrightarrow F$ est une application linéaire, 
  on appelle rang de $u$, la dimension du
sous-espace vectoriel $\Im u$ de $F$. On le note $\rg u$.
  \item Si $A\in M_{n,p}(K)$, on appelle rang de $A$, la dimension du 
  sous-espace vectoriel engendré par
les colonnes de $A$. On le note $\rg A$.
  \item Si $x_1,\dots,x_p$ sont des vecteurs de $E$, on appelle rang du 
  système $(x_1,\dots,x_p)$ la dimension du sous-espace vectoriel engendré 
  par $(x_1,\dots,x_p)$, c'est le nombre maximal de vecteurs linéairement 
  indépendants du système.
\end{enumerate} 
\end{definition}

\begin{proposition}
Si $A$ est la matrice d'une application linéaire $u$ exprimée 
dans une base $\mathcal{B}$ de $E$ et une base $\mathcal{B}'$ 
de $F$, on a $\rg u=\rg A$.
  
\end{proposition}

\begin{proof}
Si $\mathcal{B}=(e_1,\dots,e_n)$ est une base de $E$, 
le sous-espace vectoriel $\Im u$ de $F$ est engendré par 
les vecteurs $u(e_1),\dots,u(e_n)$ qui sont les vecteurs 
colonnes de la matrice $A$ de $u$.
\end{proof}

 

Remarquons que si $A\in M_n(K)$, alors $\rg A=n\iff A$ est inversible.

\begin{proposition}
Soit $A\in M_{n,p}(K)$,
\begin{enumerate}
  \item si $P\in GL_n(K)$, alors $\rg PA=\rg A$.
  \item si $Q\in GL_p(K)$, alors $\rg AQ=\rg A$.  
\end{enumerate}
\end{proposition}

\begin{proof}
1) Soient $$\begin{array}{cc}f:K^p&\longrightarrow K^n\cr X=(x_1,\dots,x_p)&\longmapsto AX\end{array}$$ et 
$$\begin{array}{cc}g:K^p&\longrightarrow K^n\cr X=(x_1,\dots,x_p)&\longmapsto PAX\end{array}$$
On a $\rg A=\rg f=\dim\Im f$ et $\rg PA=\rg g=\dim\Im g$. Mais, comme $P$ est inversible, $\Ker f=\Ker g$. Or, $\dim \Im f+\dim\Ker f=\dim\Im g +\dim\Ker g=p$, d'où $\rg A=\rg PA$.


2) Notons $a_1,\dots,a_p$ les vecteurs colonnes de la matrice $A$, soit $X=(x_1,\dots,x_p)$ un vecteur de $K^p$. Le vecteur $AX$ s'écrit $AX=x_1a_1+\cdots+x_pa_p$, il appartient donc au sous-espace vectoriel engendré par les $(a_i)_{1\leq i\leq p}$, si l'on note $X_1,\dots,X_p$ les vecteurs colonnes de la matrice $Q$, pour tout $i$, $1\leq i\leq p$, le vecteur $AX_i$ appartient au sous-espace vectoriel engendré par les vecteurs colonnes de la matrice $A$. Ainsi les vecteurs colonnes de la matrice $AQ$ appartiennent-ils à ce même sous-espace vectoriel. En écrivant $A=(AQ)Q^{-1}$, pour les mêmes raisons que précédemment, on peut dire que les vecteurs colonnes de la matrice $A$ sont dans le sous-espace vectoriel engendré par les vecteurs colonnes de la matrice $AQ$, ce qui prouve que $\rg A=\rg AQ$.

\end{proof}



\begin{theoreme}
Soient $A\in M_{n,p}(K)$ et $r$ un entier $1\leq r\leq \min(n,p)$, notons $J_r$ la matrice
$$J_r=\left(\begin{matrix}I_r &O\cr O & O\end{matrix}\right)$$ où $I_r$ est la matrice unité $r\times r$ et $O$ désigne une
matrice nulle. Alors
$$\rg A= r\iff \exists P\in GL_n(K), Q\in GL_p(K)\ / PAQ=J_r$$
\end{theoreme} 

\begin{proof}
$(\Leftarrow)$ Il est clair que $\rg J_r=r$, par conséquent, 
s'il existe $P\in GL_n(K), Q\in GL_p(K)$ telles que $PAQ=J_r$, 
comme d'après la proposition $2$, $\rg PAQ=\rg A$, on a $\rg A=\rg J_r=r$.

$(\Rrightarrow)$ Supposons $\rg A=r$, notons $u: K^p\longrightarrow K^n$ 
l'application linéaire de matrice $A$ dans les bases canoniques. 
On a $\rg A=\rg u=\dim\Im u$. Or, $\dim\Im u+\dim\Ker u=p$. 
Il existe une base de $K^p$, $(x_1,\dots,x_p)$, dont les $p-r$ 
derniers vecteurs forment une base de $\Ker u$. Pour tout $i$, 
$1\leq i\leq r$, notons $y_i=u(x_i)$, les vecteurs $y_1,\dots,y_r$ 
engendrent $\Im u$. Par le théorème de la base incomplète, il 
existe des vecteurs $y_{r+1},\dots,y_n$ tels que les $(y_i)_{1\leq i\leq n}$ 
forment une base de $K^n$. La matrice de $u$ exprimée dans 
les bases $(x_1,\dots,x_p)$, $(y_1,\dots,y_n)$ est la matrice $J_r$. 

La matrice $A$ étant la matrice de $u$ exprimée dans les bases 
canoniques de $K^n$ et $K^p$, si $P$ est la matrice qui exprime 
les vecteurs de la base canonique de $K^n$ dans la base $(y_1,\dots,y_n)$ 
et $Q$ la matrice qui exprime les vecteurs de la base $(x_1,\dots,x_p)$ 
dans la base canonique de $K^p$, alors la matrice $PAQ$ est la matrice 
de $u$ exprimée dans les bases $(x_1,\dots,x_p)$ et $(y_1,\dots,y_n)$, 
c'est-à-dire que ses colonnes sont les coordonnées des vecteurs 
$u(x_1),\dots,u(x_p)$ dans la base $(y_1,\dots,y_n)$, on a donc $PAQ=J_r.$
\end{proof}


\begin{corollaire}
Soit $A\in M_{n,p}(K)$, on a $\rg^t\!\!A=\rg A.$
\end{corollaire} 

\begin{proof}
Notons $r=\rg A$, alors, d'après le théorème précédent, 
il existe une matrice $P\in GL_n(K)$ et une matrice 
$Q\in GL_p(K)$ telles que $PAQ=J_r$. On a alors 
$^t(PAQ)=^t\!\!J_r$, c'est-à-dire $^t\!Q^t\!A^t\!P=^t\!\!J_r$. 
Ainsi, il existe une matrice $^t\!Q\in GL_p(K)$ et une matrice 
$^t\!P\in GL_n(K)$ telles que $^t\!Q^t\!A^t\!P=^t\!\!J_r$, or 
la matrice $^t\!\!J_r$ est dans $M_{p,n}(K)$ du même type que 
$J_r$ dans $M_{n,p}(K)$, ce qui prouve, en raison de l'équivalence 
démontrée dans le théorème, que  $\rg^t\!A=r$. 
\end{proof} 


\begin{theoreme}
Deux matrices de $M_{n,p}(K)$ sont équivalentes 
si et seulement si elles ont même rang.
\end{theoreme} 

Rappelons que deux matrices $A$ et $B$ de $M_{n,p}(K)$ 
sont dites \defi{équivalentes}, noté $A\sim B$, 
s'il existe $P\in GL_n(K)$ et $Q\in GL_p(K)$ telles que $B=PAQ$.

\begin{proof}

$(\Rrightarrow)$ Soient $A$ et $B\in M_{n,p}(K)$, on 
suppose $A\sim B$, il existe alors $P\in GL_n(K)$ et 
$Q\in GL_p(K)$ telles que $B=PAQ$, on a donc $\rg B=\rg PAQ=\rg AQ=\rg A$.

$(\Leftarrow)$ Supposons que $A$ et $B$ aient même rang, 
alors $A\sim J_r$ et $B\sim J_r$ d'où $A\sim B$.

\end{proof} 

Nous allons maintenant étudier le lien entre le rang et le déterminant. 
Remarquons que dans la matrice $J_r$, qui est de rang $r$ par définition, 
le plus "grand" déterminant extrait non nul est de rang $r$, 
nous allons préciser cette notion.

\begin{definition}
Soit $\displaystyle A=(a_{i,j})_{{1\leq i\leq n}\atop{1\leq j\leq p}}\in M_{n,p}(K)$. 
Soient $I\subset\{1,\dots,n\}$ et $J\subset\{1,\dots,p\}$. 
La matrice $\displaystyle B=(a_{i,j})_{{i\in I}\atop{j\in J}}$  
s'appelle matrice \defi{extraite} de $A$.
On appelle déterminant \defi{extrait} de $A$ le déterminant 
d'une matrice carrée extraite de $A$.
\end{definition}

\begin{theoreme}
Soient $A\in M_{n,p}(K)$ et $r$ un entier $1\leq r\leq \min(n,p)$, 
le rang de $A$ est supérieur ou égal à $r$ si et seulement si il 
existe une matrice carrée d'ordre $r$ inversible extraite de $A$.
\end{theoreme} 

\begin{proof}
$(\Rrightarrow)$ Supposons $\rg A\geq r$, il existe alors $r$ colonnes 
de $A$ linéairement indépendantes. On peut supposer que ce sont les $r$ 
premières, notées $c_1,\dots,c_r$, car le rang n'est pas affecté par 
une permutation des colonnes.  On peut compléter le système $(c_1,\dots,c_r)$ 
par des vecteurs de la base canonique de $K^n$, notons les 
$e'_{r+1},\cdots,e'_n\in\{e_1,\dots e_n\}$, de façon à obtenir une 
autre base de $K^n$. On a alors 
$\det_{(e_1,\dots,e_n)}(c_1,\dots c_r,e'_{r+1},\dots,e'_n)\neq 0.$
En développant ce déterminant, successivement, par rapport à la dernière 
colonne, on obtient donc un déterminant $r\times r$ non nul qui est 
extrait de la matrice des colonnes $c_1,\dots,c_r$ donc de $A$.

$(\Leftarrow)$ Supposons qu'il existe une matrice $B\in M_r(K)$ 
extraite de $A$ et inversible. Notons $c_1,\dots, c_r$ les vecteurs 
colonnes de $A$ correspondants, ces vecteurs sont alors linéairement 
indépendants et la matrice $A$ est de rang $\geq r$. 
\end{proof}



\begin{corollaire}
Le rang d'une matrice est l'ordre maximum d'un déterminant extrait non nul de 
cette matrice. 
\end{corollaire} 

\begin{proof}
Soit $M$ une matrice, la matrice carrée d'ordre $1$ déterminée 
par un élément non nul de $M$ est inversible, par conséquent 
l'ensemble des ordres des sous-matrices carrées inversibles est 
non vide, il est majoré par $r$, il admet donc un plus grand 
élément qui ne peut-être que $r$.
\end{proof} 

 
\begin{exemple}
Soit $A=\left(\begin{matrix}5&-1&-3\cr2&1&-4\cr3&-2&1\cr1&2&-5\end{matrix}\right)$, c'est la matrice d'une application linéaire de $\Rr^3$ dans $\Rr^4$. Le rang d'une matrice n'est pas affecté par les opérations linéaires sur les colonnes, on a donc
$$\rg A=\rg\left(\begin{matrix}1&5&-3\cr-1&2&-4\cr2&3&1\cr-2&1&-5\end{matrix}\right)
=\rg\left(\begin{matrix}1&0&0\cr-1&7&-7\cr 2&-7&7\cr-2&11&-11\end{matrix}\right)
=\rg\left(\begin{matrix}1&0&0\cr-1&7&0\cr2&-7&0\cr-2&11&0\end{matrix}\right)=2.$$
En effet, tous les déterminants $3\times3$ sont nuls mais pas tous les déterminants $2\times2$.
\end{exemple}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Réduction des endomorphismes}



Soit $u$ un endomorphisme de $E$, il s'agit de trouver une base de $E$ dans laquelle la matrice de $u$ s'exprime de la manière la plus simple possible. Pour cela nous allons nous intéresser aux droites (sous-espaces vectoriels de dimension $1$) de $E$ invariantes par $u$. Par exemple 
 
 1) si l'endomorphisme $u$ est défini par $$\ u\left(\begin{matrix}x\cr y\end{matrix}\right)=\left(\begin{matrix}2&0\cr0&2\end{matrix}\right)\left(\begin{matrix}x\cr y\end{matrix}\right)=
\left(\begin{matrix}2x\cr 2y\end{matrix}\right)$$ c'est-à-dire si $u$ est une homothétie, toute droite est invariante par $u$.

2) si l'endomorphisme $u$ est défini $$\ u\left(\begin{matrix}x\cr y\end{matrix}\right)=\left(\begin{matrix}2&0\cr0&3\end{matrix}\right)\left(\begin{matrix}x\cr y\end{matrix}\right)=
\left(\begin{matrix}2x\cr 3y\end{matrix}\right)$$
seuls les axes $Ox$ et $Oy$ sont invariants par $u$.

Si une droite $D\subset E$ est invariante par un endomorphisme $u$, alors pour tout vecteur non nul
 $x\in D$, il existe $\lambda\in K$ tel que $u(x)=\lambda x$. C'est ce qui nous amène à la notion de vecteurs propres et valeurs propres.

 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Valeurs propres, vecteurs propres}

\begin{definition}
Soit $\lambda\in K$, $\lambda$ est dite \defi{valeur propre} de l'endomorphisme $u$ si il
existe un vecteur non nul $x\in E$ tel que 
$$u(x)=\lambda x,$$
le vecteur $x$ est alors appelé \defi{vecteur propre} de $u$ pour la valeur propre $\lambda$.
\end{definition} 
 
On remarque que le vecteur $x$ est vecteur propre 
de l'endomorphisme $u$ pour la valeur propre $\lambda$ 
si et seulement si $(u-\lambda\id_E)(x)=0$, c'est-à-dire $x\in\Ker(u-\lambda\id_E)$.

\begin{definition}
Si $\lambda$ est une valeur propre de $u$, on note $E_\lambda$ 
l'ensemble des vecteurs $x\in E$ tels que $u(x)=\lambda x$, 
c'est le sous-espace vectoriel de $E$ constitué des vecteurs propres 
de $u$ associés à la valeur propre $\lambda$ et du vecteur nul.
On a
$$E_\lambda =\Ker(u-\lambda\id_E),$$
c'est le \defi{sous-espace propre} associé à la valeur propre $\lambda$.
\end{definition} 

Il est clair que $\lambda$ est valeur propre de $u$ 
si et seulement si $E_\lambda\neq\{0\}$, c'est-à-dire si et 
seulement si l'endomorphisme $u-\lambda\id_E$ n'est pas injectif, 
ce qui équivaut à $u-\lambda\id_E$
non bijectif car $E$ est supposé de dimension finie.

\begin{proposition}
Soit $P$ un polynôme et $x$ un vecteur propre de $u$ pour la valeur propre $\lambda$,
alors, $x$ est vecteur propre de $P(u)$ pour la valeur propre $P(\lambda)$.
\end{proposition} 

Rappelons que si $P(X)=\sum_{k=0}^n a_k X^k$ est un polynôme et si $u$ est un endomorphisme de $E$, on note $P(u)$ l'endomorphisme
$$P(u)=a_0 \id_E+a_1 u+a_2 u^2+\cdots+a_n u^n$$
qui à un vecteur $x$ associe le vecteur
$$P(u)(x)=a_0 x+a_1 u(x)+a_2 u^2(x)+\cdots+a_n u^n(x),$$
où pour $1\leq k\leq n$, on a $u^k=\underbrace{u\circ u\cdots\circ u}_{k\times}$.

\begin{proof}
Nous allons commencer par  démontrer par récurrence sur $k$ que pour tout $k\in\Nn$ et pour tout $x$ vecteur propre de $u$ pour la valeur propre $\lambda$ on a $u^k(x)=\lambda^k x$.

La propriété est clairement vraie pour $k=0$ et $k=1$, supposons la vraie pour un $k$ arbitrairement fixé et calculons alors $u^{k+1}(x)$ :
$$u^{k+1}(x)=u(u^k(x))=u(\lambda^k x)=\lambda^k u(x)=\lambda^{k+1} x,$$
d'où le résultat. Si maintenant $P(X)=\sum_{k=0}^n a_k X^k$, on a 
$$P(u)(x)=\sum_{k=0}^n a_ku^k(x)=\sum_{k=0}^n a_k\lambda^k x=\left(\sum_{k=0}^n a_k\lambda_k\right) x=P(\lambda)x.$$
Ce qui prouve que $x$ est un vecteur propre de l'endomorphisme $P(u)$ pour la valeur propre $P(\lambda)$.  
\end{proof}

\begin{theoreme}
Soient $\lambda_1,\dots,\lambda_r$ des valeurs propres distinctes de 
$u$, alors si pour $1\leq i\leq r$ $x_i$ est un vecteur propre pour 
$\lambda_i$, les $x_i$ sont linéairement indépendants.
\end{theoreme}

\begin{proof}
On suppose $r\geq 2$. Notons, pour $1\leq i\leq r$,
$$P_i(X)=(X-\lambda_1)\cdots(X-\lambda_{i-1})(X-\lambda_{i+1})\cdots(X-\lambda_r)=\prod_{1\leq j\leq r,j\neq i}(X-\lambda_j) .$$
Pour tout $i$ et pour tout $j$, $i\neq j$, on a $P_i(\lambda_j)=0$ et,
puisque les valeurs propres sont supposées distinctes, 
$P_i(\lambda_i)\neq 0$. D'après la proposition précédente, 
le vecteur $x_j$ est vecteur propre de $P_i(u)$ pour la valeur 
propre $P_i(\lambda_j)=0$, on a donc $P_i(u)(x_j)=0$. De même, 
$x_i$ est vecteur propre de $P_i(u)$ pour la valeur propre 
$P_i(\lambda_i)\neq0$, d'où, pour tout $i$, $1\leq i\leq r$, 
on a $P_i(u)(x_i)=P_i(\lambda_i)x_i$.

Supposons qu'il existe des scalaires $a_1,\dots,a_r\in K$, tels que 
$$a_1x_1+\cdots+a_rx_r=0,$$
on a alors
$$P_i(u)(a_1x_1+\cdots+a_rx_r)=0,$$
c'est-à-dire
$$a_1P_i(u)(x_1)+\cdots+a_rP_i(u)(x_r)=a_iP_i(\lambda_i)(x_i)=0.$$
Ce qui implique $a_i=0$, car $P_i(\lambda_i)\neq 0$ et $x_i\neq 0$. 
Ceci étant vrai pour tout $i$ on a obtenu $a_1=a_2=\dots=a_r=0$, ce qui prouve que les vecteurs $x_1,\dots,x_r$ sont linéairement indépendants.  
\end{proof}

\begin{corollaire}
Si $\lambda_1,\dots,\lambda_r$ sont des valeurs propres distinctes, 
alors les sous-espaces propres correspondants, 
$E_{\lambda_1},\dots,E_{\lambda_r}$ sont en somme directe.
\end{corollaire} 

\begin{proof}
Pour chaque $i$, $1\leq i\leq r$, soit $x_i \in E_{\lambda_i}$. On suppose $x_1+\cdots+x_r=0$, nous allons montrer
qu'alors $x_1=x_2=\dots=x_r=0$.

Notons $r'$ le nombre minimal tel qu'il existe $r'$ tels vecteurs non nuls, on peut supposer sans perdre de généralité que ce sont les $r'$ premiers, c'est-à-dire que l'on a 
$x_1+\cdots+x_{r'}=0$, on a alors
$$u(x_1+\cdots+x_{r'})-\lambda_1(x_1+\cdots+x_{r'})=0$$
c'est-à-dire
$$(\lambda_2-\lambda_1)x_2+\cdots+(\lambda_{r'}-\lambda_1)x_{r'}=0.$$
Or, les vecteurs $x_2,\dots,x_{r'}$ sont linéairement indépendants et les valeurs propres sont supposées distinctes, de plus, $r'$ est supposé minimal et l'on vient d'obtenir $r'-1$ vecteurs non nuls dont la somme est nulle, cette contradiction prouve que les vecteurs $x_2,\dots,x_{r'}$ sont nuls 
et donc que tous les $(x_i)_{1\leq i\leq r}$ sont nuls, la somme est donc directe.
\end{proof}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Polynôme caractéristique}

\begin{proposition}
Soit $u$ un endomorphisme de $E$ et $A$ sa matrice dans une base $\mathcal{B}$, on appelle
polynôme caractéristique de $u$, ou de $A$ le polynôme, qui ne dépend pas de la base, 
$$P_u(X)=P_A(X)=\det(A-X.I_n)=\det(u-X\id_E).$$
De plus, on a 
$$P_A(X)=(-1)^nX^n + (-1)^{n-1}(\tr A)X^{n-1}+\dots+\det A.$$
\end{proposition} 

Avant de démontrer ce théorème, voyons le cas $n=2$, 
soit $A=\left(\begin{matrix}a&b\cr c&d\end{matrix}\right)$, 
on a $$P_A(X)=\left|\begin{matrix}a-X&b\cr c& d-X\end{matrix}\right|
=(a-X)(d-X)-bc=(-1)^2X^2+(-1)\underbrace{(a+d)}_{\tr A}X+
\underbrace{ad-bc}_{\det A}.$$

\begin{proof}


Vérifions tout d'abord que ce polynôme ne dépend pas de la base $\mathcal{B}$. Si $B$ est la matrice de $u$ dans une autre base, il existe une matrice inversible $P\in GL_n(E)$ telle que $B=P^{-1}AP$.
D'où
$$B-XI_n=P^{-1}AP-X(P^{-1}I_nP)=P^{-1}(A-XI_n)P,$$
d'où
$$\det(B-XI_n)=\det(A-XI_n).$$
Par ailleurs, si $A=(a_{i,j})_{1\leq i,j\leq n}$, on a
$$\begin{array}{cc}P_A(X)&=\left|
\begin{matrix}a_{11}-X&a_{12}&\cdots&a_{1n}\cr a_{21}&a_{22}-X& & \cr
\vdots& & & \cr a_{n1}&\cdots &\cdots&a_{nn}-X\end{matrix}\right|\cr
&=\sum_{\sigma\in S_n}\epsilon(\sigma)b_{\sigma(1)1}\cdots b_{\sigma(n)n}\ {\hbox{où}}\ b_{ij}=a_{ij}\ {\hbox{si}}\ i\neq j, b_{ii}=a_{ii}-X\cr
&=(a_{11}-X)\cdots(a_{nn}-X)
+\sum_{\sigma\in S_n,\sigma\neq\id}\epsilon(\sigma)b_{\sigma(1)1}\cdots b_{\sigma(n)n}.\end{array}$$
Or, si $\sigma\neq\id$, il y a au plus $n-2$ entiers $k$ tels que $\sigma(k)=k$, et, donc, le polynôme
$$\sum_{\sigma\in S_n,\sigma\neq\id}\epsilon(\sigma)b_{\sigma(1)1}\cdots b_{\sigma(n)n}$$
est de degré au plus $n-2$. Le polynôme $P_A(X)$ étant de degré $n$, ses termes de degré
$n$ et $n-1$ proviennent du produit 
$$(a_{11}-X)\cdots(a_{nn}-X)=(-1)^nX^n+(-1)^{n-1}(\tr A)X^{n-1}+\dots.$$ 
Le terme constant, quant à lui est donné par $P_A(0)=\det A$.
\end{proof}


\begin{proposition}
Soit $\lambda\in K$, alors
$$\lambda\  {\rm valeur\  propre\  de}\ u\iff P_u(\lambda)=0. $$
\end{proposition}
 

\begin{proof}
On a vu que $\lambda$ est valeur propre de $u$ si et seulement 
si l'endomorphisme $u-\lambda\id_E$ est non bijectif, 
ce qui équivaut à $\det(u-\lambda\id_E)=0$, c'est-à-dire $P_u(\lambda)=0$.
\end{proof}


Remarquons que si $E$ est de dimension $n$, le polynôme 
caractéristique d'un endomorphisme de $E$ est de degré $n$, 
il a donc au plus $n$ racines, l'endomorphisme $u$ a donc au 
plus $n$ valeurs propres. Si le corps $K=\Cc$, c'est-à-dire 
s'il est algébriquement clos, alors tout endomorphisme admet 
au moins une valeur propre.


\begin{theoreme}
Soient $u$ un endomorphisme de $E$ et $P_u$ son polynôme caractéristique.
Soient $\lambda$ une valeur propre de $u$, $m(\lambda)$ sa multiplicité dans $P_u$ et $E_\lambda$
le sous-espace propre associé, alors on a
$$1\leq \dim E_\lambda\leq m(\lambda)$$
\end{theoreme} 

\begin{proof}
Rappelons que si $\lambda$ est une racine de $P_u$ de multiplicité $m$, on a 
$$P_u(X)=(\lambda-X)^mQ(X)$$
avec $Q(\lambda)\neq 0$.

Soit $\lambda$ une valeur propre de $u$ et $E_\lambda$ son sous-espace propre, on note $p=\dim E_\lambda$ et $(e_1,\dots,e_p)$ une base de $E_\lambda$. On complète cette base en une base de $E$
$(e_1,\dots,e_p,e_{p+1},\dots,e_n)$.
Dans cette base, la matrice de $u$ est de la forme
$$A=\left[\begin{matrix}\lambda I_p & C\cr 0 & B\end{matrix}\right],$$
d'où
$\det(A-XI_n)=\det((\lambda-X)I_p)\det(B-XI_{n-p})=(\lambda-X)^p\det(B-XI_{n-p}).$

Ce qui prouve que $(\lambda-X)^p$ divise $P_u$, donc, par définition de la multiplicité d'une racine, on a $p\leq m(\lambda)$. Par aileurs, par définition d'une valeur propre et d'un sous-espace propre, on a
$\dim E_\lambda\geq 1$.
\end{proof} 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Endomorphismes diagonalisables}


\begin{definition}
On dit qu'une matrice $A=(a_{i,j})_{1\leq i,j\leq n}$ est \defi{diagonale} 
si $a_{i,j}=0$ dès que $i\neq j$.
\end{definition} 

\begin{definition}
On dit que l'endomorphisme $u$ de $E$ est \defi{diagonalisable}, 
s'il existe une base de $E$ formée de vecteurs propres.
\end{definition} 


\begin{definition}
Soit $A$ une matrice d'ordre $n$, on dit que $A$ est \defi{diagonalisable} 
sur $K$ si il existe une matrice $P\in M_n(K)$ inversible telle que $P^{-1}AP$ soit diagonale.
\end{definition} 


\begin{theoreme}
Soit $u\in {\cal L }(E)$, les propositions suivantes sont équivalentes :
\begin{itemize}
  \item[(i)] $u$ est diagonalisable ;
  \item[(ii)] $P_u$ a toutes ses racines dans le corps $K$ et pour toute 
  racine $\lambda$ de $P_u$, l'ordre de multiplicité de $\lambda$ est égal 
  à la dimension du sous-espace propre correspondant $E_\lambda$ ;
  \item[(iii)]  Il existe des valeurs propres $\lambda_1,\dots,\lambda_r$ telles que 
$$E=E_{\lambda_1}\oplus\cdots\oplus E_{\lambda_r}.$$ 
\end{itemize}
\end{theoreme} 

\begin{proof}
$(i)\Rrightarrow(ii)$ Supposons $u$ diagonalisable et notons $\lambda_1,\dots,\lambda_r$ ses valeurs propres et $m_1,\dots,m_r$ leur multiplicité. Soit $A$ une matrice diagonale de $u$, on suppose que  chaque $(\lambda_i)_{1\leq i\leq r}$ apparaît $h_i$ fois dans la diagonale de $A$, on a alors,
$$P_A(X)=P_u(X)=\prod_{i=1}^r(\lambda_i-X)^{h_i}.$$
Ce qui prouve que $P_u$ se décompose en facteurs linéaires dans $K[X]$ et que $h_i=m_i$. Par ailleurs, pour tout $i, 1\leq i\leq r$, il existe $h_i$ vecteurs de la base de $E$ vérifiant $u(x)=\lambda_ix$, il existe donc $h_i$ vecteurs linéairement indépendants dans $E_{\lambda_i}$, d'où 
$\dim E_{\lambda_i}\geq h_i$, or, $h_i=m_i$ et l'on a démontré que $\dim E_{\lambda_i}\leq m_i=h_i$, d'où l'égalité.

$(ii)\Rrightarrow(iii)$ On suppose que $P_u$ a toutes ses racines dans $K$ et que pour toute racine $(\lambda_i)_{1\leq i\leq r}$ de multiplicité $m_i$ on a $\dim E_{\lambda_i}=m_i$. On a alors
$$P_u(X)=\prod_{i=1}^r(\lambda_i-X)^{m_i}.$$
Notons $F=E_{\lambda_1}\oplus\cdots\oplus E_{\lambda_r}$, on sait que les sous-espaces propres sont en somme directe. On a $\dim F=\sum_{i=1}^r m_i=\deg P_u=n$ d'où $F=E$.

$(iii)\Rrightarrow(i)$ Pour tout $i$, $1\leq i\leq r$, on note $\mathcal{B}_i$ une base de $E_{\lambda_i}$, la base $\mathcal{B}=\cup_{i=1}^r\mathcal{B}_i$ est une base de $E$ puisque $E$ est somme directe des $E_{\lambda_i}$. Ainsi, il existe une base de $E$ formée de vecteurs propres de $u$, ce qui prouve que $u$ est diagonalisable.  

\end{proof}


\begin{exemple}
Diagonalisons la matrice 
$$A=\left(\begin{matrix}4&-2\cr1&1\cr\end{matrix}\right).$$
Pour cela on détermine ses valeurs propres :
$$\det(A-\lambda I)=\left|\begin{matrix}4-\lambda &-2\cr1&1-\lambda\cr\^o\end{matrix}\right|=(4-\lambda)(1-\lambda)+2=\lambda^2-5\lambda+6=(\lambda-2)(\lambda-3).$$
Ainsi, la matrice $A$ admet deux valeurs propres distinctes, qui sont $\lambda_1=2$ et $\lambda_2=3$. Elle est diagonalisable. Déterminons une base de vecteurs propres :
$$\left(\begin{matrix}4&-2\cr1&1\cr\end{matrix}\right)\left(\begin{matrix}x\cr y\end{matrix}\right)=
\left(\begin{matrix}2x\cr 2y\end{matrix}\right)\iff x=y,$$
d'où le vecteur propre $u_1=(1,1)$ associé à la valeur propre $\lambda_1=2$ . 
$$\left(\begin{matrix}4&-2\cr1&1\cr\end{matrix}\right)\left(\begin{matrix}x\cr y\end{matrix}\right)=
\left(\begin{matrix}3x\cr 3y\end{matrix}\right)\iff x=2y,$$
d'où le vecteur propre $u_2=(2,1)$ associé à la valeur propre $\lambda_2=3$ .
Dans la base $(u_1, u_2)$, la matrice s'écrit
$$A'=\left(\begin{matrix}2&0\cr0&3\cr\end{matrix}\right).$$
On a $A=PA'P^{-1}$ où
$$P=\left(\begin{matrix}1&2\cr1&1\cr\end{matrix}\right)\ {\hbox{et}}\ P^{-1}=\left(\begin{matrix}-1&2\cr1&-1\cr\end{matrix}\right).$$
\end{exemple}

\begin{exemple}
Soit $$A=\left(\begin{matrix}1&0&0\cr0&1&0\cr1&-1&2\end{matrix}\right)$$
Démontrons que $A$ est diagonalisable et trouvons une matrice $P$ telle que $P^{-1}AP$ soit diagonale.

Commençons par calculer le polynôme caractéristique de $A$ :
$$P_A(X)=\left|\begin{matrix}1-X&0&0\cr 0&1-X&0\cr1&-1&2-X\end{matrix}\right|=(1-X)^2(2-X)$$
Les racines du polynôme caractéristique sont les réels $1$ avec la multiplicité $2$, et $2$ avec la multiplicité $1$.

Déterminons les sous-espaces propres associés : Soit $E_1$ le sous-espace propre associé à la valeur propre double $1$.

$E_1=\{V(x,y,z)\in\Rr^3/\ A.V=V\}$, 
$$V\in E_1\iff \left\{\begin{array}{cc}x&=x\cr y&=y\cr x-y+z&=0\end{array}\right.\iff x-y+z=0$$
$E_1$ est donc un plan vectoriel, dont les vecteurs $e_1=(1,1,0)$ et $e_2=(0,1,1)$ forment une base.

Soit $E_2$ le sous-espace propre associé à la valeur propre simple $2$.

$E_2=\{V(x,y,z)\in\Rr^3/\ A.V=2V\}$, 
$$V\in E_2\iff \left\{\begin{array}{cc}x&=2x\cr y&=2y\cr x-y+2z&=2z\end{array}\right.\iff x=0, y=0$$
$E_2$ est donc une droite vectorielle, dont le vecteur $e_3=(0,0,1)$ est une base.

Les dimensions des sous-espaces propres sont égales aux la multiplicités des valeurs propres correspondantes, la matrice $A$ est donc diagonalisable.
Dans la base $(e_1, e_2, e_3)$ l'endomorphisme représenté par $A$ (dans la base canonique) a pour matrice
$$D=\left(\begin{matrix}1&0&0\cr 0&1&0\cr 0&0&2\end{matrix}\right)$$
la matrice de passage $$P=\left(\begin{matrix}1&0&0\cr 1&1&0\cr 0&1&1\end{matrix}\right)$$ vérifie $P^{-1}AP=D$.
\end{exemple}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Trigonalisation} 


Il est clair que tous les endomorphismes ne sont pas diagonalisables, 
on peut néanmoins pour certains d'entre eux trouver une base de $E$ dans 
laquelle la matrice est triangulaire supérieure.

\begin{definition}
On dit qu'une matrice $A=(a_{i,j})_{1\leq i,j\leq n}$ est \defi{triangulaire supérieure} 
si $a_{i,j}=0$ dès que $i> j$.
\end{definition} 


\begin{definition}
On dit qu'un endomorphisme $u$ (ou une matrice $A$) est \defi{trigonalisable} s'il existe une
base dans laquelle la matrice de $u$ soit triangulaire 
(ou s'il existe une matrice inversible $P$ telle que $P^{-1}AP$ soit triangulaire).
\end{definition} 


\begin{theoreme}
L'endomorphisme $u$ est trigonalisable si et seulement si son polynôme caractéristique
$P_u$ a toutes ses racines dans $K$ (i.e. est scindé sur $K$ ou se décompose en facteurs linéaires sur $K[X]$).

\end{theoreme} 

Remarquons que si $K=\Cc$, tout endomorphisme est trigonalisable, 
ce n'est évidemment pas le cas si $K=\Rr$.

\begin{proof}
$(\Rrightarrow)$ Si $u$ est trigonalisable, il existe une base dans laquelle la matrice de $u$ s'écrit
$$A=\left(\begin{matrix}a_{11}&a_{12}&\cdots &a_{1n} \cr 0&a_{22} & &\vdots \cr\vdots &\ddots &\ddots& a_{n-1,n}\cr0&\cdots&0&a_{nn}\end{matrix}\right).$$On a alors,
$$P_u(X)=P_A(X)=\prod_{i=1}^n(a_{ii}-X),$$
ce qui prouve que $P_u$ a toutes ses racines dans $K$.

$(\Leftarrow)$ La démonstration se fait par récurrence sur $n$. Si $n=1$, il n'y a rien à démontrer. Supposons le résultat vrai pour $n-1$, $n$ étant arbitrairement fixé. Le polynôme $P_u$ ayant au moins une racine dans $K$, notons $\lambda$ l'une d'entre elles et $x_1$ un vecteur propre associé. Soit $H$ l'hyperplan supplémentaire de la droite $Kx_1$. On considère alors une base de $E$, $(x_1,x_2,\dots,x_n)$ avec pour $2\leq i\leq n$, $x_i\in H$. La matrice de $u$ dans cette base s'écrit
$$\left(\begin{matrix}\lambda &\ &\dots& \cr0 &\ &\ & \cr \vdots& & M\cr 0&\ &\ &\ & \end{matrix}\right)$$
où $M$ est une matrice carrée d'ordre $n-1$. On a 
$$P_u(X)=(\lambda-X)\det(M-X\id_H)=(\lambda-X)P_M(X).$$
Notons $v$ l'endomorphisme de $H$ dont la matrice dans la base $(x_2,\dots,x_n)$ est égale à $M$. Par hypothèse de récurrence, $M$ est trigonalisable, en effet, 
$$P_u(X)=(\lambda-X)P_v(X)=(\lambda-X)P_M(X),$$
 et comme $P_u$ est supposé scindé, $P_v$ l'est également. Par conséquent, il existe une base $(y_2,\dots,y_n)$ de $H$ dans laquelle la matrice de $v$ est triangulaire, ainsi dans la base $(x_1,y_2,\dots,y_n)$ la matrice de $u$ est triangulaire.
   
\end{proof} 

\begin{exemple}
Soit $$A=\left(\begin{matrix}1&4&-2\cr0&6&-3\cr-1&4&0\end{matrix}\right)$$
Démontrons que $A$ est trigonalisable et trouvons une matrice $P$ telle que $P^{-1}AP$ soit triangulaire supérieure.

Commençons par calculer le polynôme caractéristique de $A$ :
$$P_A(X)=\left|\begin{matrix}1-X&4&6\cr 0&6-X&-3\cr-1&4&-X\end{matrix}\right|=(3-X)(2-X)^2$$
Les racines du polynôme caractéristique sont les réels $3$ avec la multiplicité $1$, et $2$ avec la multiplicité $2$. Les racines sont dans le corps $\Rr$, la matrice est trigonalisable et peut-être diagonalisable.

Déterminons les sous-espaces propres associés : Soit $E_3$ le sous-espace propre associé à la valeur propre simple $3$.

$E_3=\{V(x,y,z)\in\Rr^3/\ A.V=3V\}$, 
$$V\in E_3\iff \left\{\begin{array}{cc}x+4y-2z&=3x\cr 6y-3z&=3y\cr -x+4y&=3z\end{array}\right.\iff x=y=z$$
$E_3$ est donc la droite vectorielle engendrée par le vecteur $e_1=(1,1,1)$.

Soit $E_2$ le sous-espace propre associé à la valeur propre double $2$.

$E_2=\{V(x,y,z)\in\Rr^3/\ A.V=2V\}$, 
$$V\in E_2\iff \left\{\begin{array}{cc}x+4y-2z&=2x\cr 6y-3z&=2y\cr -x+4y&=2z\end{array}\right.\iff \left\{\begin{array}{cc}x&=z\cr 4y&=3z\end{array}\right.$$
$E_2$ est donc la droite vectorielle engendrée par $e_2=(4,3,4)$.

La dimension de $E_2$ est égale à $1$ alors que la multiplicité de la valeur propre $2$ correspondante est égale à $2$, par conséquent la matrice $A$ n'est pas diagonalisable.

Soit $e_3$ le vecteur de la base canonique $e_3=(0,0, 1)$, les vecteurs $ e_1, e_2, e_3$ forment une base de $E$. La matrice de passage est $$P=\left(\begin{matrix}1&4&0\cr 1&3&0\cr 1&4&1\end{matrix}\right)\ \  {\hbox{et sa matrice inverse est}}\ \ 
P^{-1}=\left(\begin{matrix}-3&4&0\cr 1&-1&0\cr -1&0&1\end{matrix}\right).$$ 
On a $Ae_1=3e_1$ et $Ae_2=2e_2$, il reste à exprimer $Ae_3$ dans la base $(e_1, e_2, e_3)$.


$$Ae_3=Ak=-2i-3j=-2(-3e_1+e_2-e_3)-3(4e_1-e_2)=-6e_1+e_2+2e_3.$$
Ainsi l'endomorphisme qui a pour matrice $A$ dans la base canonique $(i,j,k)$ de $E$ a pour matrice $A'$ dans la base
$(e_1, e_2, e_3)$ où
$$A'=\left(\begin{matrix}3&0&-6\cr 0&2&1\cr 0&0&2\end{matrix}\right).$$
On a $A'=P^{-1}AP$.
\end{exemple}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sous-espaces stables}

\begin{definition}
Soient $V$ un sous-espace vectoriel de $E$ et $u$ un endomorphisme de $E$. 
Le sous-espace vectoriel $V$ est dit stable par $u$ si pour tout vecteur 
$x\in V$, on a $u(x)\in V$.
\end{definition} 

Remarquons que les sous-espaces propres de $u$ sont évidemment stables par $u$.

\begin{proposition}
Si $V$ est un sous-espace vectoriel stable par $u$ alors pour tout polynôme $P\in K[X]$, $V$ est stable 
par $P(u)$.
\end{proposition} 

\begin{proof}
On montre par récurrence sur $n$ que pour tout $x\in V$, $u^{(n)}(x)\in V$, ainsi on a bien $P(u)(x)\in V$. En effet, $P(X)=\sum_{k=0}^n a_k X^k$ est un polynôme et si $u$ est un endomorphisme de $E$, on note $P(u)$ l'endomorphisme
$$P(u)=a_0 \id_E+a_1 u+a_2 u^2+\cdots+a_n u^n$$
qui à un vecteur $x$ associe le vecteur
$$P(u)(x)=a_0 x+a_1 u(x)+a_2 u^2(x)+\cdots+a_n u^n(x),$$
ainsi, si pour tout $x\in V$, $u^{(n)}(x)\in V$, on a bien $P(u)(x)\in V$ car $V$ est un espace vectoriel.

\end{proof}



\begin{proposition}
Soient $u$ et $v$ deux endomorphismes de $E$ tels que $u\circ v=v\circ u$, 
alors $\Ker v$ et $\Im v$ sont stables par $u$.
\end{proposition} 

\begin{proof}
Soit $x\in\Ker v$, on  a $v(x)=0$, d'où $v(u(x))=u(v(x))=0$, donc $u(x)\in\Ker v$.

Soit $y\in\Im v$, il existe $x\in E$ tel que $y=v(x)$, on a alors 
$u(y)=u(v(x))=v(u(x))$ donc $u(y)\in \Im v$.
\end{proof} 



\begin{proposition}
Soit $V$ un sous-espace vectoriel non nul de $E$ stable par $u$, alors le polynôme caractéristique de 
$u_{|V}$ divise $P_u$. De plus, si $W$ est un sous-espace vectoriel non nul supplémentaire de $V$ et stable par $u$ on a
$$P_u=P_{u_{|V}}.P_{u_{|W}}.$$
\end{proposition} 

\begin{proof}
On considère une base de $V$, $(e_1,\dots,e_p)$, et on la complète en une base $(e_1,\dots,e_p,e_{p+1},\dots,e_n)$ de $E$. La matrice de $u$ dans cette base est de la forme 
$$M=\left(\begin{matrix}A & U\cr 0 & B\end{matrix}\right)$$
où $A\in M_p(K)$ est la matrice de $u_{|V}$ dans la base $(e_1,\dots,e_p)$. On a alors
$$\begin{array}{cc}P_u(X)& = \det(M-XI_n)=\det(u-X\id_E)\cr &=\det(A-XI_p)\det(B-XI_{n-p}).\end{array}$$
Or, $\det(A-XI_p)=\det (u_{|V}-X\id_V)=P_{u_{|V}}(X)$, ce qui prouve que $P_{u_{|V}}$ divise $P_u$.

Si maintenant on suppose que les vecteurs $(e_{p+1},\dots,e_n)$ forment une base de $W$ et que $W$ est supposé stable par $u$, alors, la matrice $U$ est nulle et l'on a l'égalité
$$P_u=P_{u_{|V}}.P_{u_{|W}}.$$
\end{proof}





\begin{proposition}
On suppose  $u$ diagonalisable, on note $\lambda_1,\cdots,\lambda_r$ ses valeurs propres
et $E_{\lambda_1},\cdots,E_{\lambda_r}$ les sous-espaces propres correspondants. 
Soit $V$ stable par $u$, alors on a 
$$V=V\cap E_{\lambda_1}\oplus\cdots\oplus V\cap E_{\lambda_r}$$
\end{proposition} 

\begin{proof}
Soit $x\in V$, comme $x\in E=E_{\lambda_1}\oplus\cdots\oplus E_{\lambda_r}$, il existe 
vecteurs $x_1,\dots,x_r$, avec pour $1\leq i\leq r$, $x_i\in E_{\lambda_i}$ tels que $x=x_1+\cdots+x_r$.

Le sous-espace $V$ est stable par $u$, donc également par $P(u)$ pour tout $P\in K[X]$. Pour $1\leq i\leq r$, on note 
$$P_i(X)=\prod_{k=1,k\neq i}^r(\lambda_k-X).$$
On a $P_i(\lambda_j)=0$ si $i\neq j$ et $P_i(\lambda_i)\neq 0$. On peut alors écrire
$$\begin{array}{cc}P_i(u)(x)&=P_i(u)(x_1+\cdots+x_r)\cr &=P_i(\lambda_1)x_1+\cdots+P_i(\lambda_r)x_r\cr&=P_i(\lambda_i)x_i.\end{array}$$
Or, $P_i(u)(x)\in V$ par stabilité, donc $x_i\in V$. Ainsi, pour tout $i$, $1\leq i\leq r$, $x_i\in V\cap E_{\lambda_i}$, d'où le résultat.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Le Théorème de Hamilton-Cayley}

\begin{theoreme}
\begin{enumerate}
  \item Soient $u$ un endomorphisme de $E$ et $P_u$ son polynôme caractéristique, on a
$P_u(u)=0$.
  \item Soient $A$ une matrice carrée d'ordre $n$ et $P_A$ son polynôme caractéristique, on a
$P_A(A)=0$.
\end{enumerate}
\end{theoreme}


Avant de commencer la démonstration, rappelons que si $A$ est la matrice de 
l'endomorphisme $u$ dans la base $(e_1,\dots,e_n)$ de $E$, 
le polynôme $P_A(X)=\det(A-XI_n)$ ne dépend pas de la base, c'est aussi le 
polynôme $P_u(X)=\det(u-X\id_E)$. De même que l'on a défini précédemment un 
polynôme d'endomorphisme, on peut définir un polynôme de matrices, si 
$P(X)=\sum_{k=0}^d a_kX^k$, on a
$P(A)=\sum_{k=0}^d a_kA^k$ où $A^0=I_n$. 

\begin{exemple}
En dimension $2$, posons $$A=\left(\begin{matrix}a&b\cr c&d\end{matrix}\right)\in M_n(\Rr).$$ On a 
$$P_A(X)=X^2-(a+d)X+ad-bc.$$
D'où $$\begin{array}{cc}P_A(A)&=A^2-(a+d)A+(ad-bc)I_2\cr&=\left(\begin{matrix}a^2+bc&ab+bd\cr ac+cd&bc+d^2\end{matrix}\right)-
(a+d)\left(\begin{matrix}a&b\cr c&d\end{matrix}\right)+\left(\begin{matrix}ad-bc&0\cr 0&ad-bc\end{matrix}\right)\cr&
=\left(\begin{matrix}0&0\cr 0&0\end{matrix}\right)\end{array}$$
\end{exemple} 


\begin{proof}
On commence dans un premier temps par supposer que le polynôme $P_A=P_u$ a toutes ses racines dans $K$. Nous démontrons dans ce cas particulier le résultat du théorème par récurrence sur $n$.

Pour $n=1$, $P_A(X)=-X+a$, $A=[a]$, on a bien $P_A(A)=0$.

Pour $n$ arbitrairement fixé, on suppose que pour toute matrice $A'\in M_{n-1}(K)$ telle que $P_{A'}$ ait toutes ses racines dans $K$, on a $P_{A'}(A')=0$.

Soit $A\in M_n(K)$, comme on se place dans le cas où $P_A$ a toutes ses racines dans $K$, la matrice $A$ est trigonalisable. Si $u$ est l'endomorphisme de $E$ de matrice $A$, il existe une base $(e_1,\dots,e_n)$ de $E$ dans laquelle la matrice de $u$ est égale à la matrice $T$ où

$$T=\left(\begin{matrix}t_{11}&t_{12}&\cdots &t_{1n} \cr 0&t_{22} & &\vdots \cr\vdots &\ddots &\ddots& t_{n-1,n}\cr0&\cdots&0&t_{nn}\end{matrix}\right).$$
On a alors
$$P_A(X)=P_u(X)=P_T(X)=(t_{11}-X)\cdots(t_{nn}-X).$$
Soit $F$ le sous-espace vectoriel de $E$ engendré par $(e_1,\dots,e_{n-1})$, comme pour $1\leq i\leq n-1$, $u(e_i)\in F$, le sous-espace $F$ est stable par  $u$. On considère la restriction de $u$ à $F$, notons la $v=u_{|F}$, 
$$\begin{array}{cc}v : F&\longrightarrow F\cr x &\longmapsto v(x)=u(x).\end{array}$$
La matrice de $v$ dans la base $(e_1,\dots,e_{n-1})$ est la matrice triangulaire
$$T'=\left(\begin{matrix}t_{11}&t_{12}&\cdots &t_{1,n-1} \cr 0&t_{22} & &\vdots \cr\vdots &\ddots &\ddots& t_{n-2,n-1}\cr0&\cdots&0&t_{n-1,n-1}\end{matrix}\right).$$
Par hypothèse de récurrence, on a $P_{T'}(T')=0$, ce qui implique $P_v(v)=0$.
Ainsi, pour tout vecteur $x\in F$, on a $P_v(v)(x)=0=P_v(u)(x)$. Or, $P_u(X)=(t_{n,n}-X)P_v(X)$, d'où $P_u(u)(x)=0$ pour tout $x\in F$. Il faut maintenant calculer $P_u(u)(e_n)$ pour conclure.

Compte tenu de la forme de la matrice $T$, il existe un vecteur $y\in F$ tel que $$u(e_n)=t_{n,n}e_n+y.$$
c'est le vecteur $y=t_{1,n}e_1+\cdots+t_{n-1,n}e_{n-1}.$ 

On en déduit que 
$$(t_{n,n}\id_E-u)(e_n)=t_{n,n}e_n-t_{n,n}e_n-y=-y\in F,$$

Or, nous avons vu que 
$$P_u(X)=(t_{n,n}-X)P_v(X)=P_v(X)(t_{n,n}-X),$$
on peut donc écrire
$$P_u(u)(e_n)=P_v(u)\circ(t_{n,n}\id_E-u)(e_n)=P_v(u)(-y)=0$$
car $y\in F$.
Ainsi l'endomorphisme $P_u(u)$ s'annule-t-il sur tous les vecteurs de la base $(e_1,\dots,e_n)$ de $E$, ce qui prouve que $P_u(u)=0$. On a donc également $P_A(A)=0$. En effet, si $P_A(X)=\sum_{k=0}^r a_k X^k$, alors $P_A(X)=\sum_{k=0}^r a_k A^k$ est la matrice de l'endomorphisme $\sum_{k=0}^r a_k u^k$, or, cet endomorphisme est identiquement nul, d'où $P_A(A)=0$.

Il faut maintenant généraliser ce résultat au cas où le polynôme $P_A=P_u$ 
n'a plus nécessairement toutes ses racines dans $K$. Rappelons que le corps $K$ 
est égal à $\Rr$ ou $\Cc$. Si $K=\Cc$, l'affaire est réglée puisque $\Cc$ est 
algébriquement clos. Si $K=\Rr$, c'est un sous-corps de $\Cc$, toute matrice 
$A\in M_n(\Rr)$ peut être considérée comme une matrice de $M_n(\Cc)$. 
Le polynôme $P_A(X)=\det(A-XI_n)$ a toutes ses racines dans $\Cc$, on en 
déduit donc que $P_A(A)=0$, d'après ce qui précède. Mais cette égalité $P_A(A)=0$ 
reste vraie que l'on décompose le polynôme $P_A$ dans $\Cc[X]$ ou dans $\Rr[X]$. 
D'où le résultat.
\end{proof}


\begin{exemple}
Soit $A=\left(\begin{matrix}1&-2\cr1&-1\end{matrix}\right)$, le polynôme caractéristique de $A$ est 
$$P_A(X)=\left|\begin{matrix}1-X&-2\cr 1&-1-X\end{matrix}\right|=(1-X)(-1-X)+2=X^2+1.$$
Ce polynôme n'a pas de racines dans $\Rr$,  la matrice $A$ n'est donc pas trigonalisable sur $\Rr$, par contre elle l'est sur $\Cc$. En effet, on a $P_A(X)=(X-i)(X+i)$ où $i$ désigne la racine de $-1$. La matrice $A$ admet donc deux valeurs propres complexes qui sont $i$ et $-i$. Déterminons les sous-espaces propres associés. On cherche donc les vecteurs $v=(x,y)\in\Cc^2$ tels que $Av=iv$  ou $Av=-iv$.
$$\left(\begin{matrix}1&-2\cr1&-1\end{matrix}\right)\left(\begin{matrix}x\cr y\end{matrix}\right)
=i\left(\begin{matrix}x\cr y\end{matrix}\right)\iff\left\{\begin{array}{cc}x-2y&=ix\cr x-y=iy\end{array}\right.
\iff x=(1+i)y.$$
Le sous-espace $E_i$ est donc la droite vectorielle de $\Cc^2$ engendrée par le vecteur $e_1=(1+i,1)$. 
$$\left(\begin{matrix}1&-2\cr1&-1\end{matrix}\right)\left(\begin{matrix}x\cr y\end{matrix}\right)
=-i\left(\begin{matrix}x\cr y\end{matrix}\right)\iff\left\{\begin{array}{cc}x-2y&=-ix\cr x-y=-iy\end{array}\right.
\iff x=(1-i)y.$$
Le sous-espace $E_{-i}$ est donc la droite vectorielle de $\Cc^2$ engendrée par le vecteur 

$e_2=(1-i,1)$.

La matrice $A$ est équivalente dans $M_2(\Cc)$ à la matrice diagonale
$$D=\left(\begin{matrix}i&0\cr0&-i\end{matrix}\right).$$
On a bien $P_A(A)=P_D(A)=A^2+I_2=(A-iI_2)(A+iI_2)=0$. L'égalité $A^2+I_2=0$ est évidemment vraie dans $M_2(\Rr)$.
\end{exemple}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Polynôme minimal}

Nous venons de démontrer que si $u$ est un endomorphisme et $P_u$ son 
polynôme caractéristique, alors $P_u(u)=0$ (et de même $P_A(A)=0$ pour $A\in M_n(K)$). 
Nous allons démontrer qu'il existe un plus petit polynôme ayant cette propriété.

\begin{proposition}
Soit $u$ un endomorphisme de $E$, il existe un unique polynôme $Q\in K[X]$ 
qui vérifie les deux conditions suivantes :
\begin{itemize}
  \item $Q$ unitaire et $Q(u)=0$.
  \item Si $P\in K[X]$  est tel que $P(u)=0$, alors $Q$ divise $P$.
\end{itemize}
\end{proposition} 

Ce polynôme que nous noterons $Q_u$ est appelé \defi{polynôme minimal} de $u$.

\begin{proof}
Notons $n=\dim E$. Soit $u$ un endomorphisme de $E$ et $P_u$ son polynôme caractéristique. Le polynôme $(-1)^nP_u$ est unitaire et s'annule en $u$ (c'est le théorème de Cayley-Hamilton).
Ainsi, l'ensemble des polynômes unitaires vérifiant $P(u)=0$ n'est pas vide. Choisissons dans cet ensemble un polynôme $Q$ de degré minimal. Il est clair que tout polynôme multiple de $Q$ s'annule également en $u$. En effet, si $P=TQ$, alors $P(u)=T(u)\circ Q(u)=0$.
 
Réciproquement, soit $P\in K[X]$ tel que $P(u)=0$. On effectue la division euclidienne de $P$ par $Q$ et on obtient $P=TQ+R$ avec $\deg R<\deg Q$, on a 
$$P(u)=T(u)\circ Q(u)+R(u)=0,$$
d'où l'on déduit $R(u)=0$. Mais, compte tenu du choix de $Q$, supposé de degré minimal, 
on obtient $R=0$, d'où $P=TQ$ c'est-à-dire $Q$ divise $P$. 
Vérifions l'unicité d'un tel $Q$, supposons qu'il existe $Q_1$ et $Q_2$ 
vérifiant les propriétés de la proposition. Les polynômes $Q_1$ et $Q_2$ 
sont alors unitaires et s'annulent en $u$, on en déduit que $Q_1$ divise 
$Q_2$ et $Q_2$ divise $Q_1$, ce qui prouve leur égalité. 
\end{proof}


Nous allons maintenant démontrer les deux propositions suivantes :


\begin{proposition}
Soit $\lambda\in K$, alors $\lambda$ est valeur propre de $u$ si et seulement si $\lambda$
est racine de son polynôme minimal. 
\end{proposition} 

\begin{proof}
On sait que $Q_u$ divise $P_u$, ainsi, si $\lambda$ est racine du polynôme minimal $Q_u$ on a $$Q_u(\lambda)=0=P_u(\lambda),$$ donc $\lambda$ est  une racine du polynôme caractéristique, c'est-à-dire une valeur propre de $u$. 

Réciproquement, supposons que $\lambda$ soit une valeur propre de $u$, alors, nous avons vu dans une précédente proposition que pour tout polynôme $P$, $P(\lambda)$ est valeur propre de l'endomorphisme $P(u)$, en particulier, si $Q$ est le polynôme minimal, $Q(\lambda)$ est valeur propre de $Q(u)$. Or, $Q(u)$ est l'endomorphisme identiquement nul, il n'admet donc pas d'autre valeur propre que $0$, on a donc nécessairement $Q(\lambda)=0$. 
\end{proof}



Nous arrivons ainsi à une nouvelle condition nécessaire et suffisante de diagonalisation.


\begin{proposition}
L'endomorphisme $u$ est diagonalisable si et seulement si son polynôme minimal a toutes ses
racines dans $K$ et celles-ci sont simples. 
\end{proposition} 

\begin{proof}
Supposons l'endomorphisme $u$ diagonalisable. Notons $\lambda_1,\dots,\lambda_r$ ses valeurs propres distinctes et $Q$ le polynôme ainsi défini
$$Q(X)=(X-\lambda_1)\cdots(X-\lambda_r).$$
Pour $1\leq i\leq r$, on note 
$$P_i(X)={{Q(X)}\over{X-\lambda_i}}=(X-\lambda_1)\cdots(X-\lambda_{i-1})(X-\lambda_{i+1})\cdots(X-\lambda_r).$$
On a alors $Q(X)=P_i(X)(X-\lambda_i)$ d'où $Q(u)=P_i(u)\circ(u-\lambda_i\id_E).$
Ainsi pour tout $x\in E_{\lambda_i}$, on a $Q(u)(x)=0$ car $x\in\Ker(u-\lambda_i\id_E)$. Mais, ceci est vrai pour tout $i$, c'est-à-dire quelque soit la valeur propre $\lambda_i$, et, $u$ est supposée diagonalisable, donc $$E=E_{\lambda_1}\oplus\cdots\oplus E_{\lambda_r},$$
ce qui prouve que pour tout $x\in E$, $Q(u)(x)=0$ donc $Q(u)=0$, mézalors, le polynôme minimal $Q_u$ de $u$ divise $Q$ (par définition), or toute valeur propre de $u$ est racine de $Q_u$ donc $Q_u=Q$ et toutes les racines du polynôme minimal sont dans $K$ et sont simples.

Réciproquement, si toutes les racines de $Q_u$ sont dans $K$ et sont simples, alors, les valeurs propres étant racines, on a 
$$Q_u(X)=(X-\lambda_1)\cdots(X-\lambda_r).$$
Or, $$E=\Ker Q_u(u)=\Ker(u-\lambda_1\id_E)\oplus\cdots\oplus\Ker(u-\lambda_r\id_E).$$

Ceci prouve que $u$ est diagonalisable.

(Cette dernière égalité sera démontrée dans le paragraphe suivant sous le nom de lemme des noyaux.)
\end{proof}


Application à la diagonalisation :
\begin{exemple}
Soit $$A=\left(\begin{matrix}2&1&-1\cr -1&-1&3\cr 0&-1&3\end{matrix}\right),$$
on a $$P_A(X)=\left|\begin{matrix}2-X&1&-1\cr -1&-1-X&3\cr 0&-1&3-X\end{matrix}\right|=(2-X)(1-X)^2.$$
Le polynôme minimal $Q_A$ de $A$ divise $P_A$ il est donc égal à $(X-1)(X-2)$ où $(X-1)^2(X-2)$, or,
$$(A-I_3)(A-2I_3)=\left(\begin{matrix}-1&-1&1\cr 2&2&-2\cr 1&1&-1\end{matrix}\right)\neq 0,$$
ainsi, le polynôme minimal est égal à $(X-1)^2(X-2)$, 
il admet une racine double, par conséquent $A$ n'est diagonalisable 
ni dans $\Rr$ ni dans $\Cc$.  
\end{exemple}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sous-espaces caractéristiques et décomposition de Dunford}

Nous avons vu que dans le cas où $u$ est diagonalisable, 
on a $E=E_{\lambda_1}\oplus\cdots\oplus E_{\lambda_r}$ avec 
$E_{\lambda_i}=\Ker(u-\lambda_i\id_E)$ le sous-espace propre associé 
à la valeur propre $\lambda_i$. Nous allons démontrer que lorsque $u$ 
n'est pas diagonalisable, si son polynôme a toutes ses racines dans 
le corps $K$, on peut écrire
$$E=\Ker(u-\lambda_1\id_E)^{m_1}\oplus\cdots\oplus\Ker(u-\lambda_r\id_E)^{m_r},$$
où $m_i$ est la multiplicité de la valeur propre $\lambda_i$ comme 
racine du polynôme caractéristique de $u$.

Commençons par démontrer le lemme suivant :

\begin{lemme}[Lemme des noyaux]
Soient $P$ et $Q$ des polynômes de $K[X]$, premiers entre eux , alors
$$\Ker P(u)\oplus\Ker Q(u)=\Ker(PQ)(u)$$  
\end{lemme}
 

\begin{proof}
Si $P$ et $Q$ sont premiers entre eux, alors, d'après le théorème de Bézout, il existe des polynômes $U$ et $V$ tels que $UP+VQ=1$. On a donc 
$$U(u)\circ P(u)+V(u)\circ Q(u)=id_E.$$
Ainsi, si $x\in\Ker P(u)\cap\Ker Q(u)$, on a
$$U(u)\circ \underbrace{P(u)(x)}_{=0}+V(u)\circ\underbrace{Q(u)(x)}_{=0}=x,$$
d'où $\Ker P(u)\cap\Ker Q(u)=\{0\}$, ce qui prouve que la somme est directe. Considérons maintenant $x\in\Ker(PQ)(u)$, on a, toujours en raison du théorème de Bézout, 
$$x=\underbrace{U(u)\circ P(u)(x)}_{\in\Ker Q(u)}+\underbrace{V(u)\circ Q(u)(x)}_{\in\Ker P(u)},$$
car, c'est une propriété des polynômes, $Q(u)\circ U(u)\circ P(u)=U(u)\circ P(u)\circ Q(u)$ et $P(u)\circ V(u)\circ Q(u)=V(u)\circ Q(u)\circ P(u)=V(u)\circ P(u)\circ Q(u)$, 
et, on a $P(u)\circ Q(u)(x)=0$ car $x\in\Ker(PQ)(u)$. D'où le résultat.
\end{proof}


Ce résultat se généralise par récurrence sur $n$ au cas d'un produit de $n$ polynômes premiers entre eux. Si $P=Q_1...Q_n$ alors
$$\Ker P(u)=\Ker Q_1(u)\oplus\Ker Q_2(u)\oplus\cdots\oplus\Ker Q_n(u).$$ 

\begin{definition}
Soient $u$ un endomorphisme de $E$, $\lambda$ une valeur propre de $u$ et $m$ sa multiplicité
dans $P_u$, le sous-espace $N=\Ker(u-\lambda\id_E)^m$ est appelé \defi{sous-espace caractéristique} de $u$ pour la 
valeur propre $\lambda$.
\end{definition} 


\begin{proposition}
Soit $u$ un endomorphisme de $E$ tel que $P_u$ ait toutes ses racines dans $K$ ($u$ est trigonalisable),
 notons $P_u(X)=(\lambda_1-X)^{m{_1}}\cdots(\lambda_r-X)^{m{_r}}$ et, pour $1\leq i\leq r$, $N_i$ le sous-espace caractéristique associé à la
valeur propre $\lambda_i$, alors :

\begin{enumerate}
  \item $N_i$ est stable par $u$
  \item $E_{\lambda_i}\subset N_i$
  \item $E=N_1\oplus\cdots\oplus N_r$
  \item $\dim N_i=m_i$ 
\end{enumerate}
\end{proposition} 


\begin{proof}
\begin{enumerate}
  \item Si $x\in N_i$, on a $(u-\lambda_i\id_E)^{m_i}(x)=0$, or 
$$(u-\lambda_i\id_E)^{m_i}\circ u(x)=u\circ(u-\lambda_i\id_E)^{m_i}(x)=0,$$
d'où $u(x)\in N_i$.
  
  \item $E_{\lambda_i}=\Ker(u-\lambda_i\id_E)\subset N_i=\Ker(u-\lambda_i\id_E)^{m_i}$, évident.

  \item C'est le lemme des noyaux.
$$P_u(X)=(\lambda_1-X)^{m_i}\cdots(\lambda_r-X)^{m_r},$$
les polynômes $(\lambda_i-X)^{m_i}$ sont premiers entre eux puisque les valeurs propres sont distinctes. Par récurrence, on obtient
$$\Ker P_u=N_1\oplus\cdots\oplus N_r,$$
or, d'après le théorème de Cayley-Hamilton, on a $P_u(u)=0$, donc $\Ker P_u(u)=E$, d'où le résultat.
 
  \item Notons $v_i=u_{|N_i}$ pour $1\leq i\leq r$. Pour $i\neq j$, $N_i\cap N_j=\{0\}$, or, $E_{\lambda_j}\subset N_j$, donc la seule valeur propre de $v_i$ est $\lambda_i$. On a donc $P_{v_i}(X)=(\lambda_i-X)^{\dim N_i}$, (n'oublions pas que $P_u$, et donc ses diviseurs, ont toutes leur racines dans $K$). Or,
$$P_u=P_{v_1}\cdots P_{v_r}=(\lambda_1-X)^{m_1}\cdots(\lambda_r-X)^{m_r},$$
d'où $\dim N_i=m_i$ pour $1\leq i\leq r$.
\end{enumerate}
\end{proof}




\begin{definition}
On dit que l'endomorphisme $u$ (resp. la matrice $A$) 
est \defi{nilpotent(e)} si il existe $k\in\Nn^*$ tel
que $u^k=0$ (resp. $A^k=0$)
\end{definition} 


\begin{proposition}
Si $u$ est nilpotent, $0$ est son unique valeur propre et on a $$P_u(X)=(-1)^n X^n.$$
\end{proposition} 

\begin{proof}
$A^k=0\Rrightarrow \det A=0$, donc l'endomorphisme $u$ n'est pas bijectif, $\Ker u\neq \{0\}$, ce qui prouve que $0$ est une valeur propre de $u$.

Supposons que $\lambda$ soit une autre valeur propre, il existe $x\neq 0$ tel que $u(x)=\lambda x$, on a donc, par récurrence sur $n$, pour tout $n$, $u^n(x)=\lambda^n x$, or, $u$ est supposé nilpotent, il existe donc $k\in\Nn^*$ tel que $u^k=0$, d'où, si $x$ est vecteur propre pour la valeur propre $\lambda$, $\lambda^k x=0$, ce qui implique $\lambda^k=0$, donc $\lambda =0$. 
Ainsi, $0$ est la seule valeur propre de $u$, on a donc $P_u(X)=(-1)^nX^n$.
\end{proof}


Nous allons démontrer que les endomorphismes nilpotents et 
les endomorphismes diagonalisables permettent de décrire tous les 
endomorphismes trigonalisables (c'est-à-dire ceux dont le polynôme 
caractéristique a toutes ses racines dans $K$).



\begin{theoreme}[Décomposition de Dunford]
Soit $u$ un endomorphisme de $E$ tel que $P_u$ ait toutes ses racines dans $K$. Alors, il existe un
 unique couple $(n,d)$ d'endomorphismes, avec $n$ nilpotent et $d$ diagonalisable  tels que 
$$(i)\  u=n+d\,,\ \ \ (ii)\  n\circ d=d\circ n.$$
\end{theoreme}

Ce que l'on peut encore écrire :

\begin{theoreme}[Décomposition de Dunford]
Pour toute matrice $A\in M_n(K)$, trigonalisable, il existe une 
unique matrice $N$ nilpotente et une unique matrice $D$ diagonalisable telles que 
$$(i)\  A=N+D\,,\ \ \ (ii)\  ND=DN.$$
\end{theoreme} 
 
 
Avant de démontrer ces théorèmes, nous allons démontrer 
deux lemmes dont les résultats nous seront utiles.

\begin{lemme}
Si $u$ est diagonalisable et $V$ est stable par $u$ alors la restriction 
de $u$ à $V$ est diagonalisable.
\end{lemme}
 
\begin{proof}
Notons $v=u_{|V}$ et $\lambda_1,\dots,\lambda_r$ les valeurs propres de $u$. L'endomorphisme $u$ étant supposé diagonalisable, on a 
$$E=E_{\lambda_1}\oplus\dots\oplus E_{\lambda_r},$$
où les $E_{\lambda_i}$ sont les sous-espaces propres de $u$. On a, puisque $V$ est stable par $u$,
$$V=(V\cap E_{\lambda_1})\oplus\dots\oplus (V\cap E_{\lambda_r}).$$
Or, $\Ker(v-\lambda_i\id_V)=V\cap\Ker(u-\lambda_i\id_E)$ donc les valeurs propres de $v$ notées $\mu_1,\dots,\mu_s$ sont dans l'ensemble $\{\lambda_1,\dots,\lambda_r\}$. On a donc $V=V_{\mu_1}\oplus\cdots\oplus V_{\mu_s}$, ce qui prouve que $v$ est diagonalisable.

\end{proof}


\begin{lemme}
Soient $u$ et $v$ deux endomorphismes diagonalisables, on suppose que 
$u\circ v=v\circ u$, alors il existe une base commune de vecteurs propres. 
\end{lemme}


\begin{proof}
Soient $\lambda_1,\dots,\lambda_r$ les valeurs propres de $v$. Notons $V_i=\{x\in E/ v(x)=\lambda_i x\}$. On a alors pour $x\in V_i$,
$$v(u(x))=u(v(x))=u(\lambda_i x)=\lambda_i u(x),$$
donc $u(x)\in V_i$, $V_i$ est stable par $u$.
D'après le lemme 1, la restriction de $u$ à $V_i$ est donc diagonalisable. On considère dans $V_i$ une base ${\cal{B}}_i$ de vecteurs propres de $u$, comme $v$ est diagonalisable, on a 
$$E=\underbrace{V_1}_{{\cal{B}}_1}\oplus\cdots\oplus\underbrace{V_r}_{{\cal{B}}_r},$$
la base ${\cal{B}}={\cal{B}}_1\cup\dots{\cal{B}}_r$ est donc une base formée de vecteurs qui sont des vecteurs propres de $u$ et de $v$.

\end{proof}


 Démonstration du théorème de décomposition de Dunford.
\begin{proof}
Soit $P_u$ le polynôme caractéristique de $u$ qui a toutes ses racines dans $K$, 
$$P_u(X)=\prod_{i=1}^r(\lambda_i-X)^{m_i}.$$

Soient $N_1\dots, N_r$ les sous-espaces caractéristiques, pour $1\leq i\leq r$, on a 
$$N_i=\Ker(u-\lambda_i\id_E)^{m_i}\ {\hbox{et}}\ E=N_1\oplus\cdots\oplus N_r.$$
Nous allons définir les endomorphismes $n$ et $d$ sur chaque $N_i$ de la manière suivante, pour tout $x\in N_i$, on pose 
$$d(x)=\lambda_i x\ {\hbox{et}}\ n(x)=u(x)-d(x).$$
Pour $1\leq i\leq r$, on a  $d_i=d_{|N_i}=\lambda_i\id_{N_i}$, l'espace vectoriel $E$ étant somme directe des $N_i$, les endomorphismes ainsi construits conviennent, c'est ce que nous allons vérifier :


1) Par construction, $d$ est diagonalisable. En effet, $E$ étant somme directe des $N_i$, dans une base de $E$ formée de bases des $N_i$, $1\leq i\leq r$, la matrice de $d$ est diagonale.

2) On pose $n_i=n_{|N_i}=u_{|N_i}-\lambda_i\id_{N_i}$, on a donc $n_i^{m_i}=0$, ainsi, si $m=\sup{m_i}$, puisque $n^m$ s'annule sur chaque $N_i$ alors $n^m=0$, ce qui prouve que $n$ est nilpotent.

3) On vérifie que $d\circ n=n\circ d$, soit $x\in E$, pour $1\leq i\leq r$, il existe $x_i\in N_i$ tels que 
$x=x_1+\cdots+x_r$, on a donc
$$d\circ n(x)=d\circ n(x_1)+\cdots+d\circ n(x_r)=n\circ d(x_1)+\cdots+n\circ d(x_r)=n\circ d(x),$$
en effet, sur chaque $N_i$, $d_{|N_i}=\lambda_i\id_{N_i}$ donc commute avec tout endomorphisme.

4) Il reste à vérifier l'unicité : supposons que l'on ait deux couples $(n,d)$ et $(n',d')$ vérifiant $(i)$ et $(ii)$, on a alors $u=d+n=d'+n'$, d'où $d'\circ u=d'\circ d'+d'\circ n'=u\circ d'$. Ainsi, comme pour tout $i$, $1\leq i\leq r$, $N_i$ est stable par $d'$, pour $x\in N_i$ on a
$$(u-\lambda_i\id)^{m_i}\circ d'(x)=d'\circ(u-\lambda_i\id)^{m_i}(x)=0.$$
Par ailleurs, $d_{|N_i}=\lambda_i\id_{N_i}$, donc $d$ et $d'$ commutent sur $E$. D'après le lemmme $2$, il existe une base commune de vecteurs propres, ainsi $d-d'$ est diagonalisable. Or, $n=u-d$ et $n'=u-d'$ donc, si $d$ et $d'$ commutent, $n$ et $n'$ commutent également. Les endomorphismes $n$ et $n'$ étant nilpotents, $n-n'$ l'est également, en effet, si $p$ et $q$ sont des entiers tels que $n^p=n'^q=0$, alors $(n-n')^{p+q}=0$. Ainsi $d-d'=n-n'$ est un endomorphisme qui est à la fois diagonalisable et nilpotent donc nul 
puisque alors sa seule valeur propre est $0$. On a donc $d-d'=n-n'=0$ ce qui prouve l'unicité. 
\end{proof}

La décomposition de Dunford est utile dans la mesure où elle permet de calculer les puissances d'une matrice, nous verrons plus en détails comment et pourquoi dans le paragraphe suivant. Pour cela il est nécessaire que les matrices $D$ et $N$ commutent car on peut alors utiliser la formule du binôme de Newton :
$$(N+D)^k=\sum_{i=1}^kC_k^iN^iD^{k-i}.$$
A partir d'un certain rang $i$, les matrices $N^i$ sont nulles, quant aux matrices diagonalisables, on peut calculer leurs puissances d'une manière relativement simple. En effet, si $P$ est la matrice de passage qui exprime la base des vecteurs propres de $D$ dans la base canonique, on a pour tout entier $k$, $D^k=P\Delta^kP^{-1}$ et, si $\lambda_1,\dots,\lambda_n$ sont les valeurs propres, on a 
$$\Delta^k=\left(\begin{matrix}\lambda_1^k&0&\cdots &0 \cr 0&\lambda_2^k&\ddots &\vdots \cr\vdots &\ddots &\ddots&0\cr0&\cdots&0&\lambda_n^k\end{matrix}\right).$$
Attention, une matrice triangulaire peut toujours s'écrire 
comme somme d'une matrice diagonale et d'une matrice nilpotente, 
mais celles-ci ne commutent pas en général. 

Etudions un exemple. 

\begin{exemple}
Considérons la matrice 
$$A=\left(\begin{matrix}1&1&1\cr 0&1&1\cr 0&0&2\end{matrix}\right),$$
On peut écrire 
$$A=\underbrace{\left(\begin{matrix}1&0&0\cr 0&1&0\cr 0&0&2\end{matrix}\right)}_{D}+\underbrace{\left(\begin{matrix}0&1&1\cr 0&0&1\cr 0&0&0\end{matrix}\right)}_{N}$$
mais $$DN=\left(\begin{matrix}0&1&1\cr 0&0&1\cr 0&0&0\end{matrix}\right)\neq ND=\left(\begin{matrix}0&1&2\cr 0&0&2\cr 0&0&0\end{matrix}\right).$$
Par contre, puisque $A$ est triangulaire, on peut la décomposer de manière unique suivant la décomposition de Dunford. Le polynôme caractéristique $P_A$ est égal à
$$P_A(X)=\left(\begin{matrix}1-X & 1 & 1\cr 0 & 1-X & 1\cr 0 & 0 & 2-X\end{matrix}\right)=(1-X)^2(2-X).$$
Nous avons donc deux valeurs propres qui sont $1$ et $2$. L'espace vectoriel $E$ s'écrit comme somme directe
$$E=\Ker(A-\id)^2\oplus\Ker(A-2\id).$$
Déterminons les sous-espaces caractéristiques. Le noyau $\Ker(A-2\id)=\{u\in E,\ Au=2u\}$, si $u=(x,y,z)$, on résout
$$\left\{\begin{array}{cc}x+y+z&= 2x\cr y+z&=2y\cr 2z&=2z\end{array}\right.\iff\left\{\begin{array}{cc}-x+y+z&= 0\cr -y+z&=0\end{array}\right.
\iff\left\{\begin{array}{cc}y&= z\cr x&=2z\end{array}\right.$$
Le sous-espace $\Ker(A-2\id)$ est donc la droite vectorielle engendrée par le vecteur $(2,1,1)$.
La matrice $A$ n'est pas diagonalisable, en effet $\Ker(A-\id)=\{u=(x,y,z),\ Au=u\}$, on résout
$$\left\{\begin{array}{cc}x+y+z&=x\cr y+z&=y\cr 2z&=z\end{array}\iff y=z=0 \right.,$$
on obtient la droite vectorielle engendrée par le vecteur $(1,0,0)$ sa dimension n'est pas égale à la multiplicité de la racine. Déterminons $\Ker(A-\id)^2$, 
$$A-\id=\left(\begin{matrix}0&1&1\cr0&0&1\cr0&0&1\end{matrix}\right),\ \ \ \ 
(A-\id)^2=\left(\begin{matrix}0&0&2\cr0&0&1\cr0&0&1\end{matrix}\right).$$
Ainsi $\Ker(A-\id)^2$ est le plan vectoriel engendré par les vecteurs $i=(1,0,0)$ et $j=(0,1,0)$, notons 
$e_1=i$, $e_2=j$ et $e_3=2i+j+k$ où $k=(0,0,1)$. Soient $$N_1=\Ker(A-\id)^2=\Rr i+\Rr j=\Rr e_1+\Rr e_2\ {\hbox{et}}\  
N_2=\Ker(A-2\id)=\Rr e_3.$$ On pose $d(e_1)=e_1$, $d(e_2)=e_2$ et $d(e_3)=2e_3$, d'où $d(i)=i$, $d(j)=j$ et 
$$d(k)=-2e_1-e_2+2e_3=-2i-j+4i+2j+2k=2i+j+2k.$$
On a donc dans la base $(i,j,k)$ :
$$D=\left(\begin{matrix}1&0&2\cr 0&1&1\cr 0&0&2\end{matrix}\right)\ \ {\hbox{et}}\ \ N=\left(\begin{matrix}0&1&-1\cr 0&0&0\cr 0&0&0\end{matrix}\right).$$
On peut regarder les choses sous divers points de vue, dans la base $(e_1,e_2,e_3)$, la matrice de l'endomorphisme $u$ est la matrice $T$ dont les colonnes sont $u(e_1)=u(i)=i=e_1$, $u(e_2)=u(j)=i+j=e_1+e_2$ et $u(e_3)=2u(i)+u(j)+u(k)=2e_3$ car $e_3\in\Ker(u-2\id)$. On a $T=N'+D'$ où 
$$N'=\left(\begin{matrix}0&1&0\cr 0&0&0\cr 0&0&0\end{matrix}\right)\ \ {\hbox{et}}\ \ D'=\left(\begin{matrix}1&0&0\cr 0&1&0\cr 0&0&2\end{matrix}\right),$$
la matrice $D'$ est diagonale et la matrice $N'$ est nilpotente. La matrice de passage $P$ est égale à
$$P=\left(\begin{matrix}1&0&2\cr 0&1&1\cr 0&0&1\end{matrix}\right)\ \ {\hbox{et}}\ \ P^{-1}=\left(\begin{matrix}1&0&-2\cr 0&1&-1\cr 0&0&1\end{matrix}\right),$$
on a $PTP^{-1}=A=PN'P^{-1}+PD'P^{-1}=N+D$. La matrice $N$ est nilpotente et la matrice $D$ est diagonalisable.

On a $ND=PN'P^{-1}PD'P^{-1}=PN'D'P^{-1}=PD'N'P^{-1}=DN$, c'est la décomposition de Dunford.
\end{exemple}


\subsection*{Application à la trigonalisation}

Nous avons démontré dans un chapitre précédent qu'un polynôme 
qui a toutes ses racines dans $\Rr$ est trigonalisable dans $\Rr$. 
Par contre nous avions laissé assez libre le choix de la base de 
trigonalisation, on ne demandait au troisième vecteur de cette base 
que d'être linéairement indépendant des deux premiers (qui sont des vecteurs propres). 
La décomposition de Dunford, que nous venons de voir, permet de choisir la 
base de trigonalisation de manière à obtenir naturellement la décomposition 
de Dunford de la matrice triangulaire obtenue.

Soit $u$ un endomorphisme, on suppose que son polynôme caractéristique a toutes ses racines dans $\Rr$. On écrit
$$P_u(X)=(\lambda_1-X)^{m_1}\dots(\lambda_r-X)^{m_r}.$$
On a $E=N_1\oplus\dots\oplus N_r$, où $N_i=\Ker(\lambda_i-u)^{m_i}$.

On sait que chaque $N_i$ est stable par $u$ et si on note $\mathcal{B}_i$ une base de $N_i$, alors, dans la base de $E$, $\mathcal{B}=\cup\mathcal{B}_i$ la matrice de $u$ est une matrice par blocs
$$A=\left(\begin{matrix}A_1&\ &\ \cr\ &\ddots&\ \cr\ &\ &A_r\end{matrix}\right).$$
 Où $A_i$ est la matrice de $u_i=u_{|_{N_i}}$. On trigonalise alors chaque $A_i$.
 On a $P_{u_i}(X)=(\lambda_i-X)^{m_i}$. On choisit une base de $\Ker(u-\lambda_i\id_E)$ et on a
 $$\Ker(u-\lambda_i\id_E)\subset\Ker(u-\lambda_i\id_E)^2\subset\cdots\subset\Ker(u-\lambda_i\id_E)^{m_i}.$$

\begin{exemple}
Soit $A$ la matrice de l'endomorphisme $u$ suivante
$$A=\left(\begin{matrix}2&1&-1\cr 3&3&-4\cr 3&1&-2\end{matrix}\right)$$
$P_A(X)=-(X+1)(X-2)^2$. Déterminons les sous-espaces caractéristiques.

- $N_{-1}=F=\Ker(u+\id_E)$, c'est le sous-espace propre associé à la valeur propre $-1$.

- $N_2=G=\Ker(u-2\id_E)^2\supset\Ker(u-2\id_E)=E_2$.

L'espace $F$ est la droite vectorielle engendrée par le vecteur $e_1=(0,1,1)$ et l'espace $E_2$ est la droite vectiorielle engendrée par le vecteur $e_2=(1,1,1)$. Si $e_3$ est un vecteur de $G$ indépendant de $e_2$ alors, dans le base $(e_1,e_2,e_3)$ la matrice de $u$ sera de la forme
$$\left(\begin{matrix}-1&0&0\cr 0&2&a\cr 0&0&2\end{matrix}\right).$$
On choisit un vecteur $e_3$ dans $N_2\setminus E_2$. On a 
$$(A-2I)^2=\left(\begin{matrix}0&0&0\cr -9&0&9\cr -9&0&9\end{matrix}\right),$$
d'où $\Ker(A-2I)^2=\{(x,y,z)/\ x=z$, c'est un plan vectoriel. Le vecteur $e_3=(1,0,1)$ est dans $N_2$ mais pas dans $E_2$ est les vecteurs $e_1,e_2,e_3$ forment une base de $E$. On calcule $u(e_3)$ et on identifie
$$u(e_3)=(1,-1,1)=a(1,1,1)+2(1,0,1)$$
d'où $a=-1$.

La matrice de $u$ dans la base $(e_1,e_2,e_3)$ est triangulaire, elle s'écrit  
$$T=\left(\begin{matrix}-1&0&0\cr 0&2&-1\cr 0&0&2\end{matrix}\right)$$
et, si $P$ est la matrice de passage
$$P=\left(\begin{matrix}0&1&1\cr 1&1&0\cr1&1&1\end{matrix}\right)$$
on a $T=P^{-1}AP$.

Cette trigonalisation permet d'obtenir la décomposition de Dunford de $A$ en écrivant
$$T=\underbrace{\left(\begin{matrix}-1&0&0\cr 0&2&0\cr 0&0&2\end{matrix}\right)}_\Delta+\underbrace{\left(\begin{matrix}0&0&0\cr 0&0&-1\cr 0&0&0\end{matrix}\right)}_M.$$
On a $A=PTP^{-1}=\underbrace{P\Delta P^{-1}}_D+\underbrace{PMP^{-1}}_N$. La matrice $D$ est diagonalisable, la matrice $N$ est nilpotente et ce sont 
les matrices de la décomposition de Dunford.
\end{exemple}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Résolution de systèmes différentiels linéaires}


En première année, on apprend à résoudre les équations différentielles du premier ordre
$$x'(t)=a(t)x(t)+b(t)$$
où $x,a$ et $b$ sont des fonctions de la variable réelle $t$ à valeurs 
dans $\Rr$, la fonction $x$ étant la fonction inconnue, et les équations 
différentielles du second ordre à coefficients constants
$$x''(t)+px'(t)+qx(t)=0,$$
où $p$ et $q$ sont des constantes réelles. Nous allons maintenant 
étudier le cas où la fonction inconnue ne prend plus ses valeurs 
dans $\Rr$ mais dans $\Rr^n$. C'est-à-dire que nous étudierons 
les systèmes d'équations 
$$\left\{\begin{array}{cc}x'_1(t)&=a_{1,1}x_1(t)+a_{1,2}x_2(t)+\cdots+a_{1,n}x_n(t)+b_1(t)\cr
\vdots& \cr x'_n(t)&=a_{n,1}x_1(t)+a_{n,2}x_2(t)+\cdots+a_{n,n}x_n(t)+b_n(t)\end{array}\right.$$
que l'on peut encore écrire sous forme matricielle $X'=AX+B$, 
où $A\in M_n(\Rr)$ et $B$ est un vecteur colonne de 
$\Rr^n$, $$A=(a_{i,j})_{1\leq i,j\leq n}\ \ {\hbox{et}}\ \ B
=\left(\begin{matrix}b_1(t)\cr\vdots\cr b_n(t)\end{matrix}\right),$$
les $(b_i(t))_{1\leq i\leq n}$ sont des fonctions de la variable 
réelle $t$ et les $(a_ij)_{1\leq i,j\leq n}$ des réels.


On cherche les solutions de $$X'=AX+B\leqno (\star)$$ où $X$ est une 
application inconnue de $\Rr$ dans $\Rr^n$, $A\in M_n(\Rr)$ et 
$B$ une application de $\Rr$ dans $\Rr^n$ 

Rappelons que, dans le cas où $n=1$, on obtient les solutions de 
l'équation $x'=ax+b(t)\ (*)$ en intégrant d'abord l'équation homogène 
$x'=ax$ dont les solutions sont les fonctions $x(t)=ke^{at}$ où $k$ 
est une constante réelle. On obtient toutes les solutions de $(*)$ 
en ajoutant à la solution générale de l'équation homogène une 
solution particulière de $(*)$ que l'on peut trouver par la méthode 
de variation de la constante.

Dans le cas des systèmes d'équations, on voit immédiatement apparaître 
l'intérêt de la trigonalisation (et évidemment de la diagonalisation) 
qui permettra de résoudre le système de proche en proche. En effet, 
si $A=T$ est triangulaire on a 
$$\left\{\begin{array}{cc}x'_1&=t_{1,1}x_1+\cdots+\cdots+t_{1,n}x_n+b_1\cr x'_2&=\quad \quad \ \ t_{2,2}x_2+\cdots+t_{2,n}x_n+b_2\cr\vdots\cr x'_n&=\quad\quad\quad \quad \quad\quad \ \ t_{n,n}x_n+b_n\end{array}\right.$$
On peut alors intégrer la dernière équation puis reporter 
la solution dans la précédente et ainsi de proche en proche intégrer tout le système.

Si $A=D$ est diagonale, c'est encore plus simple on est ramené 
à la résolution de $n$ équations du type $x'=\lambda x$.

Afin de pouvoir généraliser les méthodes de résolution en 
dimension $1$ aux systèmes, nous allons introduire les 
exponentielles de matrices.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exponentielle de matrices}

L'espace vectoriel $M_n(\Cc)$ est un espace vectoriel de 
dimension finie, $n^2$, sur lequel toutes les normes sont 
équivalentes, on en choisit une que l'on note $|\!|\ |\!|$. 
La série de terme général ${{1}\over{k!}}a^k$ étant convergente 
pour tout $a\in \Rr$, la série de terme général ${{1}\over{k!}}|\!|A^k|\!|$ 
est également convergente pour toute matrice $A\in M_n(\Cc)$. 
Par conséquent, la série $\displaystyle \sum_{k=0}^{+\infty}{{1}\over{k!}}A^k$ 
est convergente dans $M_n(\Cc)$.

{\bf Rappels} : Rappelons la définition d'une série, soit 
$(u_n)_{n\in\Nn}$ une suite de réels, on appelle série de terme 
général $u_n$ la suite $S_n=\sum_{k=0}^{n}u_k$, si cette suite 
admet une limite quand $n$ tend vers l'infini, on dit que la 
série converge et on note $S=\sum_{k=0}^{+\infty}u_k$ sa limite.

Si $x\in\Cc$, on peut définir l'exponentielle de $x$ par 
$$e^x=\lim_{p\rightarrow+\infty}(1+x+{{x^2}\over{2!}}+\dots
+{{x^p}\over{p!}}=\sum_{k=0}^{+\infty}{{1}\over{k!}}x^k.$$ 
De même, nous allons définir l'exponentielle d'une matrice.

 \begin{definition}\end{definition} Soit $A\in M_n(\Cc)$, 
 la matrice définie par
$$\sum_{k=0}^{+\infty}{{1}\over{k!}}A^k
=\lim_{p\rightarrow +\infty}(I_n+A+{{1}\over{2!}}A^2+\cdots+{{1}\over{p!}}A^p)$$
est l'exponentielle de la matrice $A$, on la note $\exp A$.


Remarquons que si $A$ est diagonale
$$A=\left(\begin{matrix}\lambda_1&0&\cdots &0\cr0& \ddots & &\vdots \cr
\vdots & &\ddots&0\cr 0&\cdots&0&\lambda_n\end{matrix}\right)
\ \ {\hbox{et}}\ \ \exp A
=\left(\begin{matrix}e^{\lambda_1}&0&\cdots &0\cr
0& \ddots & &\vdots \cr\vdots & &\ddots&0\cr
0&\cdots&0&e^{\lambda_n}\end{matrix}\right).$$
Lorsque $A$ est nilpotente, la somme définissant $\exp A$ est une somme finie.

L'exponentielle de matrices vérifie les propriétés suivantes :

\begin{proposition} 
\begin{enumerate}
  \item Si $A$ est la matrice nulle, $\exp A=I_n$.
  \item Si $A$ et $B\in M_n(\Cc)$ vérifient $AB=BA$, alors $\exp(A+B)=(\exp A)(\exp B)$.
  \item Pour toute matrice $A\in M_n(\Cc)$, la matrice $\exp A$ est inversible et $(\exp A)^{-1}=\exp(-A)$.
  \item Si $A$ et $P\in M_n(\Cc)$, et $P$ inversible, on a $\exp(P^{-1}AP)=P^{-1}(\exp A) P$.
\end{enumerate} 
\end{proposition}



Nous ne démontrerons pas ces propriétés, nous pouvons cependant faire 
les remarques suivantes :
le 1) est évident, le 2) se démontre comme dans le cas de l'exponentielle 
complexe, le fait que les matrices commutent permet d'utiliser 
la formule du binôme de Newton. Pour le 3), on remarque que les 
matrices $A$ et $-A$ commutent d'où $$\exp(A-A)=\exp 0=I=(\exp A)(\exp(-A)).$$ 
Pour le 4), on note que pour tout $k\in\Nn^*$, on a $P^{-1}A^kP=(P^{-1}AP)^k$ 
et l'on revient à la définition de l'exponentielle
$$\exp(P^{-1}AP)=\sum_{k=0}^{+\infty}{{1}\over{k!}}P^{-1}A^kP
=P^{-1}\left(\sum_{k=0}^{+\infty}{{1}\over{k!}}A^k\right)P.$$

Le calcul de l'exponentielle se fait alors de la manière suivante, 
si $A$ est diagonale ou nilpotente, il n'y a pas de problème, 
sinon on utilise la décomposition de Dunford $A=N+D$ avec $D$ diagonalisable 
et $N$ nilpotente et $ND=DN$, ce qui permet d'écrire $\exp A=\exp N\exp D$. 
La matrice $D$ étant diagonalisable, il existe une matrice $P$ inversible 
telle que $P^{-1}DP$ soit diagonale, $D=P\Delta P^{-1}$ d'où
$$\exp D=\exp(P\Delta P^{-1})=P(\exp\Delta)P^{-1}.$$
On peut donc toujours calculer l'exponentielle d'une matrice à coefficients dans $\Cc$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Systèmes différentiels linéaires}


\begin{definition}
Soient $A\in M_n(\Rr)$, $I\subset\Rr$ un intervalle ouvert, 
$B: I\rightarrow\Rr^n$ une application.
Une \defi{solution} du système $X'=AX+B$, est une fonction $S$ 
définie sur $I$ et à valeurs dans $\Rr^n$, dérivable telle que
pour tout $t\in I$, on ait
$$S'(t)=AS(t)+B(t).$$
C'est-à-dire
$$\left\{\begin{array}{cc}s'_1(t)=&a_{1,1}s_1(t)+\cdots+a_{1,n}s_n(t)+b_1(t)\cr
s'_2(t)=&a_{2,1}s_1(t)+\cdots+a_{2,n}s_n(t)+b_2(t)\cr\vdots\cr s'_n(t)=&a_{n,1}s_1(t)+\cdots+a_{n,n}s_n(t)+b_n(t)\end{array}\right.$$
Si $I=\Rr$ et $B=0$, on dit que le système est \defi{homogène}.
\end{definition} 


Nous allons voir comment généraliser les méthodes de résolution 
étudiées en dimension $1$ et comment utiliser les propriétés 
de l'exponentielle de matrices et la réduction des matrices 
carrées pour écrire les solutions.


\begin{proposition}
Soient $A\in M_n(\Rr)$, $I\subset\Rr$ un intervalle ouvert, $B: I\rightarrow\Rr^n$ une application.
Si $S_0$ est une solution de $X'=AX+B$, alors toute solution de $X'=AX+B$ s'écrit $S+S_0$, où $S$ est solution du 
système homogène $X'=AX$.
\end{proposition}   

\begin{proof}
Soit $S$ telle que $S'(t)=AS(t)$ et $S_0$ telle que $S'_0(t)=AS_0(t)+B(t)$. On a bien $$(S+S_0)'(t)=A(S(t)+S_0(t))+B(t)$$
ce qui prouve que $S+S_0$ est solution de $X'=AX+B$.

Réciproquement, soit $U$ une solution de $X'=AX+B$, alors 
$$\begin{array}{cc}(U-S_0)'(t)&= U'(t)-S'_0(t)\cr &=AU(t)+B(t)-AS_0(t)-B(t)\cr&=A(U-S_0)(t).\end{array}$$
Ainsi, $U-S_0$ est solution du système homogène et on a $U=S_0+(U-S_0)$, ce qui démontre la proposition.
\end{proof} 


%---------------------------------------------------------------
\subsection{Résolution des systèmes homogènes}

Ainsi, comme dans le cas $n=1$, nous allons nous attacher à donner 
une méthode de résolution des systèmes homogènes. Mais, avant de 
démontrer le théorème général, nous allons voir quelques propositions.


\begin{proposition}
Soient $A\in M_n(\Rr)$, $\lambda$ une valeur propre de $A$ et $ V$ un vecteur propre associé. 
Alors, la fonction 
$$\begin{array}{cc}\Rr &\longrightarrow \Rr^n\cr t&\longmapsto e^{\lambda t} V\end{array}$$
est solution de $X'=AX$.
\end{proposition} 

\begin{proof}
Soit $V=\left(\begin{matrix}v_1\cr\vdots\cr v_n\end{matrix}\right)$ 
et $S(t)=e^{\lambda t}\left(\begin{matrix}v_1\cr\vdots\cr v_n\end{matrix}\right)$. 
On a alors
$$S'(t)=\lambda e^{\lambda t} V=e^{\lambda t}(\lambda V)=e^{\lambda t} AV=AS(t).$$
Ce qui prouve que $S$ est bien solution du système homogène $X'=AX$.
\end{proof}


\begin{exemple}
Soit $A=\left(\begin{matrix}3&1\cr -1&1\end{matrix}\right)$, 
on a $P_A(X)=(X-2)^2$, la seule valeur propre de $A$ est donc $\lambda=2$, 
déterminons un vecteur propre : soit $V=(x,y)\in\Rr^2$ tel que $A.V=2V$, 
on a alors $x+y=0$, le vecteur $V=(1,-1)$ est un vecteur propre de $A$ 
et l'application $S(t)=e^{2t}\left(\begin{matrix}1\cr-1\end{matrix}\right)$ 
est une solution du système $S'=AS$.
\end{exemple}


\begin{proposition}
Soit $Y : \Rr\rightarrow\Rr^n$ une fonction. Soient $A$ et 
$P\in M_n(\Rr)$, $P$ inversible, alors,
$PY$ est solution de $X'=AX$ si et seulement si $Y$ est solution de $X'=(P^{-1}AP)X$.
\end{proposition} 

\begin{proof}
On suppose $Y$ dérivable et $PY$ solution de $X'=AX$. On a $(PY)'=PY'$, d'où
$$(PY)'=A(PY)\iff PY'=APY\iff Y'=(P^{-1}AP)Y.$$
\end{proof}





\begin{proposition}
Soit $A\in M_n(\Rr)$, pour $t\in\Rr$, l'application,  de $\Rr$ dans
$M_n(\Rr)$, définie par $R(t)=\exp tA$ est dérivable et on a $R'(t)=A R(t)$.
\end{proposition} 

\begin{proof}
On a $$R(t)=\exp tA=\sum_{k=0}^{+\infty}{{1}\over{k!}}t^kA^k=\sum_{k=0}^{+\infty}R_k(t)$$
où $R_k(t)={{1}\over{k!}}t^kA^k$ et, pour tout $k$, on a 
$$R'_k(t)={{1}\over{(k-1)!}}t^{k-1}A^k=AR_{k-1}(t).$$
Pour des raisons de convergence normale, comme dans le cas des séries de fonctions, on a 
$$R'(t)=\sum_{k=0}^{+\infty}R'_k(t)=\sum_{k=0}^{+\infty}AR_k(t)=AR(t).$$
\end{proof}



On arrive alors au théorème principal.


\begin{theoreme}
Soit $A\in M_n(\Rr)$, les solutions du système différentiel homogène $X'=AX$ sont
les fonctions 
$$\begin{array}{cc}S:\Rr &\longrightarrow \Rr^n\cr t&\longmapsto \exp(tA). V\end{array}$$
où $ V$ est un vecteur de $\Rr^n$.
\end{theoreme} 

\begin{proof}
Notons $E_1,\dots,E_n$ les vecteurs de la base canonique de $\Rr^n$. Soit $i\in\{1,\dots,n\}$, on pose 
$S_i(t)=\exp(tA)E_i$, d'après la proposition précédente, on a $S'_i(t)=AS_i(t)$. Si 
$$V=v_1 E_1+\cdots+ v_nE_n=\left(\begin{matrix}v_1\cr\vdots\cr v_n\end{matrix}\right),$$
on pose
$$S(t)=(\exp(tA))V=\sum_{k=1}^n v_k(\exp(tA))E_k=\sum_{k=1}^n v_kS_k(t),$$
d'où
$$S'(t)=\sum_{k=1}^n v_kS'_k(t)=\sum_{k=1}^n v_kAS_k(t)=A\sum_{k=1}^n v_kS_k(t)=AS(t).$$
Ainsi, la fonction $S$ est bien solution. Il reste à vérifier que toutes les solutions s'écrivent de cette manière.

Soit $S$ une solution de $X'=AX$ et soit 
$$\begin{array}{cc}F: \Rr&\longrightarrow\Rr^n\cr t&\longmapsto \exp(-tA)S(t).\end{array}$$
On a alors
$$F'(t)=-A\exp(-tA)S(t)+\exp(-tA)S'(t) =-A\exp(-tA)S(t)+A\exp(-tA)S(t)=0$$ 
car les matrices $A$ et $\exp(tA)$ commutent. Ceci prouve que l'application $F$ est constante, il existe donc un vecteur $V\in\Rr^n$ tel que pour tout $t\in\Rr$, on ait $F(t)=V$. Ainsi, la solution du système s'écrit
$$S(t)=(\exp tA)V.$$
\end{proof}




{\bf Dans la pratique}, que fait-on ?

Il s'agit d'intégrer l'équation $X'=AX$ dont les solutions s'écrivent $$X(t)=\exp(tA).V,\ V\in\Rr^n.$$ Soit $P$ une matrice inversible telle que la matrice $B=P^{-1}AP$ s'écrive $B=D+N$ avec $D$ diagonale, $N$ nilpotente et $ND=DN$. D'après la proposition 3, $PY$ est solution de $X'=AX$ si et seulement si $Y$ est solution de $X'=(P^{-1}AP)X=BX$. Les solutions  $Y(t)$ sont donc de la forme $$Y(t)=\exp(tB)V,\  V\in\Rr^n.$$
On obtient alors $$PY=X=P\exp(tB)V,\ V\in\Rr^n,$$
or, $\exp(tB)=\exp(tD+tN)=\exp(tD)\exp(tN)$ et les matrices $\exp(tD)$ et $\exp(tN)$ sont faciles à calculer puisque $D$ est diagonale et $N$ ets nilpotente. Ainsi, les solutions de l'équation $X'=AX$ sont de la forme
$$X(t)=P\exp(tD)\exp(tN)V,\ V\in\Rr^n$$
et il est inutile de calculer la matrice $P^{-1}$.



\begin{proposition}
Soit $A\in M_n(\Rr)$, l'ensemble des solutions du système homogène $X'=AX$
est un espace vectoriel de dimension $n$.
\end{proposition} 

\begin{proof}
Nous allons vérifier que c'est un sous-espace vectoriel de l'espace des fonctions de $\Rr$ dans $\Rr^n$.

- Il est clair que la fonction nulle est solution du système homogène.

- Si $S_1$ et $S_2$ sont deux solutions, $a_1$ et $a_2$ deux réels, on a 
$$(a_1S_1+a_2S_2)'=a_1S'_1+a_2S'2=a_1AS_1+a_2AS_2=A(a_1S_1+a_2S_2),$$
l'ensemble des solutions est donc stable par combinaison linéaire.

Notons $\cal S$ l'espace des solutions de $X'=AX$ et soit 
$$\begin{array}{cc}u:\Rr^n&\longrightarrow {\cal S}\cr V&\longmapsto S:t\longmapsto(\exp tA)V.\end{array}$$ 
L'application $u$ est linéaire et $\Ker u=\{0\}$, en effet
$$\Ker u=\{V\in\Rr^n, S\equiv 0\}=\{V\in\Rr^n, (\exp tA)V=0,\  \forall t\in \Rr\}=\{0\}$$
car la matrice $\exp(tA)$ est inversible. D'après le théorème précédent, cette application est surjective puisque toute solution s'écrit $(\exp tA)V$, l'application $u$ est donc bijective, ce qui prouve que $\dim {\cal S}=\dim \Rr^n=n$.

Les colonnes de la matrice $\exp(tA)$ forment une base de l'espace vectoriel des solutions du système différentiel $X'=AX$.
\end{proof}


Voyons un cas particulier 


\begin{proposition}
Soit $A\in M_n(\Rr)$ une matrice diagonalisable sur $\Rr$. Notons 
$( {V_1},\ldots,{V_n})$ une base
 de vecteurs propres et $\lambda_1,\ldots,\lambda_n$ les valeurs propres correspondantes. Alors les fonctions
 $S_i(t)=e^{\lambda_i t}{{V_i}}$, ($1\leq i\leq n$) forment une base de l'espace des solutions du système $X'=AX$.
\end{proposition}

\begin{proof}
Montrons que ces solutions sont linéairement indépendantes, en effet si $a_1,\dots,a_n$ sont des réels tels que 
$$a_1S_1(t)+\cdots+a_nS_n(t)=0,$$
cette égalité étant vraie pour tout $t$, elle est vraie en particulier pour $t=0$ où elle devient
$$a_1V_1+\cdots+a_nV_n=0,$$
ce qui implique $a_1=\dots=a_n=0$ car les $V_i$ forment une base de $\Rr^n$. Comme l'espace des solutions est de dimension $n$, les $S_i$ forment une base. 

Soit $P$ la matrice dont les colonnes sont les vecteurs $V_1,\dots,V_n$. Si $X=PY$ est solution de $X'=AX$, alors $Y$ est solution de $Y'=(P^{-1}AP)Y$ où $P^{-1}AP=D$ est diagonale. On a 
$$\left\{\begin{array}{cc}y'_1&=\lambda_1y_1\cr \vdots\cr y'_n&=\lambda_ny_n\end{array}\right.\ {\hbox{d'où}}\ \ \ 
Y(t)=\left(\begin{matrix}k_1e^{\lambda_1t}\cr \vdots\cr k_ne^{\lambda_nt}\end{matrix}\right)$$
d'où 
$$PY(t)=k_1e^{\lambda_1t}V_1+\cdots+k_ne^{\lambda_nt}V_n=k_1S_1(t)+\cdots+k_nS_n(t).$$
\end{proof}


Comme corollaire du théorème, nous allons démontrer l'unicité de 
la solution sous conditions initiales.

\begin{proposition}
Soient $A\in M_n(\Rr)$, $t_0\in\Rr$ et $X_0\in\Rr^n$. 
Il existe une unique solution $S_0$ du système 
différentiel $X'=AX$ qui vérifie $S_0(t_0)=X_0$, c'est la 
fonction $S_0:\Rr\rightarrow\Rr^n$ définie par 
$$S_0(t)=(\exp(t-t_0)A)X_0.$$
\end{proposition}  

\begin{proof}
On pose 
$$S_0(t)=(\exp(t-t_0)A)X_0=\exp(tA)\exp(-t_0A)X_0=\exp(tA)Y_0.$$
D'après le théorème, $S$ est une solution de $X'=AX$ et $$S_0(t_0)=\exp(0A)X_0=I_nX_0=X_0.$$ 
S'il existe une autre solution $S_1$ de $X'=AX$ vérifiant $S_1(t_0)=X_0$, 
alors on a $$S_1(t)=\exp(tA)V\ {\hbox{et}}\ \  \exp(t_0A)V=X_0,$$ c'est-à-dire 
$V=\exp(-t_0A)X_0$ donc $V=Y_0$ et $S_1(t)=S_0(t)$ pour tout $t\in \Rr$.
\end{proof}


Nous avons donc démontré que l'espace des solutions est un 
espace vectoriel de dimension $n$ et que si l'on impose la condition 
$X(t_0)=X_0$, il y a unicité de la solution.


%---------------------------------------------------------------
\subsection{Cas général, variation de la constante}

On cherche maintenant à obtenir la solution générale du système 
$X'=AX+B$ où $$A\in M_n(\Rr)\ {\hbox{et}}\ \  
B:I\rightarrow \Rr^n.$$
Connaissant la solution générale du système homogène $X'=AX$, 
nous allons chercher une solution du système non homogène.

Soit $(S_1,\ldots,S_n)$ une base de l'espace vectoriel des solutions de 
$X'=AX$, toute solution de ce système s'écrit donc 
$$S(t)=\sum_{i=1}^{n}\alpha_iS_i(t)$$
où les $\alpha_i$, $1\leq i\leq n$ sont des réels. 

Nous allons chercher une solution du système $X'=AX+B$ sous la forme
$$S(t)=\sum_{i=1}^{n}\alpha_i(t)S_i(t)$$
où les fonctions $\alpha_i$ sont des fonctions réelles dérivables sur $I$.
On a alors, pour tout $t\in\Rr$,
$$S'(t)=\sum_{i=1}^{n}\alpha_i(t)S'_i(t)+\sum_{i=1}^{n}\alpha'_i(t)S_i(t)=AS(t)+\sum_{i=1}^{n}\alpha'_i(t)S_i(t)$$
car $S'_i(t)=AS_i(t)$, on identifie alors
$$\sum_{i=1}^{n}\alpha'_i(t)S_i(t)=B(t)$$
Les colonnes de la matrice $R(t)=\exp tA$ forment une base de l'espace des solutions de $X'=AX$, à partir de cette base on cherche
une solution de $X'=AX+B$ sous la forme $S(t)=R(t)F(t)$ où 
$$\begin{array}{cc}F: I&\longrightarrow \Rr^n\cr t&\longmapsto F(t)=\left(\begin{matrix}\alpha_1(t)\cr\vdots\cr\alpha_n(t)\end{matrix}\right)\end{array}$$
on peut écrire sous forme matricielle
$$S'(t)=R'(t)F(t)+R(t)F'(t)=AR(t)F(t)+R(t)F'(t)=AS(t)+R(t)F'(t)$$
ainsi, en identifiant on a 
$$R(t)F'(t)=B(t)$$
ce qui nous donne
$$F'(t)=R^{-1}(t)B(t)=\exp(-tA)B(t)$$
Il ne reste plus qu'à intégrer terme à terme $F'(t)$, 
c'est-à-dire les $\alpha_i(t)$.


%---------------------------------------------------------------
\subsection{Application aux équations différentielles d'ordre $n$}

Nous allons voir, là encore comment des méthodes d'algèbre linéaire 
permettent de résoudre des problèmes d'analyse.

On considère une équation différentielle d'ordre $n$ à coefficients constants
$$y^{(n)}+a_1y^{(n-1)}+\cdots+a_n y=0\leqno{(\star)}$$
où la fonction inconnue est une fonction de $\Rr$ dans $\Rr$.
On introduit les fonctions auxiliaires
$$\left\{\begin{array}{cc}y_1&=y\cr y_2&=y_1'=y'\cr\vdots& \cr y_{n-1}&
=y'_{n-2}=y^{(n-2)}\cr y_n&=y'_{n-1}=y^{(n-1)}\end{array}\right.$$
Pour intégrer l'équation $(\star)$ on intègre le système
$$\left\{\begin{array}{cc}y'_1&=y_2\cr y'_2&=y_3\cr\vdots& \cr y'_{n-1}&
=y_{n}\cr y'_n&=-a_1y_n-a_2y_{n-1}-\cdots-a_ny_1\end{array}\right.$$
c'est-à-dire
$$Y'=AY$$
avec
$$A=\left(\begin{matrix}0&1&\cdots&0\cr 0&0&\cdots&0\cr \vdots&\vdots& &1\cr
-a_n&-a_{n-1}&\cdots&-a_{1}\end{matrix}\right).$$


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exemples en dimension 2}


%---------------------------------------------------------------
\subsection{Systèmes homogènes}

Soit $A\in M_2(\Rr)$. Les solutions du système $X'=AX$ sont des applications 
de $\Rr$ dans $\Rr^2$ qui peuvent être
représentées par des courbes paramétrées. On a
$$X'=AX\iff\left\{\begin{array}{cc}x'(t)&=a_{11}x(t)+a_{12}y(t)\cr
y'(t)&=a_{21}x(t)+a_{22}y(t)\end{array}
\right.$$
Si l'application 
$$\begin{array}{cc}S:\Rr&\longrightarrow\Rr^2\cr t&\longmapsto(x(t),y(t))\end{array}$$ 
est solution de $X'=AX$, l'ensemble $S(\Rr)=\{S(t)=(x(t),y(t))\in\Rr^2,\ t\in\Rr\}$ est 
appelé \defi{trajectoire}. 

Les solutions constantes sont les applications $S(t)=X_0\in\Rr^2$ 
pour tout $t\in \Rr$, on a donc $S'(t)=0$ c'est-à-dire $AX_0=0$. 
Le vecteur $X_0$ est appelé \defi{point d'équilibre} du système, le 
point $(0,0)$ est toujours point d'équilibre.


\begin{proposition}
Les trajectoires du système $X'=AX$ sont disjointes ou confondues. 
\end{proposition} 

\begin{proof}
Soient $S_1$ et $S_2$ deux solutions du système homogène $X'=AX$. On a donc, pour tout $t\in\Rr$, 
$S'_1(t)=AS_1(t)$ et $S'_2(t)=AS_2(t)$. Alors, ou bien $S_1(\Rr)\cap S_2(\Rr)=\emptyset$, ou bien il existe $V\in\Rr^2$ tel que $V\in S_1(\Rr)\cap S_2(\Rr)$, nous allons montrer que dans ce cas, les trajectoires sont confondues, c'est-à-dire que l'on a $S_1(\Rr)=S_2(\Rr)$.

D'après l'étude des systèmes homogènes, on sait que si $S_1$ et $S_2$ sont solutions, il existe des vecteurs $V_1$ et $V_2$ dans $\Rr^2$ tels que, pour tout $t\in\Rr$,
$$S_1(t)=\exp(tA)V_1\ \ {\hbox{et}}\ \ S_2(t)=\exp(tA)V_2.$$
Si $V\in S_1(\Rr)\cap S_2(\Rr)$, alors, il existe $t_1\in\Rr$ et $t_2\in\Rr$ tels que 
$$V=S_1(t_1)=S_2(t_2)=(\exp t_1A)V_1=(\exp t_2A)V_2.$$
Or, $S_2$ est l'unique solution prenant la valeur $V$ en $t_2$, on a donc pour tout $t\in\Rr$,
$$S_2(t)=(\exp(t-t_2)A)V.$$
Ainsi pour tout $t\in\Rr$, on a 
$$S_2(t+t_2-t_1)=(\exp(t-t_1)A)V=(\exp(t-t_1)A)(\exp t_1A)V_1=(\exp tA)V_1=S_1(t).$$
Ce qui prouve que les trajectoires sont confondues, en effet, pour tout $t\in\Rr$, on a $S_1(t)\in S_2(\Rr)$ et 
$S_2(t)\in S_2(\Rr)$.
\end{proof}




%---------------------------------------------------------------
\subsection{\'Equations différentielles du second ordre}

On souhaite intégrer l'équation
$$x''(t)+px'(t)+qx(t)=b(t)$$
où $p$ et $q$ sont des constantes réelles et où la fonction inconnue $x$ 
est une fonction réelle à valeurs réelles et la fonction $b$ est définie 
et continue sur un intervalle $I$ de $\Rr$. Pour cela on pose $y=x'$ et 
on intègre le système
$$\left\{\begin{array}{cc}x'&=y\cr y'&=-qx-py+b(t)\end{array}\right.$$
On est ainsi ramené à l'étude des systèmes du paragraphe $2$.

\begin{proposition}
La fonction $s: I\rightarrow \Rr$ est solution de l'équation différentielle 
$$x''+px'+qx=b$$ si et seulement si l'application $S : I\rightarrow \Rr^2$ définie par 
$$S(t)=\left(\begin{matrix}x(t)\cr y(t)\end{matrix}\right)=\left(\begin{matrix}s(t)\cr s'(t)\end{matrix}\right)$$ est solution du système
$X'=AX+B$ avec
$$A=\left(\begin{matrix}0&1\cr-q&-p\end{matrix}\right)\ \ {\hbox{et}}\ \ 
B=\left(\begin{matrix}0\cr b(t)\end{matrix}\right).$$ 
\end{proposition} 


La démonstration est une vérification immédiate et la matrice $A$ est très simple à étudier.

Son polynôme caractéristique est égal à : $P_A(X)=X^2+pX+q$, 
l'espace vectoriel des solutions de l'équation homogène d'ordre $2$, 
$x''+px'+q=0$ est de dimension $2$, on peut en donner une base en fonction 
des racines du polynôme caractéristique.

\begin{itemize}
  \item Si $\lambda_1$ et $\lambda_2$ sont deux racines distinctes, 
l'espace des solutions de l'équation  homogène est engendré par les fonctions
$$t\mapsto e^{\lambda_1t}\ \ {\hbox{et}}\ \ t\mapsto e^{\lambda_2t}.$$
  
  \item Si $\lambda$ est une racine réelle double il est engendré par
$$t\mapsto e^{\lambda t}\ \ {\hbox{et}}\ \ t\mapsto te^{\lambda t}.$$
  
  \item Si $a+ib$ et $a-ib$ sont deux racines complexes conjuguées, il est engendré par
$$t\mapsto e^{at}\cos bt\ \ {\hbox{et}}\ \ t\mapsto e^{at}\sin bt.$$
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\'Etudes d'exemples}

Nous allons commencer par un exemple de système linéaire avec 
second membre par la méthode de variation des constantes.

\begin{exemple}
Soit  le système
$$\left\{\begin{array}{cc}x'&=3x+y+te^t\cr y'&=-x+y+e^t\end{array}\right. \leqno(\epsilon)$$ 
Notons $\epsilon_0$ le système homogène
$$\left\{\begin{array}{cc}x'&=3x+y\cr y'&=-x+y\end{array}\right.\iff X'=\left(\begin{matrix}3&1\cr-1&1\end{matrix}\right)X=AX\leqno(\epsilon_0)$$ 
Considérons la matrice $A=\left(\begin{matrix}3&1\cr-1&1\end{matrix}\right)$, son polynôme caractéristique est égal à
$$P_A(X)=(3-X)(1-X)+1=X^2-4X+4=(X-2)^2.$$
D'après le théorème de Hamilton-Cayley, on a $P_A(A)=0$, c'est-à-dire $(A-2I_2)^2=0$.
Posons $N=A-2I_2$, c'est une matrice nilpotente et on a : $A=N+2I_2$, comme $N$ et $I_2$ commutent, c'est la décomposition de Dunford.

La solution générale de l'équation homogène s'écrit
$$X(t)=\exp(tA)V$$
où $V$ est un vecteur de $\Rr^2$. Calculons $\exp (tA)$.
$$\exp (tA)=\exp(t(N+2I_2))=\exp(tN+2tI_2)=\exp(tN)\exp(2tI_2)$$
or, $(tN)^2=t^2N^2=0$ d'où
$$\exp (tA)=e^{2t}I_2(I+tN)=e^{2t}\left(\begin{matrix}t+1&t\cr-t&1-t\end{matrix}\right).$$
La solution générale du système $(\epsilon_0)$ s'écrit donc 
$$X(t)=e^{2t}\left(\begin{matrix}t+1&t\cr-t&1-t\end{matrix}\right)\left(\begin{matrix}a\cr b\end{matrix}\right)
=e^{2t}\left(\begin{matrix}a(t+1)+bt\cr-at+b(1-t)\end{matrix}\right)=ae^{2t}\left(\begin{matrix}t+1\cr-t\end{matrix}\right)
+be^{2t}\left(\begin{matrix}t\cr1-t\end{matrix}\right),$$
avec $V=(a,b)\in\Rr^2$. Les applications 
$$t\mapsto e^{2t}\left(\begin{matrix}t+1\cr-t\end{matrix}\right)\ \ {\hbox{et}}\ \ t\mapsto e^{2t}\left(\begin{matrix}t\cr1-t\end{matrix}\right)$$
sont deux solutions linéairement indépendantes, elles forment une base de l'espace des solutions de $(\epsilon_0)$.

On cherche une solution particulière de l'équation complète $(\epsilon)$ en faisant varier le vecteur $V=(a,b)$ c'est-à-dire les constantes $a$ et $b$ que l'on cherche sous forme de fonctions $a(t)$ et $b(t)$. En écrivant les choses sous forme matricielle, on obtient $X(t)=\exp(tA)V(t)$ d'où
$$X'(t)=\underbrace{A\exp(tA)V(t)}_{AX(t)}+\underbrace{\exp(tA)V'(t)}_{B(t)}.$$
Ainsi, on a $V'(t)=\exp(-tA)B(t)$ d'où
$$V'(t)=e^{-2t}\left(\begin{matrix}-t+1&-t\cr t&1+t\end{matrix}\right)\left(\begin{matrix}te^t\cr e^t\end{matrix}\right)
=e^{-t}\left(\begin{matrix}1-t&-t\cr t&1+t\end{matrix}\right)\left(\begin{matrix}t\cr 1\end{matrix}\right)
=e^{-t}\left(\begin{matrix}-t^2\cr t^2+1+t\end{matrix}\right).$$
On cherche donc des fonctions $a(t)$ et $b(t)$ qui vérifient
$$\left\{\begin{array}{cc}a'(t)&=-e^{-t}t^2\cr b'(t)&=e^{-t}(t^2+t+1)\end{array}\right.$$
On les cherche sous la forme $e^{-t}P(t)$ où $P(t)$ est un polynôme de degré $2$ et l'on obtient 
$$a(t)=(t^2+2t+2)e^{-t}\ \ {\hbox{et}}\ \ b(t)=-(t^2+3t+4)e^{-t}.$$ 
Ainsi, la solution générale de l'équation complète s'écrit 
$$X(t)=\underbrace{e^{2t}\left(\alpha\left(\begin{matrix}t+1\cr -t\end{matrix}\right)+\beta\left(\begin{matrix}t\cr 1-t\end{matrix}\right)\right)}_{\rm solution\ de\ (\epsilon_0)}
+\underbrace{e^{2t}\left(a(t)\left(\begin{matrix}t+1\cr -t\end{matrix}\right)+b(t)\left(\begin{matrix}t\cr 1-t\end{matrix}\right)\right)}_{\rm solution\ de\ (\epsilon)}$$
où $\alpha$ et $\beta$ sont des constantes réelles, $a(t)$ et $b(t)$ les fonctions trouvées ci-dessus.
Simplifions la solution particulière de $(\epsilon)$
$$S_1(t)=e^{2t}\left(e^{-t}(t^2+2t+2)\left(\begin{matrix}t+1\cr -t\end{matrix}\right)+
e^{-t}(-t^2-3t-4)\left(\begin{matrix}t\cr 1-t\end{matrix}\right)\right)=e^t\left(\begin{matrix}2\cr -t-4\end{matrix}\right).$$
La solution générale de l'équation $(\epsilon)$ s'écrit donc
$$S(t)=e^t\left(\begin{matrix}2\cr -t-4\end{matrix}\right)+{e^{2t}\left(\alpha\left(\begin{matrix}t+1\cr -t\end{matrix}\right)+\beta\left(\begin{matrix}t\cr 1-t\end{matrix}\right)\right)}$$
$\alpha$ et $\beta$ étant des constantes réelles arbitraires.  
\end{exemple}

Nous allons maintenant étudier quelques exemples simples d'équations 
homogènes dans $\Rr^2$ qui nous amèneront à tracer des trajectoires (courbes paramétrées).


\begin{exemple}
Considérons le système :
$$\left\{\begin{array}{cc}x'&=x\cr y'&=2y\end{array}\right.
\iff\left(\begin{matrix}x'\cr y'\end{matrix}\right)
=\left(\begin{matrix}1&0\cr 0&2\end{matrix}\right)
\left(\begin{matrix}x\cr y\end{matrix}\right)$$
c'est un cas où la matrice $A=\left(\begin{matrix}1&0\cr 0&2\end{matrix}\right)$ 
est diagonale. On résout les deux équations et on obtient
$$\left\{\begin{array}{cc}x(t)&=ae^t\cr y(t)&=be^{2t}\end{array}\right.$$
où $a$ et $b$ sont des constantes réelles. 
\'Etudions plus précisément les trajectoires obtenues selon les 
valeurs des constantes $a$ et $b$.

\begin{itemize}
  \item Si $a=b=0$, la solution du système obtenue est l'application nulle de 
  $\Rr\rightarrow\Rr^2$, sa trajectoire est réduite au point $(0,0)\in\Rr^2$.

  \item Si $a=0$ et $b\neq 0$, alors pour tout $t\in\Rr$, $x(t)=0$ 
  et $y(t)$ est du signe de $b$ ainsi les trajectoires sont les demi-axes $Ox$ et $-Ox$.
  
  \item Si $a\neq0$ et $b\neq0$, alors pour tout $t\in\Rr$, on a 
$\displaystyle y(t)=b\left({x(t)}\over{a}\right)^2$, 
les trajectoires sont donc les courbes d'équation 
$\displaystyle y=b\left({x}\over{a}\right)^2$, c'est-à-dire des branches de paraboles. 
\end{itemize}
\end{exemple}
 

\begin{exemple}
Considérons maintenant le système :
$$\left\{\begin{array}{cc}x'&=0\cr y'&=x\end{array}\right.$$
On a alors, pour tout $t\in\Rr$, $x(t)=k$, $k$ étant une 
constante réelle et donc $y(t)=kt+h$ où $h$ est également une 
constante réelle. Les trajectoires sont les droites $x=k$, 
parcourues  de $y=-\infty$ vers $y=+\infty$ si $k>0$ et dans l'autre sens si $k<0$.
\end{exemple}


\begin{exemple}
Nous allons maintenant étudier le système
$$\left\{\begin{array}{cc}x'&=y\cr y'&=-x\end{array}\right.$$
Avant d'intégrer ce système en appliquant les méthodes du cours, 
on peut remarquer que les applications suivantes sont solution.
$$S_1 :t\longmapsto \left(\begin{matrix}\cos t\cr-\sin t\end{matrix}\right)\ \ {\hbox{et}}\ \ 
S_1 :t\longmapsto \left(\begin{matrix}\sin t\cr \cos t\end{matrix}\right)$$
Vérifions que ces deux solutions sont linéairement indépendantes, 
soient $a$ et $b$ des réels tels que pour tout $t\in\Rr$, 
$$aS_1(t)+bS_2(t)=0$$
alors, pour $t=0$ on a 
$$aS_1(0)+bS_2(0)=a\left(\begin{matrix}1\cr 0\end{matrix}\right)+b\left(\begin{matrix}0\cr 1\end{matrix}\right)
=\left(\begin{matrix}0\cr 0\end{matrix}\right)$$ 
ce qui implique $a=b=0$. 

On a ainsi toutes les solutions du système comme combinaison 
linéaire de cette base de l'espace des solutions.

Si on ne pense pas à ces solutions évidentes, alors on résout le système $X'=AX$ avec 
$A=\left(\begin{matrix}0&1\cr -1&0\end{matrix}\right)$. 
On détermine les racines du polynôme caractéristique de $A$. On a
$$P_A(X)=X^2+1=(X-i)(X+i).$$
ce polynôme n'a pas de racines réelles, par contre il admet deux 
racines distinctes dans $\Cc$, la matrice $A$ est donc diagonalisable 
dans $\Cc$, et c'est cette diagonalisation que nous allons utiliser, 
sachant que les parties réelles et imaginaires des solutions complexes 
sont les solutions réelles recherchées. En effet
si $Z(t)=X(t)+iY(t)$ alors
$$Z'(t)=AZ(t)\Rrightarrow X'(t)+iY'(t)=AX(t)+iAY(t)$$
et, comme $A$ est une matrice à coefficients réels, on obtient par 
identification $X'=AX$ et $Y'=AY$. De plus les solutions complexes 
sont conjuguées puisque le système est à coefficients réels.

On peut diagonaliser $A$ ou plus simplement déterminer les vecteurs 
propres $V_i$ et $V_{-i}$ associés aux valeurs propres $i$ et $-i$, 
on obtient ainsi deux solutions linéairement indépendantes, et la solution générale s'écrit
$$Z(t)=\alpha e^{it}V_i+\beta e^{-it}V_{-i}$$
où $\alpha$ et $\beta$ sont des constantes complexes.
On a
$$AV=iV\iff \left(\begin{matrix}0&1\cr -1&0\end{matrix}\right)\left(\begin{matrix}x\cr y\end{matrix}\right)=\left(\begin{matrix}ix\cr iy\end{matrix}\right)
\iff y=ix $$ 
et
$$AV=-iV\iff \left(\begin{matrix}0&1\cr -1&0\end{matrix}\right)\left(\begin{matrix}x\cr y\end{matrix}\right)=\left(\begin{matrix}-ix\cr -iy\end{matrix}\right)
\iff y=-ix $$
on prend $V_i=\left(\begin{matrix}1\cr i\end{matrix}\right)$ et 
$V_{-i}=\left(\begin{matrix}1\cr -i\end{matrix}\right)$. 
La solution générale complexe s'écrit alors 
$$Z(t)=\alpha e^{it}\left(\begin{matrix}1\cr i\end{matrix}\right)+\beta e^{-it}\left(\begin{matrix}1\cr -i\end{matrix}\right)$$
où $\alpha$ et $\beta$ sont des constantes complexes. Cette solution s'écrit encore
$$Z(t)=(\alpha+\beta)\left(\begin{matrix}\cos t\cr -\sin t\end{matrix}\right)
+i(\alpha-\beta)\left(\begin{matrix}\sin t\cr \cos t\end{matrix}\right).$$ 
La solution générale réelle s'écrit donc 
$$S(t)=a\left(\begin{matrix}\cos t\cr -\sin t\end{matrix}\right)
+b\left(\begin{matrix}\sin t\cr \cos t\end{matrix}\right)$$
où $a$ et $b$ sont des constantes réelles.

Remarquons que comme les solutions complexes sont conjuguées deux à deux, 
une combinaison linéaire de deux solutions complexes indépendantes 
fournit une combinaison linéaire réelle de deux solutions réelles 
indépendantes (et non de quatre).

\'Etudions maintenant la forme des trajectoires, soit $(a,b)\in\Rr^2$ 
considérons la solution $S$ définie par $S(t)=(x(t),y(t))$ et
$$\left\{\begin{array}{cc}x(t)&=a\cos t+ b\sin t\cr y(t)&=-a\sin t+b\cos t\end{array}\right.$$
Alors pour tout $t\in\Rr$ on a,
$$\begin{array}{cc}x^2+y^2&=(a\cos t+ b\sin t)^2+(-a\sin t+b\cos t)^2\cr
&=a^2\cos^2t+b^2\sin^2t+2ab\cos t\sin t+a^2\sin^2t+b^2\cos^2t-2ab\sin t\cos t\cr
&=(a^2+b^2)(\cos^2t+\sin^2t)=a^2+b^2.\end{array}$$
Ce qui prouve que la trajectoire de $S$ est le cercle de centre 
$O=(0,0)$ et de rayon $\sqrt{a^2+b^2}$.  
\end{exemple}


\begin{exemple}
Nous allons maintenant, à partir du système étudié 
dans l'exemple précédent intégrer l'équation du second ordre $x''+x=0$. 
Les fonctions $t\mapsto\sin t$ et $t\mapsto\cos t$ sont deux solutions 
linéairement indépendantes. 

\'Ecrivons l'équation sous forme de système linéaire
$$\left(\begin{matrix}x'\cr x''\end{matrix}\right)=\left(\begin{matrix}0&1\cr-1&0\end{matrix}\right)\left(\begin{matrix}x\cr x'\end{matrix}\right),$$
ou encore $X'=AX$ avec $A=\left(\begin{matrix}0&1\cr-1&0\end{matrix}\right)$ et $X=\left(\begin{matrix}x\cr x'\end{matrix}\right)$.
On retrouve les solutions de notre système précédent via la matrice $R(t)=\exp (tA)$, c'est-à-dire après calculs 
$$R(t)
=\left(\begin{matrix}\cos t&\sin t\cr-\sin t&\cos t\end{matrix}\right)$$
dont les colonnes sont des solutions linéairement indépendantes du système $X'=AX$.
La solution générale s'écrit
$$S(t)=a\left(\begin{matrix}\cos t\cr -\sin t\end{matrix}\right)+b\left(\begin{matrix}\sin t\cr \cos t\end{matrix}\right)$$
avec $(a,b)\in\Rr^2$.

Si l'équation a un second membre, c'est-à-dire, si l'on veut intégrer $x''+x=f(t)$, où $f$ est une fonction réelle continue, les solutions seront cherchées en faisant varier les constantes $a$ et $b$. 
Les fonctions $a(t)$ et $b(t)$ étant obtenue par identification en écrivant
$$\left(\begin{matrix}a'(t)\cr b'(t)\end{matrix}\right)=R(-t)\left(\begin{matrix}0\cr f(t)\end{matrix}\right)$$
c'est-à-dire
$$\left\{\begin{array}{cc}a'(t)&=-(\sin t)f(t)\cr b'(t)&=(\cos t)f(t)\end{array}\right.$$
\end{exemple}


\textbf{Bibliographie}

Ces notes de cours ont été rédigées à l'aide des ouvrages suivants :
\begin{itemize}
  \item F. Liret, D. Martinet, \emph{Algèbre et géométrie, 2$^{\rm e}$ année}, Dunod.
  
  \item J. Lelong-Ferrand, J.-M. Arnaudiès, \emph{Cours de Mathématiques, tome 1 : Algèbre}, Dunod.
 
  \item X. Gourdon, \emph{Algèbre}, ``Les maths en tête'', Ellipses.
  
  \item J.-C. Savioz, \emph{Algèbre linéaire}, Vuibert.
\end{itemize}


\auteurs{
Sandra Delaunay
}


\finchapitre 
\end{document}



  

