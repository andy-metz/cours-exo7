
\documentclass[class=report,crop=false]{standalone}
\usepackage[screen]{../exo7book}


% Commandes spécifiques -- A DISCUTER
\newcommand{\GL}{GL}
\newcommand{\SL}{SL}
\newcommand{\iso}{\simeq}
\newcommand{\exoo}{\emph{(à faire en exercice)}}
\newcommand{\infra}[1]{\setcounter{footnote}{\#1}}
\newcommand{\Hh}{\mathbb{H}}
\newcommand{\End}{End}
\newcommand{\Sp}{Sp}
\newcommand{\soul}[1]{\underline{\#1}}
\newcommand{\Res}[1]{{\left | {}_{#1} \right.}}
\newcommand{\mult}{mult}
\newcommand{\substr}{\subsetneq}%\varsubsetneq\subsetneqq
\newcommand{\sta}{\stackrel}
\newcommand{\sinon}{\mbox{ sinon}} 

\usepackage{cancel}

\begin{document}



%====================================================================
\chapitre{Algèbre linéaire -- 2ème année -- Alexis Tchoudjem}
%====================================================================

\tableofcontents

Dans ce cours $\Kk$ est un corps qui peut être $\Qq,\Rr$ ou $\Cc$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Un peu de théorie des groupes}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Lois de composition}

De manière très générale, {\it faire de l'algèbre} c'est étudier des structures algébriques c'est-à-dire des ensembles où sont définies des opérations.

Une {\it opération}, ou {\it loi de composition}\index{loi de composition}, sur un ensemble $E$  est une application :
\[E \times E \to E .\]

Les éléments de l'ensemble $E$ peuvent être des nombres, des matrices, des fonctions, etc.

Les ensembles de nombres suivants sont des exemples basiques de structures algébriques. Ils sont munis d'au moins deux opérations, l'addition et la multiplication :

\[\Nn, \Zz, \Qq, \Rr, \Rr_+ .\]

{\bf Remarques :} 

--- les opérations d'addition et de multiplication ne sont pas définies sur tous les ensembles de nombres. Par exemple le produit de deux nombres irrationnels n'est pas toujours un nombre irrationnel ;

--- Le produit vectoriel des vecteurs de $\Rr^3$ est un exemple de loi de composition mais non le produit scalaire. 

Rappelons que le produit vectoriel sur $\Rr^3$ est défini ainsi :

\[\Qq x_i,y_j \in \Rr,\, \left(\begin{array}{c}
x_1\\
x_2\\
x_3
\end{array}\right) \wedge \left(\begin{array}{c}
y_1\\
y_2\\
y_3
\end{array}\right) :=\left(\begin{array}{c}
x_2y_3-x_3y_2\\
x_3y_1-x_1y_3\\
x_1y_2-x_2y_1
\end{array}\right)   .\]


{\bf Notations :} Pour une loi de composition sur un ensemble $E$,  la notation fonctionnelle n'est pas très pratique. On utilise plutôt une notation qui ressemble à celle utilisée pour la somme ou le produit de nombres. Par exemple, si :
\[p : E \times E \to E \; (a,b) \mapsto p(a,b)\]
est une loi de composition on notera le plus souvent $ab$ (ou parfois $ a \times b, a \circ b$ ou $ a + b$) le résultat de l'opération $p(a,b)$. Par exemple : $(ab)c=p(p(a,b),c)$.

\subsection{Associativité, commutativité}

\begin{definition}
Soit une loi de composition sur un ensemble $E$ notée multiplicativement : $(a,b) \mapsto ab$. On dit que cette loi est {\it associative}\index{associative} si :
\[(ab)c = a(bc)\]
pour tous $a,b,c \in E$. On dit que cette loi est {\it commutative}\index{commutative} si :
\[ab=ba\]
pour tous $a,b \in E$.
\end{definition}

{\bf Exemples :} les lois d'addition et de multiplications sur les ensembles de nombres $\Qq,\Rr,\Cc,...$ sont associatives et commutatives. La  loi d'addition (coordonnée par coordonnée) sur l'ensemble des vecteurs de $\Rr^n$ est aussi associative et commutative. En revanche, la loi du produit vectoriel sur les vecteurs de $\Rr^3$ n'est ni associative ni commutative. La loi de multiplication des matrices carrées (réelles ou complexes) est une loi associative.

{\bf Notations :} on note souvent $+$ les lois de composition commutatives.

{\bf Remarque :} Si une loi $(a,b) \mapsto ab$ sur un ensemble $E$ est associative, on définit le produit de $n$ éléments de $E$ par récurrence sur $n$ de la façon suivante :
\[a_1...a_n := (a_1...a_{n-1})a_n\]
pour tous $a_1,...a_n \in E$. On a alors :\[a_1...a_n = (a_1...a_i)(a_{i+1}...a_n)\] pour tout $1 \le i \le n$.

{\bf Exemple :}
On note 

[[ICI TABLEAU]]

%$\mathcal{M}_n(\Rr) :=\left\{ {\left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%a_{1,1}\ar@{.}[rrr]\ar@{.}[ddd] & && a_{1,n}\ar@{.}[ddd]\\
%\\
%& &&\\
%a_{n,1} \ar@{.}[rrr]&& & a_{n,n}}}\right)
%}
%\mid a_{i,j} \in \Rr\right\}$ 

l'ensemble des matrices réelles de taille $n \times n$. Si $A=(a_{i,j})_{1 \le i,j \le n} ,B=(b_{i,j})_{1 \le i,j \le n} $ sont des matrices carrées réelles, on pose $AB:=(c_{i,j})_{1 \le i,j \le n}$ où :\[c_{i,j} = \sum_{k=1}^n a_{i,k}b_{k,j} .\]
 
Cette loi est associative, en effet, si $A=(a_{i,j})_{1 \le i,j \le n} ,B=(b_{i,j})_{1 \le i,j \le n} , C= (c_{i,j})_{1 \le i,j \le n}$, alors pour tous $i,j$, le $(i,j)-$ième coefficient de $(AB)C$ est le même que le $(i,j)-$ième coefficient de $A(BC)$ :

\[\sum_{1\le k,l \le n} a_{i,k}b_{k,l}c_{l,j} .\]


\subsection{Identité, éléments inversibles}

\begin{definition}
Soit une loi de composition sur un ensemble $E$ :
\[E \times E \to E\,,(a,b)\mapsto ab .\]
Une {\it identité}\index{identité}, ou un {\it élément neutre}\index{élément neutre}, pour cette loi est un élément $e \in E$ tel que pour tout $a \in E$ :
\[ae=ea=a.\]

\end{definition}

\begin{proposition}
Si une loi de composition sur un ensemble $E$ a un élément neutre, alors cet élément neutre est unique.
\end{proposition}
\begin{proof}
Soient $e,e'$ deux éléments neutres, alors : \[e = ee'= e'.\]
\end{proof}

{\bf Exemples :} 

--- L'élément neutre de l'addition sur l'ensemble des nombres entiers (rationnels, réels, complexes) est $0$. 
--- L'élément neutre de l'addition sur l'ensemble des vecteurs de $\Rr^n$ est le vecteur nul (dont toutes les coordonnées sont $0$). 

--- L'élément neutre de la multiplication sur l'ensemble des nombres entiers (rationnels, réels, complexes) est $1$.


--- Il n'y a pas délément neutre pour la loi du produit vectoriel sur $\Rr^3$.

{\bf Notations :} on note souvent $1$ le neutre d'une loi de composition notée multiplicativement et $0$ le neutre d'une loi de composition notée additivement.


{\bf Exemple :} soit $F$ un ensemble. On note $F^F$ l'ensemble des applications de $F$ dans $F$. Sur cet ensemble $F^F$ on choisit la loi de composition des applications $(f,g) \mapsto f \circ g$. Pour cette loi, l'élément neutre est l'application $\id_F : F \to F\,, x \mapsto x$.

\begin{exercicecours}
Vérifier que le neutre pour la multiplication des matrices (à coefficients entiers, rationnels, réels ou complexes) $n \times n$ est la matrice :
\[  I_n := \left(\begin{array}{ccc}
1 & &\\
& \ddots &\\
&&1
\end{array}\right).\]\end{exercicecours}

\begin{definition}
Soit $E$ un ensemble muni d'une loi de composition (notée multiplicativement) qui a un élément neutre $e$. Un élément $x \in E$ est {\it inversible} \index{inversible} s'il existe un élément $y \in E$ tel que :
\[xy =yx =e \]

et dans ce cas, on dit que $y$ est un inverse de $x$.
Un élément $x \in E$ est {\it inversible à droite} \index{inversible à droite}, resp.\  {\it inversible à gauche}\index{inversible à gauche}, s'il existe un élément $y \in E$ tel que $xy  =e$, resp.\ $yx=e$.

\end{definition}

\begin{exercicecours}
Soit $E$ un ensemble. Considérons la loi de composition des applications sur l'ensemble $E^E$. Soit une application $f : E \to E$. Vérifier les équivalences suivantes :

--- l'application $f$ est injective $\iff f$ est inversible à gauche ;

---  l'application $f$ est surjective $\iff f$ est inversible à droite ;

--- l'application $f$ est bijective $\iff f$ est inversible.

Si de plus $E$ est fini, alors $f$ est inversible à gauche $\iff f$ est inversible à droite $\iff f$ est inversible.
\end{exercicecours}

\begin{proposition}
Soit une loi de composition : $E \times E \to E$, $(a,b) \mapsto ab$ {\bf associative} avec un élément neutre. Si un élément $a $ est inversible, alors il a un unique inverse, que l'on note $a^{-1}$. 
\end{proposition}

{\bf Notation :} Si la loi est notée additivement, on note plutôt l'inverse de $a$ par $-a$.
\begin{exercicecours}
Soit une loi de composition : $E \times E \to E$, $(a,b) \mapsto ab$ {\bf associative} avec un élément neutre. Si un élément $a$ a un inverse à gauche $b$, un inverse à droite $c$, alors $a$ est inversible et $a^{-1}=b=c$ est l'inverse de $a$.
\end{exercicecours}

\og L'inversion renverse la multiplication \fg\ :

\begin{proposition}
Soit une loi de composition : $E \times E \to E$, $(a,b) \mapsto ab$ {\bf associative} avec un élément neutre. Si $a,b$ sont des éléments inversibles, alors le produit $ab$ est aussi inversible et :
\[(ab)^{-1} = b^{-1} a^{-1}\]
\end{proposition}

\begin{proof}
On a : $(ab)b^{-1} a^{-1} =aa^{-1} =1 = b^{-1} a^{-1} ab$.
\end{proof}

{\bf Remarque :} pour une loi non commutative, on évite la notation $\frac{a}{b}$ car elle ne permet pas de distinguer $ab^{-1}$ de $b^{-1} a$.


{\bf Notation :} Si $E$ est un ensemble muni d'une loi associative, on note $a^n:= \underbrace{a...a}_{n\;\mathrm fois}$ pour tout $n $ entier $>0$. S'il existe un élément neutre $e$, on pose $a^0:=e$. Si de plus, l'élément $a$ est inversible, on note $a^{-n} : = (a^{-1})^n$. On vérifie alors que $a^{r+s}=a^ra^s$ et $(a^r)^s = a^{rs}$ pour tous entiers $r,s \in \Zz$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Groupes}

\begin{definition}
Un {\it groupe} \index{groupe} est un ensemble  muni d'une loi de composition {\bf associative} qui a un élément neutre et dont tous les éléments sont  inversibles. Si de plus la loi de composition est commutative, on dit que le groupe est {\it abélien} \index{abélien}.
\end{definition}

{\bf Remarque :} on note souvent avec la même lettre le groupe et l'ensemble de ses éléments.


{\bf Exemples :} \[ (\Zz,+) \mbox{ l'ensemble des nombres entiers muni de l'addition}\]
\[(\Qq,+), \mbox{ l'ensemble des nombres rationnels muni de l'addition}\]
\[\Qq^\times, \mbox{ l'ensemble des nombres rationnels NON NULS  muni de la multiplication}\]
\[(\Rr,+) , \mbox{ l'ensemble des nombres réels muni de l'addition}\]
\[\Rr^\times , \mbox{ l'ensemble des nombres réels NON NULS  muni de la multiplication}\]
\[(\Cc,+), \mbox{ l'ensemble des nombres complexes muni de l'addition}\]
\[\Cc^\times, \mbox{ l'ensemble des nombres complexes NON NULS  muni de la multiplication} \]

sont des exemples de groupes abéliens. 

En revanche, $\Nn$ n'est pas un groupe pour l'addition.

Le groupe trivial est le singleton $\{0\}$ avec la loi évidente. Notons $P$ l'ensemble des nombres pairs et $I$ l'ensemble des nombres impairs. On munit l'ensemble $\{P,I\}$ de la loi suivante :
\[\begin{array}{|c|c|c|}
\hline
+&P&I\\
\hline
P&P&I\\
\hline
I&I&P\\
\hline
\end{array}\]
c'est-à-dire : $P+P := P, P+I := I,etc.$.

On obtient ainsi un groupe abélien à deux éléments. C'est le groupe $\Zz/ 2\Zz$.

{\bf Un exemple non commutatif :} Si $\alpha \in \Rr$, on note $s_\alpha$ la réflexion orthogonale par rapport à la droite du plan qui passe par $0$ et qui fait un angle $\alpha$ avec l'axe des abscisses et on note $r_\alpha$ la rotation de centre $0$ et d'angle $\alpha$. Alors l'ensemble :
\[O_2:=\{s_\alpha,r_\beta \mid \alpha,\beta \in \Rr\}\]
muni de la loi de composition des applications est un groupe non abélien. En effet, $r_\beta s_\alpha = s_{\alpha +\beta/2}$ et $s_\alpha r_\beta = s_{\alpha-\beta/2}$. 

\begin{exercicecours}
L'ensemble des applications affines non constantes de $\Rr$ dans $\Rr$, $x \mapsto ax+b$, $a , b \in \Rr$, $a \neq 0$, est un groupe non abélien pour la loi de composition des applications.
\end{exercicecours}

\begin{proposition}[Loi de simplification]
Soient $a,b,c$ trois éléments d'un groupe $G$. Si $ab=ac$, alors $b=c$. Si $ba = ca$, alors $b=c$.
\end{proposition}

\begin{proof}
Il suffit de multiplier à gauche ou à droite par $a^{-1}$.
\end{proof}


\begin{exercicecours}
Si $E$ est un ensemble, l'ensemble des applications bijectives de $E$ dans $E$ muni de la loi de composition des applications est un groupe.
\end{exercicecours}

\begin{definition}[Groupe symétrique]

On appelle {\it groupe symétrique} d'indice $n$ le groupe des bijections de l'ensemble $\{1,...,n\}$. On le note $S_n$. Ses éléments sont aussi appelés les {\it permutations} de l'ensemble $\{1,...,n\}$.
\end{definition}


\begin{exercicecours}
Vérifier que $S_n$ est un groupe fini ayant $n!$ éléments.
\end{exercicecours}


Si $G$ est un groupe fini, on appelle {\it ordre}\index{ordre d'un groupe} son cardinal.

{\bf Exemples :} --- Si $n=2$, on note $e$ l'identité de $\{1,2\}$ et $\tau$ l'application $\tau : 1 \mapsto 2,\, 2 \mapsto 1$. On a : \[S_2 = \{e,\tau\},\, \tau^2 =e .\]


--- Si $n = 3$, on note $e$ l'identité de $\{1,2,3\}$, $s_1$ la permutation $1 \mapsto 2,\, 2 \mapsto 1,\, 3 \mapsto 3$, $s_2$ la permutation $1 \mapsto 1,\, 2 \mapsto 3,\, 3 \mapsto 2$. On a :
\[s_1s_2 \neq s_2s_1\]
($S_3$ est le plus petit groupe non abélien)
\[s_1^2=s_2^2 =e,(s_1s_2)^3 = e, s_1s_2s_1 =s_2s_1s_2 \]
\[S_3=\{e,s_1,s_2,s_1s_2,s_2s_1,s_1s_2s_1\} .\]

--- Si $A =\left(\begin{array}{cc}
a&b\\
c & d
\end{array}\right)$, on note $\det A := ad-bc$. L'ensemble $G$ des matrices $A$ $2 \times 2$ complexes de déterminant $\det A \neq 0$, muni de la loi de multiplication des matrices est un groupe.


En effet, si $A,B$ sont des matrices $2 \times 2$ complexes telles que $\det A \neq 0$, $\det B \neq 0$, alors $\det (AB) = \det A \det B \neq 0$. Donc le produit des matrices est bien une loi de composition sur $G$. Cette loi est associative car la multiplication des matrices l'est. L'élément $I_2$ est l'élément neutre. Et enfin toute matrice \[A=\left(\begin{array}{cc}
a&b\\
c & d
\end{array}\right) \in G\] est inversible d'inverse :
\[A^{-1} = \left(\begin{array}{cc}
\frac{d}{ad-bc}&\frac{-b}{ad-bc}\\
\frac{-c}{ad-bc} & \frac{a}{ad-bc}
\end{array}\right) .\] 

Ce groupe est noté $\GL_2(\Cc)$. Si on remplace $\Cc$ par $\Rr$, on obtient aussi un groupe noté $\GL_2(\Rr)$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sous-groupes}

\begin{definition}
Soit $G$ un groupe. Une partie $H$ de $G$ est un sous-groupe si les trois conditions suivantes sont vérifiées :

 i) $1 \in H$ ;

ii) $H$ est stable par multiplication ;

iii) $H$ est stable par passage à l'inverse.

\end{definition}

{\bf Remarque : } Si $H$ est un sous-groupe de $G$, alors la loi de composition de $G$ induit une loi de composition sur $H$ (exo) et pour cette loi, $H$ est un groupe. En particulier, pour montrer qu'un ensemble muni d'une loi de composition est un groupe il suffit de montrer que c'est un sous-groupe (ce qui évite d'avoir à vérifier la propriété d'associativité par exemple).


{\bf Exemples :}


--- Un groupe $G$, de neutre $e$, a toujours comme sous-groupe $G$ et $\{e\}$ ;

--- Si $k$ est un entier, l'ensemble des multiples de $k$, noté $k\Zz$ est un sous-groupe de $\Zz$ ;

--- l'ensemble des nombres complexes de module $1$ est un sous-groupe de $\Cc^\times$ ;

--- l'ensemble  $\mathbf{\mu}_n$ des racines complexes $n-$ièmes de l'unité est un sous-groupe fini de $\Cc^\times$ ;

--- l'ensemble des rotations du plan de centre $0$ est un sous-groupe du groupe $O_2$ ;

--- l'ensemble des matrices triangulaires supérieures $\left(\begin{array}{cc}
a & b\\
0 & d
\end{array}\right)$, $a,b,d \in \Cc$, $a,d \neq 0$ est un sous-groupe de $\GL_2(\Cc)$ ;

--- l'ensemble des matrices $\left(\begin{array}{cc}
\pm 1 & 0\\
0&\pm 1
\end{array}\right)$ est un sous-groupe de $\GL_2(\Cc)$ ;


--- Soient \[I:=\left(\begin{array}{cc}
i & 0\\
0 & -i
\end{array}\right)\,,\, J:=\left(\begin{array}{cc}
0 & 1\\
-1 & 0
\end{array}\right)\,,\, K:=\left(\begin{array}{cc}
0 & i\\
i & 0
\end{array}\right) \;,\] l'ensemble $\{\pm I_2,\pm I,\pm J,\pm K\}$ est un sous-groupe de $\GL_2(\Cc)$ d'ordre $8$ noté $Q_8$.

{\bf Notation :} $H \le G$ signifie \og $H$ est un sous-groupe de $G$ \fg . 

\noindent{\bf Les sous-groupes de $\boldmath \Zz$}

\begin{proposition}
Soit $H$ un sous-groupe de $\Zz$ (pour l'addition). Alors, $H=k\Zz$ pour un unique entier $k \ge 0$.
\end{proposition}

\begin{proof}
Soit $k$ le plus petit entier $>0$ appartenant à $H$. il suffit de faire une division euclidienne !
\end{proof}

Soient $a,b$ des entiers non tous deux nuls . L'ensemble :
\[a \Zz + b \Zz :=\left\{ar+bs \mid r,s \in \Zz \right\}\]
est un sous-groupe non nul de $\Zz$. Soit $d$ l'unique entier $> 0$ tel que $a\Zz+b\Zz = d\Zz$. On dit que $d$ est le {\it pgcd} de $a,b$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Groupes cycliques}

\subsection{Les groupes $\mathbf( \Zz/n\Zz,+)$}


Soit $n$ un entier. On pose $\overline{x}:= x+n\Zz := \left\{x+nr \mid r \in \Zz \right\}$ pour tout entier $x$. On appelle classes modulo $n$ ces ensembles $\overline{x}$.

Par exemple, si $n=2$ alors $\overline{0}$ est l'ensemble des nombres pairs et $\overline{1}$ est l'ensemble des nombres impairs.


\begin{proposition}
Pour deux entiers $x,y$, on a l'équivalence :
\[x+n\Zz = y + n\Zz \iff n | x-y .\]
\end{proposition}

{\bf Notation :} pour tous entiers $x,y$, si $x+n\Zz = y +n\Zz$, on note :
\[x=y \mod n .\]

Bien entendu, on a pour tous entiers $x,y,z$ :
\[x =x \mod n\,\,\]
\[ x=y \mod n\implies y=x \mod n\,,\]
\[ x=y \mod n \text{ et } y = z \mod n \implies x=z \mod n\] 

On note $\Zz/n\Zz$ l'ensemble des $x+n\Zz$, $x \in \Zz$. C'est l'ensemble des classes modulo $n$. 

\begin{proposition}
Il y a exactement $n$ classes modulo $n$ ; ce sont :
\[\overline{0},...,\overline{n-1}.\]
\end{proposition}

\begin{proof}
Soit $x$ un entier. Alors $\overline{x} = \overline{r}$ où $r$ est le reste de la division euclidienne de $x$ par $n$.
\end{proof}

On définit une addition sur $\Zz/n\Zz$ par :
\[(x+n\Zz) + (y +n\Zz) := (x+y) +n\Zz .\]

Cette addition est bien définie. En effet, on a :
\begin{proposition}
Si $x=x' \mod n$ et $y=y'\mod n$, alors $x+y = x'+y' \mod n$.
\end{proposition}


{\bf Exemple :} voici la table d'addition du groupe $\Zz/3\Zz$ :

\[\begin{array}{|c|c|c|c|}
\hline
+&0&1&2\\
\hline
0&0&1&2\\
\hline
1&1&2&0\\
\hline
2&2&0&1\\
\hline
\end{array}.\]


{\bf Remarque :} on peut aussi définir une multiplication sur $\Zz/n\Zz$.


Soit $G$ un groupe de neutre $1$. Si $x \in G$, on note :
\[\langle x \rangle := \{...x^{-2},x,^{-1} 1,x,x^2,...\}\]
\[= \{x^m\mid m \in \Zz\} .\]

C'est le plus petit sous-groupe de $G$ contenant $x$ \exoo. 
On appelle ce sous-groupe le sous-groupe engendré par $x$.

\begin{lemme}
Soit $G$ un groupe de neutre $1$. Si $x \in G$, alors l'ensemble des entiers $n \in \Zz$ tels que $x^n =1$ est un sous-groupe de $\Zz$.
\end{lemme}

\begin{proposition}
Soit $G$ un groupe de neutre $1$. Si $x \in G$, alors :

--- soit le sous-groupe engendré par $x$ est infini et dans ce cas les $x^k$ sont deux à deux distincts, $k\in \Zz$ ;

--- soit le sous-groupe engendré par $x$ est d'ordre fini $m$. Dans ce cas, $m$ est le plus petit entier $>0$ tel que $x^m =1$, les éléments $1,...,x^{m-1}$ sont deux à deux distincts et $\langle x \rangle = \left\{1,...,x^{m-1}\right\}$.


\end{proposition}

\begin{definition}
Un groupe fini $G$ d'ordre $n$ de la forme $G=\langle x \rangle$ est appelé un {\it groupe cyclique} d'ordre $n$. On dit que $x$ engendre $G$. Un élément $x$ dans un groupe tel que $\langle x\rangle $ est fini d'ordre $n$ est appelé un élément d'ordre $n$.
\end{definition}

{\bf Exemples : } La matrice $\left(\begin{array}{cc}
1&1\\
-1& 0
\end{array}\right)$ est d'ordre $6$ dans $\GL_2(\Rr)$. 

En effet, $A^2= \left(\begin{array}{cc}
0&1\\
-1& -1
\end{array}\right)$, $A^3 = -I_2$, $A^4 = -A$, $A=-A^2$, $A^6 =I_2$.


En revanche, la matrice $
\left(\begin{array}{cc}
1&1\\
 0&1
\end{array}\right)$ est d'ordre infini car $\left(\begin{array}{cc}
1&1\\
 0&1
\end{array}\right)^n = \left(\begin{array}{cc}
1&n\\
 0&1
\end{array}\right)$.

{\it Remarque :} on peut montrer que si $A$ est une matrice à coefficients entiers d'ordre fini $n$, alors $n =1,2,3$ ou $6$.

\begin{exercicecours}

Le groupe $(\Zz/n\Zz,+)$ est un groupe cyclique d'ordre $n$, engendré par $\overline{k}$ pour tout entier $k$ premier à $n$.
\end{exercicecours}


\begin{exercicecours}
Le groupe  {$ \mathbf{\mu}_n$} est cyclique d'ordre $n$, engendré par $e^{2ik\pi/n}$ pour tout entier $k$ premier $n$.
\end{exercicecours}

Plus généralement, si $G$ est un groupe, on peut parler du sous-groupe de $G$ engendré par une partie $X$ de $G$. On le note $\langle X\rangle$. C'est le sous-groupe formé par les produits d'une cha\^ine finie d'éléments de $X$ et d'inverses d'éléments de $X$.

Si $X=\{x_1,...,x_n\}$ est fini, on note $\langle x_1,...,x_n\rangle$ le groupe engendré par $X$. On dit aussi que $x_1,...,x_n$ engendrent $\langle x_1,...,x_n\rangle$. 

Dans ce cas, on a :
\[\langle x_1,...,x_n\rangle = \{x_{i_1}^{\epsilon_1}...x_{i_k}^{\epsilon_k} \mid k \ge 0, 1 \le i_1,...,i_k \le n, \epsilon_1,...,\epsilon_k = \pm 1\}.\]

{\bf Exemples :} le groupe $S_3$ est engendré par $s_1,s_2$, le groupe $Q_8$ est engendré par $I,J$. 

\begin{exercicecours}
Tout sous-groupe de $\Qq$ de type fini (c'est-à-dire qui peut être engendré par un nombre fini d'éléments) est de la forme $r \Zz$ pour un certain $r \in \Qq$.
\end{exercicecours}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Morphismes de groupes}
\begin{definition}
Soient $G,G'$ deux groupes. Une application $\phi : G \to G'$ est un morphisme de groupes si :\[\phi(ab)=\phi(a)\phi(b)\]
pour tous $a,b \in G$.
\end{definition}


{\bf Exemples :} 

--- l'application $\Cc^\times \to \Cc^\times$, $z \mapsto z^n$ pour un entier $n$,

--- l'exponentielle  $(\Rr,+) \to \Rr^\times$, $x \mapsto \exp x$, 

--- le logarithme  $\Rr^\times \to \Rr$, $x \mapsto \ln |x|$,

--- le déterminant : $\GL_2(\Cc) \to \Cc^\times$, $\left(\begin{array}{cc}
a & b\\
c& d
\end{array}\right) \mapsto ad-bc$,

--- l'application $\Rr \to \Cc^\times$, $x \mapsto \cos x + i \sin x$,

--- l'application $(\Zz,+) \to G$, $n \mapsto x^n$, pour un groupe $G$ et un élément fixé $x $ de $G$, 

--- l'inclusion $H \to G$, $ x \mapsto x$ pour un sous-groupe $H $ d'un groupe $G$

sont des morphismes de groupes.

\begin{proposition}
Si $\phi : G \to G'$ est un morphisme de groupes, alors $\phi(1_G) = 1_{G'}$.
\end{proposition}

\begin{definition}[noyau et image]
Soit $\phi : G \to G'$ un morphisme de groupes. On note \[\Im \phi := \{y \in G' \mid \exists x \in G , y = \phi(x)\} \]
c'est un sous-groupe de $G'$ \exoo, c'est l'image de $\phi$.

On note \[\Ker \phi := \{x \in G \mid \phi(x) =1\}=\phi^{-1}{\{1\}}\]
c'est un sous-groupe de $G$, appelé noyau de $\phi$.
\end{definition}

{\bf Remarque :} plus généralement, si $H'$ est un sous-groupe de $G'$, alors $\phi^{-1}(H')$ est un sous-groupe de $G$.

{\bf Exemples :} 

--- le noyau de $\Rr \to \Cc^\times$, $x \mapsto \cos x +i \sin x$ est $2\pi \Zz$ et son image est le sous-groupe des nombres complexes de module $1$ ;

--- le noyau de $\det : \GL_2(\Cc) \to \Cc^\times$ est le groupe spécial linéaire d'indice $2$, noté $\SL_2(\Cc)$ ;

--- le noyau de $\Rr^\times \to \Rr^\times$, $x \mapsto x^2$ est $\{\pm 1\}$ et son image est le sous-groupe des nombres réels strictement positifs ;

--- si $n $ est un entier $>1$, le noyau du morphisme $\Cc^\times \to \Cc^\times$, $z \mapsto z^n$ est le sous-groupe des racine $n-$ièmes de l'unité $\mathbf{\mu}_n$.

{\bf Remarque :} un morphisme de groupes est injectif $\iff $ son noyau est trivial.


\subsection{Sous-groupes distingués}

Le noyau d'un morphisme de groupes $\phi : G \to G'$ possède une propriété remarquable :

\[\Qq x \in \Ker \phi, \Qq g \in G, \, gxg^{-1} \in \Ker \phi .\]

\begin{definition}
Si $H$ est un sous-groupe de $G$, on dit que $H$ est {\it distingué} dans $G$ si pour tout $g \in G$, tout $h \in H$, $ghg^{-1} \in H$. 
\end{definition}

{\it Le noyau d'un morphisme est toujours distingué.}

{\bf Exemples :} --- Dans un groupe abélien, tous les sous-groupes sont distingués ;

--- le sous-groupe des rotations est distingué dans $O_2$ ;

--- le sous-groupe $\SL_2(\Cc)$ est distingué dans $\GL_2(\Cc)$ ;

--- le sous-groupe des matrices diagonales n'est pas distingué dans $\GL_2(\Cc)$.


\begin{exercicecours}
Soit $D_n$ le sous-groupe de $\GL_2(\Cc)$ engendré par les matrices $\left(\begin{array}{cc}
\zeta&0\\
0&\zeta^{-1}
\end{array}\right)$, $\left(\begin{array}{cc}
0&1\\
1&0
\end{array}\right)$. Montrer que $D_n$ est fini d'ordre $2n$ et qu'il contient un sous-groupe cyclique distingué d'ordre  $n$. C'est le groupe diédral d'ordre $2n$.
\end{exercicecours}



\begin{definition}
Si $G$ est un groupe, on note :\[Z(G):=\{x \in G \mid \Qq g \in G, gx=xg\} .\]

C'est un sous-groupe de $G$ appelé le {\it centre de $G$}\index{centre d'un groupe}.
\end{definition}

{\bf Remarque :} le centre de $G$ est distingué dans $G$.

{\bf Exemple :} le centre de $\GL_2(\Cc)$ est formé des matrices $\left(\begin{array}{cc}
\lambda & 0\\
0&\lambda
\end{array}\right)$, $\lambda \in \Cc$.





\subsection{Isomorphismes}

Soit $U$ l'ensemble des matrices de la forme $\left(\begin{array}{cc}
1 & x\\
0&1
\end{array}\right)$, $x \in \Cc$. On vérifie facilement que $U$ est un sous-groupe de $\GL_2(\Cc)$.

De plus, pour tous $x,y \in \Cc$, on a :
\[\left(\begin{array}{cc}
1 & x\\
0&1
\end{array}\right) \left(\begin{array}{cc}
1 & y\\
0&1
\end{array}\right)=\left(\begin{array}{cc}
1 & x+y\\
0&1
\end{array}\right)\]
donc multiplier de telles matrices revient à faire des additions dans $\Cc$. Plus précisément, l'application bijective :
\[\Cc \to U ,\, x \mapsto \left(\begin{array}{cc}
1 & x\\
0&1
\end{array}\right)\]
est un morphisme de groupes.
 
Si $\phi$ est un morphisme de groupes bijectif alors l'application réciproque $\phi^{-1}$ est aussi un morphisme de groupes.

\begin{definition}
Un isomorphisme entre deux groupes $G$ et  $G'$ est un morphisme bijectif entre $G$ et $G'$. S'il existe un isomorphisme de groupes $\phi : G \to G'$, on dit que $G$ est isomorphe à $G'$ ce qui se note : $G \iso G'$. Si $G=G'$ et si $\phi : G \to G$ est un isomorphisme, on dit que $\phi$ est un automorphisme.
\end{definition}

\begin{exercicecours}
\[(\Rr,+) \iso (\Rr_+^*,\times)\]
\[ (\Zz/n\Zz,+) \iso {\mathbf{\mu}_n}\]
\end{exercicecours}




\begin{proposition}
Tout groupe cyclique d'ordre $n$ est isomorphe à $\Zz/n\Zz$.
\end{proposition}
\begin{proof}
Soit $G$ un groupe cyclique d'ordre $n$ engendré par un élément $x$. Alors, l'application :
\[\Zz/n\Zz\; k +n\Zz \mapsto x^k\]
est bien définie et c'est un isomorphisme de groupes.
\end{proof}


\begin{exercicecours}
Montrer que l'ensemble des matrices $2 \times 2$ réelles inversibles de la forme $\left(\begin{array}{cc}
a & -b\\
b&a
\end{array}\right)$ est un sous-groupe de $\GL_2(\Rr)$ isomorphe à $\Cc^\times$.

Montrer l'ensemble des matrices $2 \times 2$ réelles de la forme $\left(\begin{array}{cc}
\cos \theta & -\sin \theta\\
\sin \theta&\cos \theta
\end{array}\right)$, $\theta \in \Rr$ est un sous-groupe de $\GL_2(\Rr)$ isomorphe à $S^1$ le groupe des nombres complexes de module $1$.
\end{exercicecours}

{\bf Exemples : } l'application $\overline{0} \mapsto \overline{0}$, $\overline{1}\mapsto \overline{2}$, $\overline{2} \mapsto \overline{1}$ est un automorphisme de $\Zz/3\Zz$ ;

--- l'application $A \mapsto {}^tA^{-1}$ est un automorphisme de $\GL_2(\Cc)$ ;

--- si $G$ est un groupe et si $g \in G$ est un élément fixé, l'application $x \mapsto gxg^{-1}$ est un automorphisme de $G$. Un tel automorphisme est appelé un {\it automorphisme intérieur}\index{intérieur (automorphisme)}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classes à gauche et à droite}

Soient $G$ un groupe et $H$ un sous-groupe de $G$.

\begin{definition}
Une classe à gauche est une partie de $G$ de la forme :
\[gH:=\{gh \mid h \in H\}\]

pour un certain $g \in G$.
\end{definition}

Par exemple le sous-groupe $H$ lui-même est une classe à gauche : celle de $1$ (et plus généralement celle de $h$ pour tout $h \in H$).

\begin{proposition}
Si $x,y \in G$, alors $xH = y H \iff y^{-1} x \in H \iff x \in y H \iff y \in xH \iff xH \cap yH \neq \varnothing$. 
\end{proposition}

\begin{corollaire}
Les classes à gauche forment une partition de $G$.
\end{corollaire}

{\bf Notation :} on note $G/H$ l'ensemble des classes à gauche $gH$, $g \in G$.

Le nombre de classes à gauche, s'il est fini, est appelé {\it l'indice} de $H$ dans $G$ \index{indice d'un sous-groupe} et noté $[G:H]$.

{\bf Remarque : } on peut définir aussi des classes à droite. L'application $gH \mapsto Hg$ est une bijection entre l'ensemble des classes à gauche et l'ensemble des classes à droite.

\begin{proposition}
Le sous-groupe $H$ est distingué si et seulement si $gH=Hg$ pour tous $g \in G$.
\end{proposition}

{\bf Remarque :} si $g \in G$ est fixé, alors l'application $H \to gH$, $h \mapsto gh$ est bijective ; donc si $H$ est fini, toutes les classes à gauche ont $|H|$ éléments.

\begin{corollaire}[théorème de Lagrange]
Si $G$ est un groupe fini et si $H$ est un sous-groupe de $G$, alors $|H| $ divise $|G|$.
\end{corollaire}

\begin{proof}
$|G| = |H|[G:H]$.
\end{proof}

{\bf Conséquence : } l'ordre d'un élément divise l'ordre du groupe (dans un groupe fini $G$). En particulier, si $G$ est un groupe fini, $x^{|G|}=1$ pour tout $x \in G$.

{\bf Exemple : }dans $S_3$ tous les éléments ont un ordre qui divise $6$ (en fait, $1,2$ et $3$ sont les seules possibilités).

\begin{exercicecours}
Soit $p$ un nombre premier. Si $G$ est un groupe d'ordre $p$, alors $G$ est cyclique et tout $1 \neq g \in G$ engendre $G$. 
\end{exercicecours}

\begin{proposition}
Si $G$ est cyclique d'ordre $n$, alors pour tout $d |n$ il existe un unique sous-groupe de $G$ d'ordre $d$ et ce sous-groupe est cyclique. De plus tout sous-groupe de $G$ est cyclique.
\end{proposition}

\begin{exercicecours}
Les seuls sous-groupes finis de $\Cc^\times$ sont les groupes de racines de l'unité $\mathbf{\mu}_n$, $n >0$.
\end{exercicecours}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Le groupe symétrique}

L'ensemble des permutations de l'ensemble $\{1,...,n\}$ muni de la loi de compositon des applications est un groupe appelé {\it groupe symétrique} d'indice $n$ et noté $S_n$.

\begin{proposition}
Si $G$ est un groupe fini, alors $G$ est isomorphe à un sous-groupe de $S_n$ pour un $n$ assez grand.
\end{proposition}

\begin{proof}
Soit $G=\{g_1,...,g_n\}$ un groupe fini d'ordre $n$. Soit $g \in G$. Pour tout $i$, il existe un unique entier $1 \le k \le n$ tel que $g_k = gg_i$ ; on pose $\sigma_g(i):=k$. Alors $\sigma_g \in S_n$ et $G \to S_n$, $g \mapsto \sigma_g$ est un morphisme injectif de groupes. 
\end{proof}

{\bf Exemple :} dans $S_4$, la permutation $\left(\begin{array}{cccc}
1&2&3&4\\
1&3&2&4
\end{array}\right)$ est aussi notée $(2,3)$.


Plus généralement si $1 \le i_1,...,i_k\le n$, sont des entiers deux à deux distincts, on note :
\[(i_1,...,i_k)\]
la permutation qui envoie $i_1$ sur $i_2$, $i_2$ sur $i_3$, ..., $i_k$ sur $i_1$ et laisse fixe tous les entiers $j \not\in \{i_1,..,i_k\}$.


On dit qu'une telle permutation est un {\it cycle de longueur $k$} ou un {\it $k-$cycle}\index{cycle}.

Un cycle de longueur $1$ est l'identité. Un cycle de longueur $2$ est aussi appelé une {\it transposition} \index{transposition}.

{\bf Exemple :} dans $S_3$, il y a $3$ transpositions : $(1,2)$, $(2,3)$, $(1,3)$ et $2$ $3-$cycles : $(1,2,3)$ et $(1,3,2)$.

{\bf Remarque :} on a bien sûr : $(1,2,3) = (2,3,1) = (3,1,2)$.

\begin{exercicecours}
Vérifier que dans $S_n$, le cycle $(i_1,...,i_k)$ est un élément d'ordre $k$.
\end{exercicecours}

{\bf Remarque :} pour tout $\sigma \in S_n$, on a :
\[\sigma (i_1,...,i_k) \sigma^{-1} = (\sigma(i_1),...,\sigma(i_k)).\]

\subsection{Décomposition en cycles }

Soit $\sigma \in S_n$ une permutation. Le {\it support} \index{support} de $\sigma$ est l'ensemble des $1 \le x \le n$ tel que $\sigma(x) \neq x$.

\begin{exercicecours}
Vérifier que si $\sigma,\tau \in S_n$ sont des permutations à supports disjoints, alors $\sigma \tau = \tau \sigma$.
\end{exercicecours}

\begin{proposition}
Toute permutation se décompose en un produit de cycles à supports disjoints. Cette décomposition est unique à permutation près de l'ordre des cycles.
\end{proposition}


{\bf Exemple :} soit $\sigma := \left(\begin{array}{ccccc}
1&2&3&4&5\\
5&4&3&2&1
\end{array}\right)$. Alors $\sigma = (1,5)(2,4)$.


\begin{proof}
Soit $\sigma \in S_n$. Si $1 \le k \le n$, on pose :
\[\mathcal{O}_k:=\{\sigma^l (k) \mid l \in \Zz\}.\]

Comme $\{1,...,n\}$ est fini, $\mathcal{O}_k= \{k,...,\sigma^{l-1}(k)\}$ où $l$ est le plus petit entier $>0$ tel que $\sigma^l(k)=k$. On a aussi $l = |\mathcal{O}_k|$.

Si $1 \le k,k' \le n$, alors $\mathcal{O}_{k}=\mathcal{O}_{k'}$ ou $\mathcal{O}_k \cap \mathcal{O}_{k'}= \varnothing$. Les ensembles $\mathcal{O}_k$ forment donc une partition de $\{1,...,n\}$. Soient $k_1,...,k_r$ des entiers tels que les ensembles $\mathcal{O}_{k_i}$ soient deux à deux distincts et tels que :
\[\{1,...,n\} = \mathcal{O}_{k_1} \cup ... \cup \mathcal{O}_{k_r} .\]

Posons $n_i := |\{\mathcal{O}_{k_i}\}|$ pour tout $i$. Alors :
\[\mathcal{O}_{k_i} = \{ k_i,\sigma(k_i),...,\sigma^{n_i-1}(k_i)\} \]
et :
\[\sigma = (k_1,\sigma(k_1),...,\sigma^{n_1-1}(k_1))...(k_r,\sigma(k_r),...,\sigma^{n_r-1}(k_r))\]
\end{proof}

\begin{corollaire}
L'ensemble des transpositions engendre $S_n$.
\end{corollaire}

\begin{proof}
Il suffit de remarquer qu'un cycle est un produit de transpositions :
\[(i_1,...,i_k) = (i_1,i_2)(i_2,i_3)....(i_{k-1}i_k) .\]

\end{proof}

{\bf Remarque :} par exemple : $(1,2,3) =(1,2)(2,3)=(2,3)(1,2)(2,3)(1,2)$. La décomposition en produit de transpositions n'est pas unique, ni le nombre de transpositions mais la parité du nombre de transpositions est unique (voir plus loin ...). Autre exemple : $(1,3)=(1,2)(2,3)(1,2)$.


\begin{corollaire}
Les transpositions $(i,i+1)$, $1 \le i \le n-1$, engendrent $S_n$.
\end{corollaire}

\begin{proof}
Il suffit de vérifier qu'une transposition quelconque est un produit de transpositions de la forme $(i,i+1)$. Or, si $1 \le i < j \le n$, on a :
\[(i,j) = s_is_{i+1}...s_{j-2}s_{j-1}s_{j-2}...s_{i+1}s_i\]
où pour tout $1\le k\le n-1$, $s_k$ est la transposition $(k,k+1)$.
\end{proof}

\begin{exercicecours}
Les transpositions $(1,i)$, $2 \le i \le n$ engendrent $S_n$.
\end{exercicecours}

\subsection{Signature}

Soit $\sigma \in S_n$. Une {\it inversion}\index{inversion} de $\sigma$ est une paire $\{i,j\}$ telle que $i < j$ et $\sigma(i)>\sigma(j)$. 

{\it \og Le nombre d'inversions de $\sigma$ est le nombre de fois où, dans la liste $\sigma(1),...,\sigma(n)$, un entier plus grand appara\^it à gauche d'un plus petit.\fg}


\begin{exemple}
Voici les inversions et les signatures des $6$ permutations $\sigma \in \mathcal{S}_3$ :

[[ICI TABLEAU]]

%\[\begin{array}{|c|c|c|} 
%\hline
%\sigma & I(\sigma)\atop \# I(\sigma) & \epsilon (\sigma)\\
%\hline
%\id : \left\{\raisebox{0.5\depth}{%
%\xymatrixcolsep{3ex}%
%       \xymatrixrowsep{1ex}% 
%\xymatrix{
%1 \ar@{|->}[r] & 1\\
%2 \ar@{|->}[r]& 2\\
%3 \ar@{|->}[r] & 3
%}
%}\right.&\varnothing \atop 0& 1\\
%\hline
%
%s_1 : \left\{\raisebox{0.5\depth}{%
%\xymatrixcolsep{3ex}%
%       \xymatrixrowsep{1ex}% 
%\xymatrix{
%1 \ar@{|->}[rd] & 1\\
%2 \ar@{|->}[ru]& 2\\
%3 \ar@{|->}[r] & 3
%}
%}\right.&\{\{1,2\}\}\atop 1& -1 \\
%\hline
%s_2 : \left\{\raisebox{0.5\depth}{%
%\xymatrixcolsep{3ex}%
%       \xymatrixrowsep{1ex}% 
%\xymatrix{
%1 \ar@{|->}[r] & 1\\
%2 \ar@{|->}[rd]& 2\\
%3 \ar@{|->}[ru] & 3
%}
%}\right.& \{\{2,3\}\} \atop 1 & -1 \\
%\hline
%s_1s_2 : \left\{\raisebox{0.5\depth}{%
%\xymatrixcolsep{3ex}%
%       \xymatrixrowsep{1ex}% 
%\xymatrix{
%1 \ar@{|->}[rd] & 1\\
%2 \ar@{|->}[rd]& 2\\
%3 \ar@{|->}[ruu] & 3
%}
%}\right.&\{\{2,3\},\{1,3\}\}\atop 2&1\\
%\hline
%s_2s_1 : \left\{\raisebox{0.5\depth}{%
%\xymatrixcolsep{3ex}%
%       \xymatrixrowsep{1ex}% 
%\xymatrix{
%1 \ar@{|->}[rdd] & 1\\
%2 \ar@{|->}[ru]& 2\\
%3 \ar@{|->}[ru] & 3
%}
%}\right.&\{\{1,2\},\{1,3\}\} \atop 2&1\\
%\hline
%{s_1s_2s_1 \atop (= s_2s_1s_2) }: \left\{\raisebox{0.5\depth}{%
%\xymatrixcolsep{4ex}%
%       \xymatrixrowsep{1ex}% 
%\xymatrix{
%1 \ar@{|->}@/_/[rdd] & 1\\
%2 \ar@{|->}[r]& 2\\
%3 \ar@{|->}[ruu] & 3
%}
%}\right.&\{\{1,2\},\{2,3\},\{1,3\}\} \atop 3& -1 \\
%\hline
%\end{array}\]
\end{exemple}


{\it Le nombre d'inversions d'une permutation est aussi le nombre de \og croisements \fg\ dans le diagramme qui la représente}.



{\bf Exemple :} les inversions de la transposition $(r,s)$, $r<s$, sont les paires :
\[\{r,r+1\},...,\{r,s-1\},\{r,s\},\{r+1,s\},...,\{s-1,s\}\]

ce qui fait $2(s-r)-1$ inversions.

\begin{definition}
Si $\sigma$ est une permutation avec un nombre pair (resp.\ impair) d'inversions, on dit que c'est une permutation paire (resp.\ impaire). On définit $\epsilon(\sigma) := (-1)^{{\mathrm nombre \,d'inversions\, de\, } \sigma}$ ; c'est la {\it signature}\index{signature} de $\sigma$. 
\end{definition}

{\bf Exemple :} les transpositions sont de signature $-1$.

\begin{lemme}
Soit $\sigma \in S_n$ et soit $\tau$ une transposition. Alors $\epsilon(\sigma\tau) = \epsilon(\tau \sigma) = -\epsilon(\sigma)$.
\end{lemme}


\begin{proof}[du lemme]


Soient $I(\sigma)$ l'ensemble des inversions de $\sigma$. On a une bijection :

\[\left(I( \sigma\tau) \setminus I(\sigma) \right) \cup  \left(I(\sigma) \setminus I(\sigma\tau) \right)\stackrel{1:1}{\longleftrightarrow} I(\sigma \tau \sigma^{-1})\]

\[\{i,j\} \mapsto \{\sigma(i),\sigma(j)\}\]
\exoo.

Or, $\sigma\tau \sigma^{-1}$ est une transposition : c'est la transposition qui échange $\sigma(i)$ et $\sigma(j)$. Donc $|I(\sigma\tau\sigma^{-1})|$ est impair. 
Or :

\[|I(\sigma)| = |I(\sigma) \cap I(\sigma\tau)| + |I(\sigma) \setminus I(\sigma\tau)|\]
\[|I(\sigma\tau)| = |I(\sigma) \cap I(\sigma\tau)| + |I(\sigma\tau) \setminus I(\sigma)|\]
\[\implies |I(\sigma)| + |I(\sigma\tau)| = 2 |I(\sigma) \cap I(\sigma\tau)| + |I(\sigma) \setminus I(\sigma\tau)| + |I(\sigma\tau) \setminus I(\sigma)|\]
\[\implies |I(\sigma)| + |I(\sigma\tau)| = 2 |I(\sigma) \cap I(\sigma\tau)|  + |I(\sigma\tau\sigma^{-1})|\]
et $|I(\sigma)| + |I(\sigma\tau)|$ est impair. Donc $\epsilon(\sigma) = - \epsilon(\sigma\tau)$.
\end{proof}


\begin{exercicecours}
On note $s_i$ la transposition $(i,i+1)$. Redémontrer le lemme en montrant que si $\sigma(i) < \sigma(i+1)$, alors :

\[I(\sigma s_i) = \left\{\{s_i(k),s_i(l)\} \mid \{k,l\} \in I(\sigma) \right\} \cup \{i,i+1\} .\]
\end{exercicecours}

En particulier si $\sigma$ est un produit de $p$ transpositions, $\epsilon(\sigma) = (-1)^p$. On en déduit donc qu'un $k-$cycle a pour signature $(-1)^{k-1}$.


\begin{proposition}
La signature  $\epsilon : S_n \to \{\pm 1\}$ est un morphisme de groupes.
\end{proposition}




\begin{definition}
L'ensemble des permutations paires est le noyau de la signature. C'est donc un sous-groupe distingué de $S_n$. On le note $A_n$ : c'est le groupe alterné d'indice $n$. \index{groupe alterné}.
\end{definition}

{\bf Notation :} $A_n :=\Ker \epsilon$.

{\bf Exemple :} le groupe $A_3$ a trois éléments : $1,(1,2,3), (1,3,2)$.


\begin{exercicecours}
Si $n \ge 3$, le groupe $A_n$ est engendré par les $3-$cycles $(1,2,i)$, $3 \le i \le n$.
\end{exercicecours}

Comme $\epsilon$ est un morphisme, on en déduit que $\epsilon(\sigma) = \epsilon(\sigma^{-1})$ pour toute permutation $\sigma$.

En fait on a même plus :
\begin{exercicecours}
Vérifier qu'il y a une bijection entre $I(\sigma)$ et $I(\sigma^{-1})$. 
\end{exercicecours}

\begin{proposition}
La signature est le seul morphisme non trivial de $S_n$ vers $\Cc^\times$.
\end{proposition}

\begin{proof}
Une transposition est d'ordre $2$ et donc son image par un morphisme dans $\Cc^\times$ est $\pm 1$. De plus, toutes les transpositions sont conjuguées et engendrent $S_n$.

Remarque, en général : On dit que deux éléments $x$ et $y$ d'un groupe $G$ sont {\it conjugués} \index{conjugué} s'il existe $g \in G$ tel que $y = gxg^{-1}$.

\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Rappels sur les matrices}

\begin{definition}
Une {\it matrice} $m \times n$ à coefficients dans $\Kk$ est un \og tableau \fg\ de nombres $\in \Kk$  à $m$ lignes et $n$ colonnes. Notation : $\mathcal{M}_{m,n}(\Kk)$.
\end{definition}

{\bf Notation :} On note $a_{i,j}$ le coefficient de la ligne $i$ et de la colonne $j$.

\begin{exemple}
{\it Triangles de Pascal}\index{Pascal}. Voici $3$ exemples de matrices de taille $n+1 \times n+1$ :

\[T_- := \left(\begin{array}{ccccccc}
1&0&...&&&&0\\
1 & 1 &\ddots&&&&\vdots\\
1 & 2 & 1&&&&\\
1&3 & 3 &  1&&&\\
1 & 4 & 6 & 4 & 1 &&\\
\vdots &&&&&\ddots&0\\
1 & n & ... & & &&1
\end{array}\right) = \left({i \choose j}\right)_{0 \le i,j \le n}\]
\[T_+:= \left(\begin{array}{ccccccc}
1&1&1&1&1&...&1\\
0&1 & 2 &3 &4&&n\\
\vdots & &1 & 3&6&&\vdots\\
& & &1 &4 &&\\
&&&&1&&\\
&&&&&\ddots&\\
0 &...&&&&&1
\end{array}\right) = \left({j \choose i}\right)_{0 \le i,j \le n}\]

\[P:= \left(\begin{array}{ccccccc}
1 & 1 &1 & 1 &1 & ... & 1\\
1 & 2 & 3 & 4 &&&n+1\\
1&3 & 6&&&&\\
1 & 4 &&\ddots &&&\\
1 &&&&&&\\
\vdots&&&&&&\\
1 & n+1 &&&&& 2n \choose n
\end{array}\right) = \left({ i+j \choose i} \right)_{0 \le i,j \le n}\]
\end{exemple}

\subsection{Opérations}

--- On peut additionner deux matrices de même taille (addition terme à terme) ;

--- On peut multiplier une matrice par un scalaire $\lambda$ (tous les coefficients sont multipliés par $\lambda$) ;

--- On peut multiplier une matrice $A$ de taille $m \times n$ par une matrice $B$ de taille $n \times p$ pour obtenir une matrice $C=AB$ de taille $m \times p$ :

\[c_{i,j} := \sum_{k=1}^n a_{i,k}b_{k,j}
\] 

\og le coefficient $(i,j)$ de la matrice $AB$ est le produit scalaire de la ligne $i$ de $A$ par la colonne $j$ de $B$. \fg .

\begin{exercicecours}

--- matrices de rotations : \[\left(\begin{array}{cc}
\cos a & -\sin a\\
\sin a & \cos a
\end{array}\right) . \left(\begin{array}{cc}
\cos b & -\sin b\\
\sin b & \cos b
\end{array}\right)\]

\[= \left(\begin{array}{cc}
\cos (a+b) & -\sin (a+b)\\
\sin (a+b) & \cos (a+b)
\end{array}\right) \] 

--- nombres complexes \[\left(\begin{array}{cc}
\Re z & - \Im z\\
\Im z & \Re z
\end{array}\right) . \left(\begin{array}{cc}
\Re z' & - \Im z'\\
\Im z' & \Re z'
\end{array}\right)\]
\[ = \left(\begin{array}{cc}
\Re (zz') & - \Im (zz')\\
\Im (zz') & \Re (zz')
\end{array}\right)\]

--- matrices de Pascal\label{matdeP} 

\[T_-T^+ = P\]

{\it indication : $P_{i,j}$ est le coefficient de degré $j$ du polynôme $(1+X)^{i+j}$. Or, $(1+X)^{i+j} = (1+X)^i(1+X)^j$ ; exprimer le polynôme $(1+X)^i$ (respectivement $(1+X)^j$) en fonction des coefficients de $T_-$ (respectivement $T^+$).}
\end{exercicecours}



\begin{proposition}[Associativité]
Soient $A$ une matrice de taille $m \times n$, $B$ une matrice de taille $n \times p$ et $C$ une matrice de taille $p \times q$. Alors on a l'égalité de matrices $m \times q$ :
\[(AB)C = A(BC)\]
\end{proposition}

\begin{proof}
Notons $u_{i,j}$  les coefficients du membre de gauche et $v_{i,j}$ ceux du membre de droite. Alors :

\[u_{i,j} = \sum_{1 \le k \le n \atop 1 \le l \le p} a_{i,k}b_{k,l}c_{l,j} = v_{i,j} .\]
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrices carrées}


\[\left(\begin{array}{ccc}
\ddots & ... & (i < j)\\
\vdots & (i=j) & \vdots\\
(i>j) & ... & \ddots
\end{array}\right)\]


--- Propriétés de $\Kk-$algèbre :

{\it distributivité}

\[A(B+C) = AB + AC \; ,\; (A+B)C = AB + BC \;, \] \[ \Qq t \in \Kk,\, t(A)B=A(tB) = t(AB) \]

--- Propriétés \og négatives \fg :

{\it non commutativité :} 


\[\left(\begin{array}{cc}
0&1 \\
0&0
\end{array}\right) \left(\begin{array}{cc}
1&0 \\
0 & 0
\end{array}\right) = 0 \neq  \left(\begin{array}{cc}
1&0 \\
0 & 0
\end{array}\right)\left(\begin{array}{cc}
0&1 \\
0&0
\end{array}\right) = \left(\begin{array}{cc}
0&1 \\
0&0
\end{array}\right)\] 

{\it existence d'éléments nilpotents non nuls :}

\[\left(\begin{array}{cc}
0&1 \\
0&0
\end{array}\right)^2 = 0\]

--- Effet de la multiplication à gauche ou à droite par une matrice diagonale : 

\begin{equation}\label{eq:diagg}
\left( \begin{array}{ccc}
d_1 &&\\
&\ddots &\\
&&d_n
\end{array} \right) \left(\begin{array}{ccc}
a_{1,1}&...& a_{1,n}\\
\vdots &&\vdots\\
a_{n,1} && a_{n,n}
\end{array}\right) = \left(\begin{array}{ccc}
d_1a_{1,1}&...& d_1a_{1,n}\\
\vdots &&\vdots\\
d_na_{n,1} && d_na_{n,n}
\end{array}\right) \end{equation}

\begin{equation}\label{eq:diagd}
\left(\begin{array}{ccc}
a_{1,1}&...& a_{1,n}\\
\vdots &&\vdots\\
a_{n,1} && a_{n,n}
\end{array}\right) \left( \begin{array}{ccc}
d_1 &&\\
&\ddots &\\
&&d_n
\end{array} \right) = \left(\begin{array}{ccc}
d_1a_{1,1}&...& d_na_{1,n}\\
\vdots &&\vdots\\
d_1a_{n,1} && d_na_{n,n}
\end{array}\right) \end{equation}

\begin{definition}
La matrice identité, notée $I_n$ :

\[ I_n := \left(\begin{array}{ccc}
1 & &\\
& \ddots &\\
&&1
\end{array}\right)\]

\og Des $1$ sur la diagonale, des $0$ en dehors\fg .
\end{definition}

D'après \ref{eq:diagg} et \ref{eq:diagd}, 

\[\Qq A \in \mathcal{M}_n(\Kk), \; AI_n=I_n A = A .\]


\begin{definition}
Matrices inversibles. Une matrice $A \in \mathcal{M}_n(\Kk)$ est inversible s'il existe une matrice, notée $A^{-1}$, telle que :
\[AA^{-1} = A^{-1} A = I_n .\]

Notation : $\GL_n(\Kk)$ est l'ensemble des matrices $n \times n$ inversibles à coefficients dans $\Kk$.
\end{definition}

\begin{remarque*}
$\GL_n(\Kk)$ est un groupe pour la multiplication : c'est le {\it groupe général linéaire}\index{groupe général linéaire}\index{$\GL_n(\Kk)$, groupe des matrices inversibles}.
\end{remarque*}

\begin{remarque*}
$AB= I_n \implies BA=I_n$ (non trivial !)
\end{remarque*}



\subsubsection{La transposée}

\begin{definition}
${}^t(A)_{i,j} := A_{j,i}$
\og Les lignes de ${}^tA$ sont les colonnes de $A$ (et vice versa)\fg .
\end{definition}

{\bf Propriétés :} \[{}^t(tA) = t{}^tA ,\;\, {}^t(A+B) = {}^t A + {}^t B \;,\; {}^t(AB) = {}^tB{}^t A\]




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Applications}

\subsection{La suite de Fibonacci}\label{fibo}

C'est la suite :
\[0,1,1,2,3,5,8,13,...\]
\[f_0 :=0,f_1:=1,\Qq n \ge 1,\, f_{n+1}=f_n+f_{n-1} \]

{\bf Problème :} exprimer $f_n$ en fonction de $n$.

{\bf Solution : } On remarque que :
\[\Qq n \ge 1,\, \left(\begin{array}{c}
f_{n}\\
f_{n+1}
\end{array}\right) = A  \left(\begin{array}{c}
f_{n-1}\\
f_{n}
\end{array}\right)\]

où $A$ est la matrice 
\[\left(\begin{array}{cc}
0 & 1 \\
1 & 1
\end{array}\right)\]
et donc : \[\Qq n \ge 0,  \left(\begin{array}{c}
f_{n}\\
f_{n+1}
\end{array}\right) = A^{n} \left(\begin{array}{c}
0\\
1
\end{array}\right) .\]

Vérifier que : \[\Qq n \ge 1, \, A^n = \left(\begin{array}{cc}
f_{n-1}&f_n\\
f_n & f_{n+1}
\end{array}\right) .\]


Pour calculer $A^n$, on introduit :
\[\delta := \frac{1+ \sqrt{5}}{2},\, \overline{\delta}:= \frac{1-\sqrt{5}}{2},\, P:=\left(\begin{array}{cc}
1 & 1\\
\delta & \overline{\delta}
\end{array}\right),\, P^{-1}= \frac{1}{\sqrt{5}}\left(\begin{array}{cc}
- \overline{\delta} & 1\\
\delta & -1
\end{array}\right) \]

(\og pourquoi ? patience !\fg )

et on remarque que :
\[A = PD P^{-1}\]
où $D:= \left(\begin{array}{cc}
\delta & 0\\
0& \overline{\delta}
\end{array}\right)$.

On trouve donc : \[\Qq n\ge 0,\, A^n = PD^nP^{-1}\]
\[\Qq n\ge 0,\, A^n =P\left(\begin{array}{cc}
\delta^n & 0\\
0& \overline{\delta}^n
\end{array}\right)P^{-1}\]
et donc :
\[ \Qq n \ge 0 ,\, f_n = A^{n}_{1,2} = \frac{\delta^n - \overline{\delta}^n}{\sqrt{5}}.\]

\begin{remarque*}
Les nombres $(\frac{1+ \sqrt{5}}{2})$ et $(\frac{1- \sqrt{5}}{2})$ sont les {\it valeurs propres} de $A$ (définition à venir ...).
\end{remarque*}

\subsection{Graphes}

Un pilote voyage entre trois villes suivant le graphe suivant :


[[ICI TABLEAU]]


%\[\xymatrix{
%P \ar[r] \ar@/^/[rr]&B  \ar[r] & \ar\ar@/^/[ll]M\\
%1 & 2 & 3 }\]

Les flèches représentent les trajets possibles à chaque étape.

{\bf Question : } En $n$ étapes, combien y a-t-il de fa\c{c}ons d'aller de $i$ à $j$ ? 

{\bf Réponse : } Notons $b_{i,j,n}$ le nombre cherché. On pose $A$ la matrice d'incidence du graphe c'est-à-dire \[a_{i,j} := \left\{ \begin{array}{l}
1 \mbox{ s'il y a une flèche de $i$ vers $j$ }\\
0 \mbox{ sinon.}
\end{array}\right.\]

Ici : \[A = \left( \begin{array}{ccc}
0&1&1\\
0 &0&1\\
1 &0 &0
\end{array}
\right)\]

On a $b_{i,j,n} = A^n_{i,j}$. \begin{proof}
Par récurrence sur $n \ge 1$ :

\[b_{i,j,n}= \sum_k b_{i,k,n-1}b_{k,j,1} = \sum_k A^{n-1}_{i,k}A_{k,j}\]
\[ = A^n_{i,j} .\]

En particulier, si on sait calculer $A^n$ (voir suite du cours), on peut vérifier qu'un pilote qui part de  $P$ atterrira, au bout d'une infinité d'étapes, $\rho^2$ fois plus souvent à $M$ qu'à $B$ avec :
\[\rho^2:=\lim_{n \infty}\frac{A^n_{1,3}}{A^n_{1,2}} \approx 1,75...\]\label{exdupilote}
où $\rho$ est l'unique racine réelle de $X^3-X-1$.

Ici encore, $\rho$ est une {\it valeur propre} de $A$.

\subsection{Équation différentielle} 

{\bf Problème : } Résoudre l'équation différentielle :

\[(E)\; y''+k^2y=0 \]
où $0\neq k \in \Rr$.

{\bf Solution :} On pose $Y:=\left( \begin{array}{c}
y\\
y'
\end{array}\right)$. On a :
\[(E) \iff Y'=AY\]
où $A$ est la matrice :
\[\left(\begin{array}{cc}
0 &1\\
-k^2 & 0
\end{array}\right)\]

or, nous verrons plus loin que :

\[Y' = AY \iff Y(t) = e^{tA}Y(0)\]
où $e^{tA}$ est une matrice définie de la même manière que l'exponentielle complexe. 

Ici :
\[e^{tA} = \left(\begin{array}{cc}
\cos (kt) & \frac{\sin (kt)}{k}\\
-k\sin (kt) & \cos (kt)
\end{array}\right) .\]

Donc : \[\Qq t,\, y(t) = y(0)\cos(kt) +\frac{y'(0)}{k}\sin (kt) .\]

\end{proof} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Systèmes linéaires}

Les matrices permettent d'écrire de fa\c{c}on condensée les systèmes d'équations linéaires :

\[\left\{\begin{array}{ccc}
a_{1,1}x_1 + ... + a_{1,n}x_n & = & b_1\\
... && \\
a_{m,1}x_1 + ... + a_{m,n}x_n & = & b_n
\end{array}\right. \iff AX = B \]

où \[A = (a_{i,j})_{1 \le i\le m \atop 1 \le j \le n}, \, B=\left(\begin{array}{c}
b_1 \\
\vdots \\
b_n
\end{array}\right) ,\, X= \left(\begin{array}{c}
x_1 \\
\vdots \\
x_n
\end{array}\right) .\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Rang d'une matrice}


\subsection{Rappels sur les espaces vectoriels}

Soit $E$ un $\Kk-$espace vectoriel c'est-à-dire :

$E$ est un ensemble muni d'une addition $+$ et d'une multiplication par les éléments de $\Kk$ (appelés scalaires) telles que :

\begin{enumerate}
\item $(E,+)$ est un groupe abélien c'est-à-dire :

\begin{itemize}
\item $\Qq x,y,z \in E,\, x+ (y +z) = (x+y) +z \; ;$
\item $\Qq x,y \in E,\, x + y = y +x \; ;$
\item $ \exists 0 \in E,\, \Qq x \in E, \, x+ 0 = 0 + x = x \; ;$
\item $  \Qq x \in E,\, \exists (-x) \in E ,\, x + (-x) = (-x) + x = 0 \; ; $
\end{itemize} 

\item $\Qq \lambda \in \Kk, \, \Qq x,y \in E,\, \lambda(x+y) = \lambda x + \lambda y \;;$
\item $\Qq \lambda,\mu \in \Kk,\, \Qq x \in E,\, (\lambda + \mu)x = \lambda x + \mu x $ ;
\item $\Qq \lambda,\mu \in \Kk,\, \Qq x \in E,\, (\lambda\mu ) x = \lambda (\mu x)$ ;
\item $\Qq x \in E, \, 1x = x$. 
\end{enumerate}

Les éléments de $E$ sont appelés des {\it vecteurs}.

\begin{exemple}[de base]
$E = \Kk^n$ muni de l'addition :
\[\left(\begin{array}{c}
x_1 \\
\vdots \\
x_n
\end{array}\right) + \left(\begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array}\right) : = \left(\begin{array}{c}
x_1 +y_1\\
\vdots \\
x_n +y_n
\end{array}\right)\]

et de la multiplication par les scalaires :
\[\lambda \left(\begin{array}{c}
x_1 \\
\vdots \\
x_n
\end{array}\right) := \left(\begin{array}{c}
\lambda x_1 \\
\vdots \\
\lambda x_n
\end{array}\right)\]

pour tous $\lambda,x_1,...,y_1,... \in \Kk$.
\end{exemple}


Soient $v_1,...,v_m$ des vecteurs de $E$.

Une {\it combinaison linéaire} des vecteurs $v_1,...,v_m$ est un vecteur de la forme :
\[t_1v_1+...+t_mv_m\]
où $t_1,...,t_m \in \Kk$.

On dit que $v_1,...,v_m$ sont {\it $\Kk-$linéairement indépendants} (ou {\it libres}) si :
\[\Qq t_1,...,t_m \in \Kk, \, t_1v_1+...+t_mv_m = 0 \implies t_1 = ...= t_m=0 \]
 sinon, on dit qu'ils sont {\it liés}.

On dit que $v_1,...,v_m$ sont générateurs (de $E$) ou qu'ils engendrent $E$ si tout vecteur de $E$ est une combinaison linéaire de $v_1,...,v_m$.

Si les vecteurs $v_1,...,v_m$ sont à la fois libres et générateurs on dit qu'ils forment une {\it base} de $E$.

\begin{exemple}
{\it La base canonique de $\Kk^n$ est la base formée des vecteurs :
\[\left(\begin{array}{c}
1 \\
0\\
\vdots\\
0
\end{array}\right), ..., \left(\begin{array}{c}
0 \\
\vdots\\
0\\
1
\end{array}\right)\]
}
\end{exemple}


\begin{proposition}
Soit $v_1,...,v_n$ une base de $E$. 

i) Si $w_1,...,w_m$ engendrent $E$, alors $m \ge n$ ;

ii) Si $w_1,...,w_m$ sont libres, alors $m \le n$ ;


\end{proposition}


{\bf Conséquence : }

\begin{definition}
Deux bases de $E$ ont le même cardinal. Ce cardinal commun est la dimension de $E$, notée $\dim E$.
\end{definition}


\begin{remarque*}
$\dim \Kk^n = n$. D'après la proposition $n+1$ vecteurs de $\Kk^n$ sont toujours liés.
\end{remarque*}

\begin{proof}
i : supposons, quitte à diminuer $m$, que pour tout $k$, \[w_{k+1} \not\in \langle w_1,...,w_k \rangle .\]

Il existe $t_1,...,t_n \in \Kk$ tels que \[w_1 = t_1 v_1 + ... + t_n v_n .\]
Soit $1 \le i_1 \le n$ tel que $t_{i_1} \neq 0$. Alors :
\[v_1, ..., \cancel{v_{i_1} },...,v_n,w_1\]
(\og dans la liste $v_1,...,v_n$, on remplace $v_{i_1}$ par $w_1$\fg) est encore une base de $E$ \exoo. On peut montrer plus généralement, par récurrence sur $k$ que pour tout $1 \le k \le \min\{m,n\}$, il existe $1 \le i_1,...,i_k\le n$ deux à deux distincts tels que :
\[v_1,...,\cancel{v_{i_1}}, ..., ,\cancel{v_{i_k}},...,v_n,w_1,...,w_k\]
est encore une base de $E$ (où on a remplacé dans la liste $v_1,...,v_n$ les vecteurs $v_{i_1},...,v_{i_k}$ par les vecteurs $w_1,...,w_k$).

En particulier, si, par l'absurde, $m <n$, on obtient une base de $E$ de la forme :
\[v_i,w_1,...,w_m, \, i \in \{1,...,n\}\setminus \{i_1,...,i_m\}\]
ce qui est absurde car $\langle w_1,...,w_m \rangle$ engendre $E$.

Donc $m \ge n$.

ii:  On raisonne de la même fa\c{c}on. Il existe $t_1,...,t_n$ tels que $w_1 = t_1 v_1 +...+ t_n v_n$. Il existe $i_1$ tel que $t_{i_{1}} \neq 0$. Alors, $w_1, v_1,...,\cancel{v_{i_1}},...v_n$ forment une base de $E$. De même, par récurrence sur $ 1\le k \le \min_{\{m,n\}}$, on peut montrer qu'il existe $1 \le i_1,...,i_k \le n$ deux à deux distincts tels que les vecteurs :
\[w_1,...,w_k, v_1,...,\cancel{v_{i_1}},...,\cancel{v_{i_k}},...v_n\]
forment une base de $E$. Si ({\it par l'absurde}), $m >n$, on peut prendre $k = n$ :
\[w_1,...,w_n\]
forment une base de $E$. Mais cela est absurde car alors $w_m$ ($m>n$) est une combinaison linéaire de $w_1,...,w_n$ ce qui contredit l'indépendance linéaire des $w_j$.
\end{proof}



\begin{definition}
Une application $f : E \to F$ entre deux $\Kk-$espaces vectoriels est {\it linéaire} si :

\[i)\;\; \Qq u,\Qq v \in E, \, f(u+v) = f(u) +f(v) \]
\[ii)\;\, \Qq u \in E,\Qq t \in \Kk, f(tu) = tf(u)\]
\end{definition}


\begin{definition}
Soit $E$ un $\Kk-$espace vectoriel. Une partie $F \subset E$ est un sous-espace de $E$ si :
\[0 \in F, \; \Qq u,v \in F, \Qq t\in \Kk, tu \in F \text{ et } u+v \in F\] 
\end{definition}

\begin{exemple}
Soit $f : E \to F$ une application linéaire. Son noyau $\Ker f := f^{-1}(\{0\})$ est un sous-espace de $E$ et son image $\Im f := \{y \in F \mid \exists x \in E, y=f(x)\}$ est un sous-espace de $F$.
\end{exemple}

\begin{proposition}
Une application linéaire $f : E \to F$ est {\it injective} $\iff$ $\Ker f = \{0\}$ 
\end{proposition}

\begin{proof}
Si $\Ker f = 0$, si $f(u) = f(v)$, alors $f(u-v)=0$ c'est-à-dire $u-v \in \Ker f = 0 $ donc $u-v=0$ c'est-à-dire $u=v$. 
\end{proof}

\begin{definition}
Le rang d'une famille de vecteurs $v_1,...,v_n$ dans un espace vectoriel $E$ est la dimension de $\langle v_1,...,v_n \rangle$, l'espace vectoriel engendré par ces vecteurs.
\end{definition}


Soit $A$ une matrice de taille $m \times n$ à coefficients dans $\Kk$. 

On notera $\underline{A}$, ou simplement $A$, l'application linéaire associée :
\[\underline{A} : \Kk^n \to \Kk^m\]
\[X=\left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array}\right) \mapsto AX= \left(\begin{array}{c}
y_1\\
\vdots\\
y_m
\end{array}\right)\]

où \[\Qq 1 \le i \le m ,\, y_i=\sum_{j=1}^na_{i,j}x_j .\]

\begin{remarque*}
Soient $A , B \in \mathcal{M}_{m,n}(\Kk)$. Alors :
\[A = B \iff \Qq X \in \Kk^n, \; AX = BX.\]
\end{remarque*}


\begin{exercicecours}
Soient $A \in \mathcal{M}_{m,n}(\Kk),\,B \in \mathcal{M}_{n,p}(\Kk)$. Soient $\underline{A} : \Kk^n \to \Kk^m,\, \underline{B} : \Kk^p \to \Kk^n$ les applications linéaires associées. Alors $\underline{AB} = \underline{A} \circ \underline{B}$.
\end{exercicecours}


\begin{definition}[image et noyau]
Soit $A$ une matrice $m \times n$. Son noyau est le sous-espace ({\it exo}) :
\[\Ker A = \biggm\{ X= \left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array}\right) \in \Kk^n \mid AX = 0\biggm\}\] 


\[\Im A = \biggm\{ Y= \left(\begin{array}{c}
y_1\\
\vdots\\
y_m
\end{array}\right) \in \Kk^m \mid \exists X= \left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array}\right) \in \Kk^n ,\, Y=AX \biggm\}\]
\end{definition}

\begin{remarque*}
La $j-$ième colonne de $A$ est le vecteur $Ae_j$ où :
$e_j$ est le vecteur colonne \[\left(\begin{array}{c}
\vdots\\
1 \\\vdots
\end{array}\right)\]
avec un \og $1$ \fg\ en $j-$ième position et des \og $0$ \fg\ ailleurs.

En particulier, $\Im A$ est le sous-espace de $\Kk^m$ engendrée par les vecteurs colonnes de $A$.
\end{remarque*}


\subsection{Matrices échelonnées}

\begin{definition}
Une matrice échelonnée est une matrice de la forme :

\[\left(\begin{array}{ccccccc}
0 & ...  & a_{1,{j_1}} & ...& &\\
0& ... &  & a_{2,{j_2}} & ...&\\
\vdots &&&&&\\
&&  && a_{r,j_r}&... \\
0 &...& &&&0
\end{array}\right)\]
où $0 \le r \le n$, pour tout $1 \le k \le r$, $a_{k,j_k}$ est le premier terme non nul de la ligne $k$ et $j_1 < ... < j_r$.
\end{definition}

On appellera opération élémentaire sur les lignes une des opérations suivantes :

--- ajouter à une ligne ($i$) une autre ligne ($j$) multipliée par un coefficient $t\in \Kk$ ;

--- échanger deux lignes : $i$ et $j$ ;

--- multiplier la ligne $i$ par un coefficient non nul $\alpha \in \Kk^*$.

\begin{remarque*}
Chaque opération élémentaire revient à multiplier à gauche par une matrice \og simple \fg : respectivement :

--- $T_{i,j}(t)$ \og la matrice $I_n$ à laquelle on a ajouté un $t$ en position $i,j$ \fg\ 

\[\left(
\begin{array}{cccc}
1 & ...&t &\\
&\ddots &&\\
&&\ddots&\\
&&&1
\end{array}
\right)
\]

--- $\Sigma_{i,j}$ \og la matrice obtenue à partir de $I_n$ en permutant les colonnes $i$ et $j$ \fg :
\[\mbox{ exemple : } \Sigma_{1,n }=
\left(
\begin{array}{ccccc}
0 & ... &&&1\\
\vdots & 1 &&&\vdots\\
&&\ddots&&\\
&&&1&\\
1 &&&&0
\end{array}\right)\]

--- $D_i(\alpha)$ \og la matrice obtenue à partir de $I_n$ en rempla\c{c}ant le $i$ème coefficient diagonal par $\alpha$ :

\[\left(\begin{array}{ccccc}
1 &&&&\\
&\ddots &&&\\
&&\alpha &&\\
&&&\ddots &\\
&&&&1
\end{array}\right).\]
\end{remarque*}

\begin{theoreme}
Chaque matrice peut être transformée en une matrice échelonnée par une suite finie d'opérations élémentaires.
\end{theoreme}

\begin{proof}
Par récurrence sur le nombre de lignes. Soit $A$ une matrice $m \times n$. Soit $j_1$ la première colonne non nulle. Quitte à permuter la première ligne avec une autre, on peut supposer que $a_{1,j_1} \neq 0$. On note $L_1,...,L_m$ les lignes  de $A$. On remplace alors, pour tout $k >1$, la ligne $L_k$ par $L_k -\frac{a_{k,j_1}}{a_{1,j_1}}L_1$. On obtient alors une matrice de la forme :

\[\left(\begin{array}{cccc}
0 & ... & a_{1,j_1}&...\\
\vdots & &0&...\\
&&\vdots&\\
0&&0&
\end{array}
\right)\]

et on termine par récurrence.
\end{proof}

\begin{definition}
Le nombre $r$ est le rang des lignes de la matrice.
\end{definition}


\begin{proposition}
Le rang $r$ est indépendant des transformations effectuées : $r$ est la dimension du sous-espace de $\mathcal{M}_{1,n}(\Kk)$ engendré par les lignes $L_1,...,L_m$ de la matrice.
\end{proposition}



\begin{proof}
En effet, une opération élémentaire ne change pas l'espace vectoriel $\langle L_1,...,L_m\rangle$ et les lignes non nulles d'une matrice échelonnée sont clairement indépendantes.
\end{proof}

On peut aussi définir le rang des colonnes de la matrice en utilisant des opérations élémentaires sur les colonnes. Ce rang est égal à la dimension du sous-espace de $\mathcal{M}_{m,1}(\Kk)$ engendré par les colonnes de la matrice.


\begin{proposition}
Si $A$ est une matrice carrée de taille $n$, si $A$ est de rang $n$, alors $A$ est inversible.
\end{proposition}



\begin{proof}
On applique des transformations élémentaires à $A$ jusqu'à obtenir la matrice $I_n$. Si $r = n$, c'est possible. Si on applique les mêmes transformations à $I_n$ on obtient $A^{-1}$. En effet, soit $E_1,..., E_N$ des matrices \og élémentaires \fg\ telles que :
\[E_1...E_NA = I_n\]
alors : \[E_1...E_N = A^{-1} .\] 
\end{proof}
\begin{exemple}
Soit $A : = \left(\begin{array}{cc}
0 & 1 \\
1 & 1
\end{array}\right)$.

On effectue les opérations suivantes :

\[\left(\begin{array}{cc|cc}
0 & 1 &1 & 0\\
1 & 1 & 0 & 1
\end{array}
\right) \stackrel{L_1 \leftrightarrow L_2 }{\leadsto} \left(\begin{array}{cc|cc}
1& 1 &0 & 1\\
0 & 1 & 1 & 0
\end{array}
\right) \]
\[\stackrel{L_1 \leftarrow L_1 -L_2 }{\leadsto} \left(\begin{array}{cc|cc}
1& 0 &-1 & 1\\
0 & 1 & 1 & 0
\end{array}
\right)\]

conclusion : $A^{-1} = \left(\begin{array}{cc}
-1& 1\\
1& 0
\end{array}
\right)$.
\end{exemple}

Plus généralement, pour une matrice $A  \in \mathcal{M}_{m,n}(\Kk)$, le même raisonnement montre qu'il existe $P \in \GL_m(\Kk)$ tel que :
\[PA =\left( \begin{array}{c}
I_n\\ \hline
0
\end{array}\right) \]
(en particulier, dans ce cas $n \le m$).

Si l'on multiplie à gauche par $\left(\begin{array}{c|c}
I_n & 0
\end{array}\right)$, on trouve :
\[\left(\begin{array}{c|c}
I_n & 0
\end{array}\right) PA = I_n .\]

On peut raisonner de même avec les colonnes. En résumé :
\begin{theoreme}\label{thm:inv-gd}
Soit $A \in \mathcal{M}_{m,n}(\Kk)$. 

i) Si le rang des lignes de $A$ est $n$, alors $n \le m$ et il existe $P \in \GL_m(\Kk)$ tel que $PA = \left( \begin{array}{c}
I_n\\ \hline
0
\end{array}\right) $ et il existe $B\in \mathcal{M}_{n,m}(\Kk)$ tel que $BA=I_n$. On dit que $A$ est inversible à gauche.

ii) Si le rang des colonnes de $A$ est $m$, alors $m \le n$ et il existe $Q \in \GL_n(\Kk)$ tel que $AQ = \left( \begin{array}{c|c}
I_m & 0
\end{array}\right) $ et il existe $B\in \mathcal{M}_{n,m}(\Kk)$ tel que $AB=I_m$. On dit que $A$ est inversible à droite.

\end{theoreme}

\subsection{\'Egalité entre le rang des lignes et le rang des colonnes}


Soit $A \in \mathcal{M}_{m,n}(\Kk)$ une matrice. On rappelle que le rang des lignes de $A$, notons-le $\rg_L(A)$, est la dimension du sous-espace vectoriel de $\mathcal{M}_{1,n}(\Kk)$ engendré par les lignes de $A$. On rappelle que  rang des colonnes de $A$, notons-le $\rg_C(A)$, est la dimension du sous-espace vectoriel de $\mathcal{M}_{m,1}(\Kk)$ engendré par les colonnes de $A$.

Alors :
\begin{theoreme}


\[\rg_L(A)\]
\[ = \min\left\{ t \ge 1 \mid \exists B\in \mathcal{M}_{m,t}(\Kk) ,\, \exists C \in \mathcal{M}_{t,n}(\Kk) ,\, A =BC\right\} \]
 \[= \rg_C(A) .\]
On notera $\rg(A)$ le rang de $A$ (des lignes ou des colonnes).

En particulier, $\rg (A) \le \min\{m,n\}$.
\end{theoreme}
\begin{proof}

Montrons par exemple la première égalité (la deuxième se montre de la même fa\c{c}on) :

Notons $r_0$ le minimum des $t$ tels que $A =BC$ pour un certain $B \in \mathcal{M}_{m,t}(\Kk)$ et un certain $C\in \mathcal{M}_{t,n}(\Kk)$.

Soit $r:= \rg_L(A)$. Alors, il existe une base $l_1,...,l_r$ du sous-espace engendré par les lignes de $A$. En particulier, pour toute ligne $L_i$ de $A$, \[* \;\; L_i = b_{i,1}l_1 + ... +b_{i,r}l_r\]
pour certains coefficients $b_{i,j} \in \Kk$, $1 \le i \le m$, $1 \le j \le r$. Soit $B \in \mathcal{M}_{m,r}(\Kk) $ la matrice des $b_{i,j}$ et soit $C\in \mathcal{M}_{r,n}$ la matrice dont les lignes sont $l_1,...,l_r$.
La relation $*$ pour tout $i$, donne : $A =BC$. Donc, $r_0\le r$.

D'un autre côté, si $A = BC$ avec $B \in \mathcal{M}_{m,t}(\Kk) ,\,C\in \mathcal{M}_{t,n}(\Kk)$. alors pour tout $1 \le i\le m$, la ligne $L_i$ de $A$ vérifie :
\[L_i = B_{i,1}l_1 + ..; +B_{i,t}l_t\]
où $l_1,...,l_t$ sont les lignes de $C$. Donc le sous-espace engendré par les lignes de $A$ est de dimension $\le t$. Donc $r \le t$. Et donc, $r \le r_0$ si on prend $t=r_0$.
   
En résumé, le rang d'une matrice $A$ est à la fois le rang des lignes de $A$, le rang de ses colonnes et la dimension de son image.

\end{proof}

On déduit de cette caractérisation du rang que :
\[\rg (AB) \le \min \{\rg A , \rg B\}\]

pour toutes matrices $A \in \mathcal{M}_{m,n}(\Kk),\, B \in \mathcal{M}_{n,p}(\Kk)$.

\begin{proposition}
Si $A \in \mathcal{M}_{m,n}(\Kk)$ est de rang $r$, il existe $B\in\mathcal{M}_{m,r}(\Kk),\, C\in \mathcal{M}_{r,n}(\Kk)$ tels que $A =BC$. Dans ce cas, $\rg B = r=\rg C$. De plus, il existe $P \in \GL_m(\Kk),\, Q \in \GL_n(\Kk)$ tels que :
\[PAQ = \left(\begin{array}{c|c}
I_r &0\\ \hline
0 & 0
\end{array}\right) .\]
\end{proposition}

\begin{proof}
Si $A =BC$ avec $B\in\mathcal{M}_{m,r}(\Kk),\, C\in \mathcal{M}_{r,n}(\Kk)$, alors $r =\rg A \le \rg B \le r$ donc $\rg B =r$. De même, $\rg C =r$. D'après le théorème \ref{thm:inv-gd}, il existe donc $P \in \GL_m(\Kk),\, Q \in \GL_n(\Kk)$ tels que : \[PB = \left( \begin{array}{c}
I_r\\ \hline
0
\end{array}\right)\,,\; CQ = \left( \begin{array}{c|c}
I_r & 0
\end{array}\right)\]
D'où : \[PAQ = PBCQ =   \left( \begin{array}{c}
I_r\\ \hline
0
\end{array}\right)\left( \begin{array}{c|c}
I_r & 0
\end{array}\right) = \left(\begin{array}{c|c}
I_r &0\\ \hline
0 & 0
\end{array}\right)\]
\end{proof}


Pour terminer voici un critère pratique pour calculer le rang d'une matrice :

\begin{proposition}
Soit $A =(a_{i,j})_{1\le i \le m \atop 1 \le j \le n}\in \mathcal{M}_{m,n}(\Kk)$. On dit que $B$ est une matrice extraite de $A$ si $B$ est de la forme :
\[B=(a_{i,j})_{i \in I\atop j\in J}\]
pour un certain $I \subset \{1,...,m\}$ et un certain $J \subset \{1,...,n\}$.

Le rang de $A$ est le plus grand entier $r$ tel qu'il existe une matrice $B$, extraite de $A$, carrée, inversible de taille $r$. 
\end{proposition}

{\it Exemple :} la matrice 
\[\left(\begin{array}{ccc}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{array}\right)\]
n'est pas inversible \exoo mais la matrice extraite :
\[
\left(\begin{array}{ccc}
1 & 2 \\
4 & 5 
\end{array}\right)\]
l'est \exoo. Donc $A$ est de rang $2$.

\begin{proof}
Soit $B$ une matrice extraite de $A$, carrée, inversible de taille $r$. Supposons pour simplifier que $B= (a_{i,j})_{1\le i,j\le r}$. Alors, les $r$ premières lignes de $B$ sont linéairement indépendantes. A fortiori, les $r$ premières lignes de $A$ sont aussi linéairement indépendantes. Donc $\rg A \ge r $. Supposons que $A$ est de rang $R$. Alors il existe $R$ lignes de $A$ qui sont linéairement indépendantes, par exemple les $R$ premières. La matrice 
\[(a_{i,j})_{1\le i \le R \atop 1 \le j \le n}\]
est donc de rang $R$. Elle admet donc au moins $R$ colonnes indépendantes, par exemple les $R$ premières. Alors la matrice extraite :
\[(a_{i,j})_{1 \le i,j \le R}\]
est carrée, de taille $R$ et inversible (car de rang $R$). 
\end{proof}

\subsection{Image et noyau d'une matrice}

Soit $A \in \mathcal{M}_{m,n}(\Kk)$. On dit que $A$ est {\it injective} si ses colonnes sont indépendantes. On dit que $A$ est surjective si ses lignes sont indépendantes.

{\it Remarque :} si $x_1,...,x_n \in \Kk$, si on note $C_1,...,C_n$ les colonnes de $A$, alors :
\[x_1C_1+...+x_nC_n = A.\left(\begin{array}{c}
x_1\\\vdots\\x_n
\end{array}\right)\]
notons : \[\Ker A:= \left\{{\left(\begin{array}{c}
x_1\\\vdots\\x_n
\end{array}\right) \in \Kk^n \mid A.\left(\begin{array}{c}
x_1\\\vdots\\x_n
\end{array}\right) = 0}\right\} \]
c'est un sous-espace de $\Kk^n$ : c'est le {\it noyau} de $A$.
Alors $A$ est injective $\iff \Ker A =0$.

Notons \[\Im A: = \left\{{ A.\left(\begin{array}{c}
x_1\\\vdots\\x_n
\end{array}\right) \mid \left(\begin{array}{c}
x_1\\\vdots\\x_n
\end{array}\right) \in \Kk^n }\right\}\]

c'est le sous-espace de $\Kk^m$ engendré par les colonnes de $A$, on l'appelle {\it l'image} de $A$. Comme le rang des lignes est aussi le rang des colonnes, les lignes sont indépendantes c'est-à-dire $A$ est surjective si et seulement si $\rg A = m$ $\iff \Im A = \Kk^m$. 


\begin{exercicecours}
Soit $P \in \GL_m(\Kk)$. Alors : $\Ker (PA) = \Ker A$.
\end{exercicecours}

\begin{exercicecours}

Si $A := \left(\begin{array}{cccc}
0 & ... & a_{1,j_1}&...\\
\vdots & &0&...\\
&&\vdots&\\
0&&0&
\end{array}
\right)$  est une matrice échelonnée avec $j_1 < ...<j_r$ et  $a_{i,j_i}\neq 0$ pour tout $1\le i \le r$, alors l'application linéaire :
\[\Ker A \to \Kk^{n-r}  \;,\;(x_j)_{1 \le j \le n} \mapsto (x_j)_{1 \le j \le n \atop j \neq j_1,...,j_r}\]
 est un isomorphisme (voir plus bas le rappel de la notion d'isomorphisme).

En particulier, $\Ker A$ est de dimension $n -r$.
\end{exercicecours}

On déduit des deux exercices précédents le 
\begin{theoreme}[du rang]
\[\dim \Ker A + \rg A = n\]
(le nombre de colonnes).
\end{theoreme}


Pour résumé, on a démontré les équivalences suivantes :
\[A \mbox{ injective }\iff \rg A = n \iff A \mbox{ inversible à gauche}\]
\[A\mbox{ surjective }\iff \rg A = m \iff A \mbox{ inversible à droite}\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Lien avec les applications linéaires}


\subsection{
 Matrice associée à une application linéaire}

Soit $E$ un $\Kk-$espace vectoriel. 
\begin{definition}
Un endomorphisme de $E$ est une application linéaire $f : E \to E$. On note $\End_\Kk(E)$ ou ${\cal L}(E)$ l'ensemble des endomorphismes de $E$.
\end{definition}

Soit $f \in \End_\Kk(E)$. Soit $e_1,...,e_n$ une base de $E$. Pour tout $1 \le j \le n$, il existe $a_{1,j},...,a_{n,j} \in \Kk$ tels que 
\[f(e_j) = a_{1,j}e_1 + ...+ a_{n,j}e_n\]

en fait, $f$ est entièrement déterminé par ces coefficients : $a_{i,j}$, $ 1 \le i,j \le n$ :
\[\Qq v = x_1e_1+...+x_n e_n\in E,\, f(v) = \sum_{i=1}^n\sum_{j=1}^n x_ja_{i,j}e_i .\]
 autrement dit, si $X:=\left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array}\right)$ est le vecteur \og coordonnées de $v$ dans la base $(e)$\fg\, si $Y:=\left(\begin{array}{c}
y_1\\
\vdots\\
y_n
\end{array}\right)$ est le vecteur \og coordonnées de $f(v)$ dans la base $(e)$\fg\ alors : 
\[Y = AX\]
où $A$ est la matrice $(a_{i,j})$.

On dit que la matrice $A := (a_{i,j})_{1 \le i,j\le n}$ est la {\it matrice de $f$ dans la base $(e) := e_1,...,e_n$}.

Notation : \[A:= \Mat(f)_{(e)} .\]

\begin{exercicecours}
 Soient $f,f' : E \to E $ deux applications linéaires. Soit $(e)$ une base de $E$, alors :
\[ \Mat (f \circ f')_{(e)} = \Mat (f)_{(e)} \Mat (f')_{(e)} . \]
\end{exercicecours}

\begin{exemple}

--- matrices de rotations : soit $R_\theta : \Rr^2 \to \Rr^2$ la rotation de centre $0$ et d'angle $\theta$ dans le plan. C'est une application linéaire. Dans la base usuelle $e_1,e_2$ de $\Rr^2$, la matrice de $R_\theta$ est donnée par :
\[\left(\begin{array}{cc}
\cos \theta & -\sin \theta \\
\sin \theta & \cos \theta
\end{array}\right).\]

--- matrices de la dérivation sur l'espace des polynômes de degré $\le n$ :

\[\left(\begin{array}{ccccc}
0 & 1 &0 &...&0\\
\vdots &\ddots & 2&&\vdots \\
&&&&\\
&&&&n\\
0 & ...&&&0
\end{array}\right) \text{ et } \left(\begin{array}{ccccc}
0 & 1 &0 &...&0\\
\vdots &\ddots & 1&&\vdots \\
&&&&\\
&&&&1\\
0 & ...&&&0
\end{array}\right)\]
respectivement dans la base $1,X,...,X^n$ et dans la base $1,X,...,\frac{X^n}{n!}$.
\end{exemple}

\subsection{Théorème du rang}

\begin{theoreme}
Soient $E,F$ deux $\Kk-$espaces vectoriels. On suppose que $f : E \to F$ est linéaire. Alors si $E$ est de dimension finie :

\[\dim E = \dim \Ker f + \rg f\]
où $\rg f$ est la dimension de l'image de $f$. 
\end{theoreme}

\begin{proof}
Soit $e_1,...,e_r$ une base de $\Ker f$. On la complète en une base $e_1,...,e_r,e_{r+1},...,e_n$ de $E$. Alors $f(e_{r+1}),...,f(e_n)$ est une base de $\Im f$ ({\it exo}).
\end{proof}


\begin{definition}
Un {\it isomorphisme} entre deux espaces vectoriels $E$ et $F$ est une application linéaire bijective $f : E \to F$, notation : $E \iso F$.
\end{definition}

\begin{corollaire}[Miracle de la dimension finie]
Si $E$ est de dimension finie, si $f : E \to E$ est linéaire, alors $f$ injectif $\iff$ $f$ surjectif $\iff$ $ f$ isomorphisme.
\end{corollaire}
\begin{remarque*}
{\bf ATTENTION : } il faut le même espace au départ et à l'arrivée (ou au moins deux espaces de même dimension au départ et à l'arrivée).
\end{remarque*}

\begin{proof}

\[f \mbox{ injectif } \implies \Ker f = 0\]
\[\implies \dim f(E) = \dim E\]
\[\implies f(E) = E \]
c'est-à-dire $f$ surjectif. Réciproque :
\[f \mbox{ surjectif } \implies f(E) = E\]
\[\implies \dim \Ker f = 0\]
\[\implies \Ker f = 0\]
c'est-à-dire $f$ injecftif.
On utilise que si $V$ est un sous-espace de $U$, alors $V$ est de dimension finie $\le \dim U$ avec égalité si et seulement si $V =U$.
\end{proof}

{\bf Versions matricielles : }

Soit $A \in \mathcal{M}_{m,n}(\Kk)$. Alors : $\dim \Ker A + \dim \Im A = n$. 

On dira qu'une matrice $A \in \mathcal{M}_{m,n}(\Kk)$ est injective si 
\[\Ker A = 0\]
c'est-à-dire \[\Qq X \in \Kk^n,\, AX = 0 \implies X =0\]

et que $A$ est surjective si :
\[\Im A = \Kk^m\]
c'est-à-dire 
\[\Qq Y \in \Kk^m,\, \exists X \in \Kk^n,\, Y=AX .\]

Une matrice $A$ est injective $\iff$ $A$ est inversible à gauche\index{inversible à gauche} c'est-à-dire :
\[\exists B \in \mathcal{M}_{n,m}(\Kk),\, BA = I_n.\]


En effet, si $BA = I_n$, alors, $AX = 0 \implies BAX=0 \implies I_nX=X =0$. Réciproquement, si $A$ est injective, le rang de ses lignes est aussi le rang de ses colonnes c'est-à-dire la dimension de son image. D'après le théorème du rang, ce rang vaut $n$. Donc le sous-espace de $\mathcal{M}_{1,n}(\Kk) (=\Kk^n)$ engendré par les  lignes $L_1,...L_m$ de $A$ est de dimension $n$. Donc les lignes de $A$ engendrent $\mathcal{M}_{1,n}(\Kk)$. En particulier, si on note $l_1,...,l_n$ les lignes de la matrice $I_n$ (c'est-à-dire la base canonique de $\mathcal{M}_{1,n}(\Kk)$), il existe, pour tout $1 \le i\le n$, des coefficients $b_{i,1},...,b_{i_m}\in \Kk$ tels que :
\[l_i = b_{i,1}L_1+...+b_{i,m}L_m .\]

Autrement dit, si on note $B\in \mathcal{M}_{n,m}(\Kk)$ la matrice des $b_{i,j}$, on a :
\[BA = I_n .\]

On peut montrer de même (en raisonnant plutôt avec les colonnes) que $A$ est surjective $\iff$ $A$ est inversible à droite c'est-à-dire :
\[\exists B \in \mathcal{M}_{n,m}(\Kk),\, AB = I_m .\]

Supposons maintenant que $m=n$ et que $A,B \in \mathcal{M}_n(\Kk)$.
Alors :
\[AB = I_n \iff BA =I_n\]

\begin{proof}
$AB = I_n \implies A$ surjective $\implies A$ injective.

Or : \[\Qq X \in \Kk^n,\, A(BA X) = (AB)(AX)  = A(X) \implies BAX = X \]

donc $BA = I_n$. 
\end{proof}



\begin{theoreme}
Soit $A \in\mathcal{M}_{m,n}(\Kk)$. Alors, \[A \mbox{ injective } \iff \rg(A) = n\]
\[A \mbox{ surjective }\iff \rg(A) =m\]
en particulier si $m=n$, $A$ injective $\iff A$ surjective $\iff A$ inversible.
\end{theoreme}

\begin{proof}
Par exemple : $\rg A = n \implies \dim Im A = n \implies \dim \Ker A =0$ et réciproquement si $A$ est injective, alors $A$ est inversible à gauche : il existe $B \in \mathcal{M}_{n,m}(\Kk)$ tel que : $BA = I_n$. Donc $n \ge \rg A\ge \rg BA =n$ et $\rg A =n$.
\end{proof}
\subsection{Changements de base}

--- {\bf Interprétation de l'ensemble des bases au moyen des matrices inversibles.}

Soit $e_1,...,e_n$ une base de $\Kk^n$. Soit $P \in \GL_n(\Kk)$ une matrice inversible. Les vecteurs :
\[e'_1=Pe_1,...,e'_n=Pe_n\]
forment encore une base de $\Kk^n$. On obtient ainsi toutes les bases de $\Kk^n$.


En effet, soient $(e')$, $(e)$ deux bases de $\Kk^n$ (ou de n'importe quel $\Kk-$espace vectoriel de dimension $n$). Pour tout $j$,
\[e'_j = p_{1,j}e_1 + ... + p_{n,j}e_n\]
pour certains $p_{i,j} \in \Kk$.

Soit $ v \in \Kk^n$. Si on note $\left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array}\right)$ ses coordonnées dans la base $(e)$, $\left(\begin{array}{c}
x'_1\\
\vdots\\
x'_n
\end{array}\right)$ ses coordonnées dans la base $(e')$, alors :

\[X = PX'\]
où $P:= (p_{i,j})_{1 \le i,j\le n}$.

\begin{proof}
\[v = x_1e_1 +...+x_n e_n = x'_1e'_1+...+x'_ne'_n\]
\[= \sum_{j=1}^nx'_j \sum_{i=1}^n p_{i,j}e_i\]
\[= \sum_{i=1}^n \left(\sum_{j=1}^np_{i,j}x'_j\right)e_i\]
\[\iff \Qq\, 1 \le i \le n,\, x_i = \sum_{j=1}^np_{i,j}x'_j\]
\[\iff X = PX' .\]
\end{proof}



\begin{definition}[Matrice de passage]
La matrice $P$ est la matrice de passage de $(e)$ à $(e')$ ; notation : \[P_{(e)}^{(e')} .\] 
\end{definition}

\begin{exercicecours}
$P$ est inversible d'inverse la matrice de passage de $(e')$ à $(e)$.
\end{exercicecours}


--- {\bf Formule de changement de base :}

Soit $f$ un endomorphisme de $\Kk^n$ (ou de n'importe quel $\Kk-$espace vectoriel de dimension $n$). Soit $A$ la matrice de $f$ dans la base $(e)$, soit $A'$ la matrice de $f$ dans la base $(e')$. Ces deux matrices sont reliées par :
\[A = P A'P^{-1}\]

\begin{proof}
Soit $v \in \Kk^n$. Soient $X,X',Y,Y'$ respectivement les vecteurs (colonnes) coordonnées de $v$ dans les bases $(e)$ et $(e')$, de $f(v)$ dans les bases $(e)$ et $(e')$. 

Alors : $Y = AX$ et $Y'= AX'$ ({\it exo}). De plus :
\[X = PX',\, Y = PY' \]
\[\implies PY' = APX'\]
\[\implies Y' = P^{-1} AP X' = A'X'\]
(pour tout $X' \in \Kk^n$). Donc $P^{-1} AP = A'$.

\end{proof}

\begin{exercicecours}
\[\left(\begin{array}{cc}
1&1\\
-i&i
\end{array}\right)  \left(\begin{array}{cc}
\cos t& -\sin t\\
\sin t& \cos t
\end{array}\right)  \left(\begin{array}{cc}
1&1\\
-i &i
\end{array}\right)^{-1} = \left(\begin{array}{cc}
e^{it}&0\\
0&e^{-it}

\end{array}\right)\]
\end{exercicecours}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Le déterminant}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dimension $2$ et $3$}

\begin{definition}
\[\left|\begin{array}{cc}
a_{1,1}& a_{1,2}\\
a_{2,1}& a_{2,2}
\end{array}\right| := a_{1,1}a_{2,2}- a_{1,2}a_{2,1}\]
\[\left|\begin{array}{ccc}
a_{1,1}& a_{1,2} & a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2} & a_{3,3}
\end{array}\right| := a_{1,1}a_{2,2}a_{3,3} + a_{2,1}a_{3,2}a_{1,3} + a_{3,1}a_{1,2}a_{2,3} \]\[ - a_{2,1}a_{2,1}a_{3,3} - a_{1,1}a_{3,2}a_{2,3} - a_{3,1}a_{2,2}a_{1,3} \]


\end{definition}

{\bf Moyen mnémotechnique : }


[[ICI TABLEAU]]

%\xymatrix{
%\bullet \ar[dr] & \bullet \ar[dr]&\bullet\ar[ddl] && \bullet \ar[drr] & \bullet \ar[ddr]&\bullet\ar[dl]\\
%\bullet \ar[rru]& \bullet\ar[dr]& \bullet\ar[lld]&& \bullet \ar[ur] & \bullet \ar[dl]&\bullet\ar[dl]\\
%\bullet\ar[uur] & \bullet \ar[ul]& \bullet &&\bullet  & \bullet \ar[uul]&\bullet\ar[ull]\\
%& + & && & - &
%}





{\bf Interprétation géométrique } (sur $\Rr$) : $\det(A)$ est une aire ou un volume \og orienté \fg.

\begin{exercicecours}
--- $\det A \neq 0 \iff A \mbox{ inversible }$ ;
 --- $\det (AB) = \det A \det B$
\end{exercicecours} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Déterminant en dimension quelconque}

\subsection{Arrangements}

Un {\it arrangement d'ordre $n$}\index{arrangement} est une suite 
\[\soul{k}:=(k_1,...,k_n)\]
des $n$ entiers $1,...,n$ dans un ordre quelconque. (Autrement dit une suite $(k_1,...,k_n)$ où chaque entier $1,...,n$ appara\^it exactement une fois.  

{\it Exemples}: les arrangements d'ordre $2$ sont $(1,2)$ et $(2,1)$. Les arrangements d'ordre $3$ sont $(1,2,3),(1,3,2),(2,1,3),(2,3,1),(3,1,2),(3,2,1)$. Plus généralement, il y a exactement $n!$ arrangements d'ordre $n$ ($n$ possibilité pour le premier terme, $n-1$ pour le deuxième, {\it etc}).


Soit $\soul{k}$ un arrangement. Une {\it inversion}\index{inversion} de $\soul{k}$ est une paire $\{k_i,k_j\}$ telle que $i<j$ et $k_i>k_j$ (\og c'est quand un plus grand est à gauche d'un plus petit \fg ).

On note $I(\soul{k})$\index{$I(\soul{k})$, ensemble des inversions} l'ensemble des inversions de $\soul{k}$.

On dit qu'un arrangement est {\it pair}\index{arrangement pair,impair} s'il a un nombre pair d'inversions ; on dit qu'il est {\it impair} s'il a un nombre impair d'inversions. Pour tout arrangement $\soul{k}$ on pose :
\[\epsilon(\soul{k}) := 1 \mbox{ si $\soul{k}$ est un arrangement pair }\]\[ -1\,  \mbox{ si $\soul{k}$ est un arrangement impair }\]

{\it Exemple :} voici les inversions et les signatures des $6$ arrangements d'ordre $3$ :

\[\begin{array}{|c|c|c|} 
\hline
\sigma & I(\sigma)\atop \# I(\sigma) & \epsilon (\sigma)\\
\hline
(1,2,3) &\varnothing \atop 0& 1\\
\hline

 (2,1,3) &\{\{1,2\}\}\atop 1& -1 \\
\hline
 (1,3,2) & \{\{2,3\}\} \atop 1 & -1 \\
\hline
 (2,3,1) &\{\{2,3\},\{1,3\}\}\atop 2&1\\
\hline
(3,1,2)&\{\{3,1\},\{3,2\}\} \atop 2&1\\
\hline
(3,2,1) &\{\{1,2\},\{2,3\},\{1,3\}\} \atop 3& -1 \\
\hline
\end{array}\]

\subsection{Définitions du déterminant}

\begin{definition}
Soit $A=(a_{i,j})_{1\le i,j\le n}$ une matrice. On note :
\[\det A := |A| := \sum_{\sigma \mathrm{\, arrangement\,d'ordre\,n}}\epsilon(\sigma) a_{\sigma(1),1}....a_{\sigma(n),n}\] le déterminant de $A$.
\end{definition}

\begin{exercicecours}
Pour $n=2,3$ on retrouve la définition usuelle.
\end{exercicecours}


\begin{proposition}[déterminant d'une matrice triangulaire]
Soit $T = (t_{i,j})_{1 \le i,j \le n}$ une matrice triangulaire supérieure c'est-à-dire $t_{i,j}=0$ si $i >j$. Alors :

[[ICI TABLEAU]]

%\[\left|\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%t_{1,1}\ar@{.}[rrr]\ar@{.}[rrrddd] & && t_{1,n}\ar@{.}[ddd]\\
%0\ar@{.}[dd]\ar@{.}[rrdd]&&&\\
%& &&\\
%0 \ar@{.}[rr]&& 0& t_{n,n}}}\right| 
% = t_{1,1} ... t_{n,n}\]
 
 
le produit des coefficients diagonaux. En particulier, \[|I_n| = 1\]. 
\end{proposition}

\begin{proof}
Par définition :
\[|T| = \sum_{\sigma } \epsilon(\sigma)t_{\sigma(1),1}...t_{\sigma(n),n}\]
(somme sur les arrangements $\sigma$ d'ordre $n$)

Or, le produit $t_{\sigma(1),1}...t_{\sigma(n),n}$ est nul sauf si, éventuellement, $\sigma(1) \le 1 ,...,\sigma(n) \le n$. Cela n'arrive que si $\sigma(1) = 1,...,\sigma(n) = n$ c'est-à-dire si $\sigma = (1,...,n)$. Donc :

\[|T| = \epsilon((1,2,...,n))t_{1,1}...t_{n,n}= t_{1,1}...t_{n,n} .\]
\end{proof}

En particulier, $\det (I_n) = 1$.

Cette définition du déterminant et \og la seule possible \fg\ au sens suivant :

\begin{theoreme}\label{th:det1}
Il existe une unique application :
\[D : \mathcal{M}_n(\Kk) \to \Kk \,,\; A \mapsto D(A)\]
i) linéaire en les colonnes de $A$, ii) alternée c'est-à-dire $D(A) =0$ si $A$ a deux colonnes identiques, iii) $D(I_n) =1$.

De plus, $D = \det$.
\end{theoreme}

\begin{proof}
Le $i)$ signifie que si on note $C_1,...C_n$ les colonnes d'une matrice $A$. Pour tout $j$, si $C'_j$ est un vecteur colonne, alors, pour tous $\lambda,\mu \in \Kk$, on a :
\[D(C_1|...|\lambda C_j + \mu C'_j|...|C_n) = \lambda D(C_1|...|C_j|...|C_n) + \mu D(C_1|...|C'_j|...|C_n).\]

{\it Existence} : Il est clair que $\det$ vérifie i) et iii). Vérifions ii) :

supposons que $A\in\mathcal{M}_{n}(\Kk)$ a ses colonness $C_i$ et $C_j$ identiques, $i<j$. Pour tout arrangement $\sigma$ d'ordre $n$, posons $\sigma'$ l'arrangement défini par :
\[\sigma'(p)=\left\{\begin{array}{l}
\sigma(p) \mbox{ si $p\neq i,j$,}\\
\sigma(j) \mbox{ si $p=i$,}\\
\sigma(i) \mbox{ si $p=j$.}
\end{array}\right.\]  

Alors \[(*)\;\; \epsilon(\sigma) = -\epsilon(\sigma').\] En effet,
\[(I(\sigma) \cup I(\sigma') )\setminus (I(\sigma) \cap I(\sigma') ) = \]
\[\{\{k_i,k_j\}\} \cup \{\{k_i,k_p\}\mid i <p < j\}\cup\{\{k_q,k_j\}\mid i <q < j\} \]
qui est un ensemble de cardinal $2(j-i)-1$
\exoo . Donc :
\[|I(\sigma)| + |I(\sigma')|=  2|I(\sigma) \cap I(\sigma') | + 2(j-i)-1\]
\[\implies |I(\sigma)| = |I(\sigma')| - 1 \mod 2\]
\[\implies \epsilon(\sigma) = -\epsilon(\sigma').\]

On a donc une bijection :
\[\left\{\sigma \mbox{ arrangement pair}\right\} \sta{1:1}{\to } \left\{\sigma \mbox{ arrangement impair}\right\}\]

\[\sigma \mapsto \sigma'\]

De plus, comme les colonnes $C_i$ et $C_j$ de la matrice $A$ sont identiques, on a :
\[a_{\sigma(1),1}....a_{\sigma(n),n} = a_{\sigma'(1),1}...a_{\sigma'(n),n}\]
pour tout arrangement $\sigma$. 

Donc :
\[\det A = \sum_{\sigma \mathrm{ arrangement\, pair}} a_{\sigma(1),1}....a_{\sigma(n),n} - \sum_{\sigma \mathrm{ arrangement\, impair}} a_{\sigma(1),1}....a_{\sigma(n),n} \]

\[= \sum_{\sigma \mathrm{ arrangement\, pair}} a_{\sigma(1),1}....a_{\sigma(n),n} - \sum_{\sigma \mathrm{ arrangement\, pair}} a_{\sigma'(1),1}....a_{\sigma'(n),n}\]
\[= \sum_{\sigma \mathrm{ arrangement\, pair}} a_{\sigma(1),1}....a_{\sigma(n),n} - a_{\sigma'(1),1}....a_{\sigma'(n),n}\]
\[=0 .\]

{\it Unicité :} Soit $D$ qui vérifie i), ii) de l'énoncé. Alors, si on note $E_1,...,E_n$ la base canonique de $\mathcal{M}_{n,1}(\Kk)$, on a :

\[C_j = \sum_{i=1}^n a_{i,j E_i}\]
pour toute colonne $C_j$ de $A$ et par linéarité :
\[D(A) = \sum_{i_1,...,i_n = 1}^n a_{i_1,1}...a_{i_n,n}D(E_{i_1}|...|E_{i_n}) .\]
Or, $D(E_{i_1}|...|E_{i_n}) = 0$ si les $i_1,...,i_n$ ne sont pas tous distincts. Donc :
\[D(A) =  \sum_{(i_1,...,i_n)\mathrm{\, arrangment}} D(E_{i_1}|...|E_{i_n}) .\]

Il reste à montrer que pour un arrangement $(i_1,...,i_n)$, $D(E_{i_1}|...|E_{i_n}) = \epsilon(i_1,...,i_n)D(I_n)$. On le démontre par récurrence  sur $k \ge 1$ tel que :
\[i_{k-1}>  i_k< ...<i_n.\]

Si $k=1$, c'est évident car alors, $i_1=1,...,i_n=n$. Si $k >1$, on échange $i_{k-1}$ et $i_k$ : on obtient un arrangement :
$(i'_1,...,i'_n)$ où $i'_j:=i_j$ si $j \neq k,k+1$, $i'_k:= i_{k-1}$ et $i'_{k-1}:=i_k$. Comme :
\[i'_{k-1}<...<i'_n\]
on a par hypothèse de récurrence :
\[D(E_{i'_1}|...|E_{i'_n}) = \epsilon(i'_1,...,i'_n) D(I_n).\]
Or, d'après  (*), on a :
\[\epsilon(i'_1,...,i'_n) = - \epsilon(i_1,...,i_n) .\]
De plus, on a :
\[D(E_{i_1}|...|E_{i_{k-1}}+E_{i_k}|E_{i_{k-1}}+E_{i_k}|...| E_{i_n}) = 0\]
\[\iff D(E_{i_1}|...|E_{i_{k-1}}|E_{i_{k-1}}|...| E_{i_n}) + D(E_{i_1}|...|E_{i_{k-1}}|E_{i_{k}}|...| E_{i_n}) \]
\[+ D(E_{i_1}|...|E_{i_k}|E_{i_{k-1}}|...| E_{i_n}) + D(E_{i_1}|...|E_{i_k}|E_{i_k}|...| E_{i_n}) = 0\]
\[ \iff D(E_{i_1}|...|E_{i_{k-1}}|E_{i_{k}}|...| E_{i_n}) + D(E_{i_1}|...|E_{i_k}|E_{i_{k-1}}|...| E_{i_n}) =0\]
\[\iff D(E_{i_1}|...|E_{i_n}) = -D(E_{i'_1}|...|E_{i'_n}) .\]

Conclusion : $D(E_{i_1}|...|E_{i_n}) = \epsilon(i_1,...,i_n)D(I_n)$.
\end{proof}

On a en fait démontrer le résultat suivant :
\begin{theoreme}\label{th:det2}
Soit $D: \mathcal{M}_n(\Kk) \to \Kk$ une application $n-$linéaire et alternée en les colonnes (c'est-à-dire qui vérifie i) et ii) du théorème précédent), alors : \[D(A) = \det A D(I_n).\]
\end{theoreme}

\subsection*{Déterminant de la transposée}

Si $A \in \mathcal{M}_{m,n}(\Kk)$, on note ${}^tA \in \mathcal{M}_{n,m}(\Kk)$ la matrice de coefficients :
\[({}^tA)_{i,j} := A_{j,i}\]
pour tous $1 \le i \le m,\,1 \le j \le n$.

\begin{theoreme}
Soit $A \in \mathcal{M}_n(\Kk)$. Alors, $\det ({}^tA) = \det A$.
\end{theoreme}

\begin{corollaire}
Dans les théorèmes \ref{th:det1} et \ref{th:det2}, on peut remplacer \og colonne \fg\ par \og ligne \fg\ . 
\end{corollaire}

\begin{proof}
Si $\sigma$ est un arrangement d'ordre $n$, on note $\sigma^{-1}:= (l_1,...,l_n)$ l'arrangementy d'ordre $n$ tel que $\sigma_{l_i} = i$ pour tout $i$. Si $1 \le i \neq j \le n$, alors $\left\{\sigma_i,\sigma_j\right\}$ est une inversion de $\sigma$ si et seulement si $\left\{i,j\right\}$ est une inversion de $\sigma^{-1}$ \exoo . En particulier, $\epsilon(\sigma) = \epsilon(\sigma^{-1})$ pour tout arrangement $\sigma$. 

Or, on a :
\[a_{\sigma(1),1}...a_{\sigma(n),n} =a_{\sigma(\sigma^{-1}(1)),\sigma^{-1}(1)}...a_{\sigma(\sigma^{-1}(n)),\sigma^{-1}(n)} \]
\[= a_{1,\sigma^{-1}(1)}...a_{n,\sigma^{-1}(n)} .\]

Donc :
\[\det A = \sum_{\sigma} \epsilon(\sigma) a_{\sigma(1),1}...a_{\sigma(n),n}\]
\[=  \sum_{\sigma} \epsilon(\sigma) a_{1,\sigma^{-1}(1)}...a_{n,\sigma^{-1}(n)}\]
\[= \sum_{\sigma} \epsilon(\sigma^{-1}) a_{1,\sigma(1)}...a_{n,\sigma(n)}\]
\[=\sum_{\sigma} \epsilon(\sigma) a_{1,\sigma(1)}...a_{n,\sigma(n)}\]
\[= \det ({}^tA) .\]


\end{proof}
{\bf Conséquences :}

\begin{theoreme}[déterminant du produit]
Soient $\,B \in \mathcal{M}_n(\Kk)$ : \[\det(AB) = \det A \det B .\]
\end{theoreme}
\begin{proof}
En effet, si $A$ est fixée, l'application : 
\[F \,:\, B \mapsto \det(AB)\]
est $n-$linéaire alternée en les colonnes de $B$. Donc :
\[\Qq B \in \mathcal{M}_n(\Kk),\, F(B) = \det B F(I_n)\]
\[=\det A \det B .\] 
\end{proof}

\begin{exemple}
Pour les matrices de Pascal de la page \pageref{matdeP}, on trouve : \[\det P = \det T_-T^+ = 1 .\]
\end{exemple}

\begin{proposition}[déterminant des matrices triangulaires par blocs]
Si $A \in \mathcal{M}_m(\Kk)$, $B \in \mathcal{M}_{m,n}(\Kk)$, $D \in \mathcal{M}_{n}(\Kk)$, alors :
\[\left|\begin{array}{c|c}
A & B\\
\hline 
0 & D
\end{array}\right| = \det A \det D .\]
\end{proposition}

\begin{proof}
Fixons $A$. L'application :
\[D \mapsto \left|\begin{array}{c|c}
A & B\\
\hline 
0 & D
\end{array}\right|\]
est $n-$linéaire alternée en les lignes de $D$ donc :

\[\left|\begin{array}{c|c}
A & B\\
\hline 
0 & D
\end{array}\right| = \det D \left|\begin{array}{c|c}
A & B\\
\hline 
0 & I_n 
\end{array}\right| \]
Ensuite $B$ étant fixé, l'application :
\[A \mapsto \left|\begin{array}{c|c}
A & B\\
\hline 
0 & I_n
\end{array}\right| \]
est $n-$linéaire alternée en les colonnes de $A$ donc :

\[\left|\begin{array}{c|c}
A & B\\
\hline 
0 & I_n
\end{array}\right| = \det A \left|\begin{array}{c|c}
I_m & B\\
\hline 
0 & I_n
\end{array}\right| \]
enfin, on sait calculer le déterminant d'une matrice triangulaire supérieure (on fait le produit des termes diagonaux) :
\[\left|\begin{array}{c|c}
I_m & B\\
\hline 
0 & I_n
\end{array}\right|  = 1\]
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Règle de Cramer}

Notation : Soit $A$ une matrice $n \times n$. On note $A^{i,j}$ la matrice obtenue en biffant la ligne $i$ et la colonne $j$ de $A$.


On peut calculer un déterminant $n \times n$ si on sait calculer un déterminant $(n-1) \times (n-1)$ :

\begin{proposition}[Développement par rapport à une ligne ou une colonne]

Soit $A$ une matrice $n \times n$. Alors: 
\[\Qq 1 \le j \le n, \, \det A = \sum_{i=1}^n (-1)^{i+j}a_{i,j}|A^{i,j}| \]
\[\Qq 1 \le i \le n,\, \det A = \sum_{j=1}^n (-1)^{i+j}a_{i,j}|A^{i,j}| .\]

\end{proposition}

\begin{proof}
Par $n-$linéarité du déterminant selon les colonnes, comme on a :


[[ICI TABLEAU]]

%\[A = \sum_{i=1}^n a_{i,j}\bordermatrix{
%&&&j\atop |&&\cr
%&a_{1,1} & ... & 0 & ... & a_{1,n}\cr
%&\vdots & & \vdots & &\vdots\cr
%&a_{i,1} & & 1 && a_{i,n}\cr
%&\vdots && \vdots && \vdots\cr
%&a_{n,1}&...& 0 &...& a_{n,n}\cr
%} \; , \] on a :
%\[\det A = \sum_{i=1}^n a_{i,j}\bordermatrix[{||}]{
%&&&j\atop |&&\cr
%&a_{1,1} & ... & 0 & ... & a_{1,n}\cr
%&\vdots & & \vdots & &\vdots\cr
%&a_{i,1} & & 1 && a_{i,n}\cr
%&\vdots && \vdots && \vdots\cr
%&a_{n,1}&...& 0 &...& a_{n,n}\cr
%} .\]

Or en échangeant la colonne $j$ avec  la colonne $j-1$ puis la colonne $j-1$ avec la colonne $j-2$, etc, on trouve :
\[\left|\begin{array}{ccccc}
a_{1,1} & ... & 0 & ... & a_{1,n}\\
\vdots & & \vdots & &\vdots\\
a_{i,1} & & 1 && a_{i,n}\\
\vdots && \vdots && \vdots\\
a_{n,1}&& 0 && a_{n,n}
\end{array}\right| = (-1)^{j-1} \left|\begin{array}{cccc}
0 &a_{1,1} & ...  & a_{1,n}\\
\vdots & \vdots&&\vdots\\
1& a_{i,1}& ...&a_{i,n}\\
\vdots &\vdots && \vdots\\
0 & a_{n,1}&...& a_{n,n}
\end{array}\right|\]

ensuite,  en échangeant la ligne $i$ avec la ligne $i-1$ puis la ligne $i-1$ avec la ligne $i-2$, etc, on obtient :
\[\left|\begin{array}{ccccc}
a_{1,1} & ... & 0 & ... & a_{1,n}\\
\vdots & & \vdots & &\vdots\\
a_{i,1} & & 1 && a_{i,n}\\
\vdots && \vdots && \vdots\\
a_{n,1}&& 0 && a_{n,n}
\end{array}\right| = (-1)^{j-1}(-1)^{i-1} \left|\begin{array}{cccc}
1 &a_{i,1} & ...  & a_{i,n}\\
0&a_{1,1} &...&a_{1,n}\\
\vdots& \vdots&&\vdots\\
0 & a_{n,1}&...& a_{n,n}
\end{array}\right|\]
\[= (-1)^{i+j}|A^{i,j}| .\]

Et on démontre de même la formule de développement par rapport à la ligne $i$.
\end{proof}

\begin{exemple}
\[\left|\begin{array}{ccc}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{array}\right| = \left|\begin{array}{cc}
5& 6\\
8 & 9
\end{array}\right| -4  \left|\begin{array}{cc}
2&3\\
8&9
\end{array}\right| + 7  \left|\begin{array}{cc}
2&3\\
5&6
\end{array}\right| = 0 .\]
\end{exemple}

\begin{proposition}[formule de Cramer pour les solutions des systèmes linéaires]
Si :

\[\left\{ \begin{array}{ccc}
a_{1,1}x_1 + ... + a_{n,1}x_n &=& y_1\\
&...&\\
a_{n,1}x_1 + ... + a_{n,n}x_n & =& y_n
\end{array}
\right.\]

alors, $\det A x_k = \det A_k$
où $A_k$ est la matrice obtenue en rempla\c{c}ant la $k-$ième colonne de $A$ par la colonne  $\left( \begin{array}{c}
y_1\\
\vdots\\
y_n
\end{array}\right)$. 
\end{proposition}

\begin{proof}
On développe par rapport à la $k-$ième colonne :
\[\det A_k = \sum_{i=1}^n y_i(-1)^{i+k}|A^{i,k}| \]
(on remplace les $y_i$ par leur expression en fonction des $x_j$) :
\[\det A_k = \sum_{i=1}^n\sum_{j=1}^na_{i,j}x_j(-1)^{i+k}|A^{i,k}|\]
\[= \sum_{j=1}^n \left( \sum_{i=1}^n a_{i,j}(-1)^{i+k}|A^{i,k}|\right) x_j .\]

Or : \[\sum_{i=1}^n a_{i,j}(-1)^{i+k}|A^{i,k}|\]
est le déterminant de la matrice obtenue en rempla\c{c}ant la colonne $k$ de la matrice $A$ par la colonne $j$ \exoo. Donc :
 \[\sum_{i=1}^n a_{i,j}(-1)^{i+k}|A^{i,k}| = \left\{\begin{array}{l}
0 \si k \neq j\\
\det A \sinon.
\end{array}\right.\]
\end{proof}

\begin{remarque*}
Si $\det A \neq 0$, alors $A$ inversible. En effet, dans ce cas, les formules de Cramer montrent que l'on peut inverser le système défini par $A$.
\end{remarque*}

Plus précisément, on peut décrire la matrice inverse de $A$ si $\det A \neq 0$.

\begin{definition}[Comatrice]
Soit $A$ une matrice $n \times n$, sa comatrice, notée $\mathrm{com
}(A)$ ou $ \tilde{A}$ est la matrice $n \times n$ dont le $(i,j)-$ième coefficient est : \[ \text{com}{A}_{i,j} = (-1)^{i+j}|A^{i,j}|.\]
\end{definition}

\begin{corollaire}
Pour toute matrice $A$ de taille $n \times n$ :
\[{}^t\tilde{A} A = A{}^t\tilde{A}=\det A I_n \] 
\end{corollaire}

\begin{proof}
En effet, le $(i,j)-$ème coefficient de ${}^t\tilde{A} A $ est donnée par la formule :
\[\sum_{k=1}^n(-1)^{i+k}a_{k,j}|A^{k,i}|\]
qui est le déterminant de la matrice obtenue en rempla\c{c}ant, dans la matrice $A$, la colonne $i$ par la colonne $j$. Donc :
\[({}^t\tilde{A} A )_{i,j} = \sum_{k=1}^n(-1)^{i+k}a_{k,j}|A^{k,i}|\]
\[= \left\{\begin{array}{c}
0 \si i \neq j \\
\det A \si i=j.
\end{array}\right.\]
\end{proof}

\begin{remarque*}
Cette formule reste vraie si $\Kk$ est remplacé par un anneau commutatif (p. ex : $\Zz,  \Kk[T]$).
\end{remarque*}

\begin{exemple}

--- Si $ad-bc \neq 0$, \[\left(\begin{array}{cc}
a & b \\
c & d
\end{array}
\right)^{-1} = \frac{1}{ad-bc} \left(\begin{array}{cc}
d & -b \\
-c & a
\end{array}
\right)\]

--- Si $A$ est une matrice $3 \times 3$ et si $|A| \neq 0$, alors :

\[A^{-1}=\frac{1}{|A|} \left(\begin{array}{ccc}
|A^{1,1}|& -|A^{2,1}| & |A^{3,1}|\\
-|A^{1,2}| & |A^{2,2}| & -|A^{3,2}|\\
|A^{1,3}| & -|A^{2,3}| & |A^{3,3}|
\end{array}\right) .\]
\end{exemple}

\begin{theoreme}
$A$ est inversible $\iff \det A \neq 0$ et l'inverse est donné par :

\[A^{-1} = \frac{1}{\det A} {}^t\tilde{A} = \frac{1}{\det A} \left(\pm |A^{j,i}|\right)\]
\end{theoreme}

Terminons ce chapitre par quelques déterminants remarquables :

\begin{exercicecours}
Déterminant de Vandermonde. C'est le déterminant $(n+1) \times (n+1)$ suivant :


[[ICI TABLEAU]]

%\[V(x_0,...,x_n) = \left|\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%1\ar@{.}[rrr] & && 1\\
%x_0\ar@{.}[dd]\ar@{.}[rrr]&&&x_n\ar@{.}[dd]\\
%& &&\\
%x_0^n \ar@{.}[rrr]&& & x_n^{n}}}\right| .\]

On a : $V(x_1,...,x_n) = \prod_{0\le i < j \le n}(x_j-x_i)$.

{\it Indication : on peut raisonner par récurrence et remarquer que : $V(x_0,...,x_n)$ est un polynôme de degré $\le n$ en $x_n$, de coefficient dominant $V(x_0,...,x_{n-1})$ qui s'annule lorsque $x_n=x_0,...,x_{n-1}$ ; donc : $V(x_0,...,x_n) = V(x_0,...,x_{n-1})(x_n-x_0)...(x_n-x_{n-1})$}.

{\it Conséquence : }
Si $x_1,...,x_n \in \Cc$, alors :
\[\left\{ \begin{array}{c}
x_1 + ... + x_n= 0 \\
\vdots\\
x_1^{n} + ... + x_n^{n} = 0 
\end{array}\right. \implies x_1=...=x_n =0 \]
({\it ce système n'est pas linéaire}).
\end{exercicecours}


\begin{exercicecours}
Montrer que le déterminant $n \times n$ suivant :

[[ICI TABLEAU]]

%\[
%\left|
%     \raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%         1
% \ar @{-}[ddddrrrr] &  -1 \ar @{-}[dddrrr] & 0 \ar @{.}[rr] \ar@{.}[ddrr]& & 0 \ar@{.}[dd]\\
%         1  \ar@{-}[dddrrr]&&&&\\
%         0\ar@{.}[dd] \ar@{.}[ddrr]&&&& 0\\
%         &&&& -1 \\
%         0 \ar@{.}[rr] & & 0& 1& 1
%       }%
%     }
%   \right|
%\]


est le $n+1-$ième nombre de Fibonacci $f_{n+1}$ (voir p. \pageref{fibo}).
\end{exercicecours}

Enfin voici une autre fa\c{c}on de calculer un déterminant $3 \times 3$ :

\begin{exercicecours}
Soit $A$ une matrice $3 \times 3$. Alors si $a_{2,2} \neq 0$ :

[[ICI TABLEAU]]

%\[|A| =\frac{\left|
%\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{%
%    {     \left|\begin{array}{cc}
%a_{1,1} & a_{1,2}\\
%a_{2,1}& a_{2,2}
%\end{array}\right|}&  {     \left|\begin{array}{cc}
%a_{1,2} & a_{1,3}\\
%a_{2,2}& a_{2,3}
%\end{array}\right|}\\
%     {     \left|\begin{array}{cc}
%a_{2,1} & a_{2,2}\\
%a_{3,1}& a_{3,2}
%\end{array}\right|}   &  {     \left|\begin{array}{cc}
%a_{2,2} & a_{2,3}\\
%a_{3,2}& a_{3,3}
%\end{array}\right|}  
%       }%
%     }\right|
%}{a_{2,2}}\]

\end{exercicecours}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Déterminant d'un endomorphisme}

Soit $E$ un $\Kk-$espace vectoriel de dimension finie. Soit $u$ un endomorphisme de $E$. Le déterminant :
\[\det \left(\Mat(u)_{(e)}\right)\]
est indépendant de la base $(e)$ de $E$ choisie. 

En effet, les matrices de $u$ dans $2$ bases différentes sont semblables et par {\it multiplicativité} du déterminant :
\[\Qq A \in \mathcal{M}_n(\Kk),\, \Qq P \in \GL_n(\Kk),\, \det (P AP^{-1}) = \det P \det A \det (P^{-1}) \]\[= \det A \det P \det P^{-1}\]\[ = \det A \]
(leurs déterminants sont égaux).

On peut donc définir le déterminant de $u $ :
\begin{definition}[déterminant d'un endomorphisme]
\[\det u := \det A\]
 où $A$ est la matrice de $u$ dans une base quelconque de $E$.
\end{definition}

\begin{remarque*}[(s) importantes]

--- $\det \id_E = 1$ et pour tous $ u,v$  endomorphismes de $E$, 
\[\det (u\circ v) = \det u \det v\]

--- $u$ est un isomorphisme $\iff \det u \neq 0$ $\iff \Mat(u)_{(e)}$ inversible (pour toute base $(e)$ de $E$. 


En effet, $u$ est un isomorphisme $\iff$ $\Mat(u)_{(e)}$ est inversible (quelle que soit la base $(e)$ de $E$ choisie \exoo.
\end{remarque*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Valeurs propres, vecteurs propres}
Dans ce chapitre $E$ est un $\Kk-$espace vectoriel.
 
 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sous-espaces invariants}


\begin{definition}[sous-espace invariant]
Soit $u$ un endomorphisme de $E$. On dit qu'un sous-$K-$espace vectoriel $F$ de $E$ est {\it invariant}, ou {\it stable}, par $u$ si :
\[\Qq x \in F,\, u(x) \in F .\]

On note alors $u\Res{F} : F \to F$, $x \in F \mapsto u(x) \in F$ la {\it restriction} de $u$ à $F$. L'application $u\Res{F}$ est un endomorphisme de $F$.

\end{definition}

{\bf Effet sur les matrices :}

Supposons que $E$ est de dimension $n$, que $u$ est un endomorphisme de $E$, que $F$ est un sous-espace de $E$ invariant par $u$. Alors si :
\[e_1,...,e_k\]
est une base de $F$, on peut la compléter en une base de $E$ :
\[(e) = e_1,...,e_k,e_{k+1},...,e_n.\]

La matrice de $u$ dans la base $(e)$ est {\it triangulaire} par blocs :

[[ICI TABLEAU]]

%\[\Mat(u)_{(e)}  = \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{a_{1,1}\ar@{.}[dd]\ar@{.}[rr]&&a_{1,k}\ar@{.}[dd]&b_{1,1}\ar@{.}[dd]\ar@{.}[rr]&&b_{1,n-k}\ar@{.}[dd]\\
%&&&&\\
%a_{k,1}\ar@{.}[rr]&&a_{k,k}&b_{k,1}\ar@{.}[rr]&&b_{k,n-k}\\
%0 \ar@{-}[dd]\ar@{-}[rr]&&0\ar@{-}[dd]&d_{1,1}\ar@{.}[dd]\ar@{.}[rr]&&d_{1,n-k}\ar@{.}[dd]\\
%&&&&&\\
%0\ar@{-}[rr]&&0&d_{n-k,1}\ar@{.}[rr]&&d_{n-k,n-k}
%}}\right)
%\]


où $A:=(a_{i,j})_{1 \le i,j \le k}\in \mathcal{M}_k(\Kk)$ est la matrice de $u\Res{F}$ dans la base $e_1,...,e_k$ de $F$, $B:= (b_{i,j})_{1 \le i \le n-k\atop 1 \le j \le n-k} \in \mathcal{M}_{k,n-k}(\Kk)$, $D:= (d_{i,j})_{1 \le i,j \le n-k}\in \mathcal{M}_{n-k}(\Kk)$.

\begin{remarque*}
 si le sous-epace :
\[G:=\langle e_{k+1},...,e_n\rangle\]
est aussi stable par $u$, le bloc rectangulaire $B$ est nul et :

[[ICI TABLEAU]]
%
%\[\Mat(u)_{(e)} = \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{a_{1,1}\ar@{.}[dd]\ar@{.}[rr]&&a_{1,k}\ar@{.}[dd]&0\ar@{-}[dd]\ar@{-}[rr]&&0\ar@{-}[dd]\\
%&&&&\\
%a_{k,1}\ar@{.}[rr]&&a_{k,k}&0\ar@{-}[rr]&&0\\
%0 \ar@{-}[dd]\ar@{-}[rr]&&0\ar@{-}[dd]&d_{1,1}\ar@{.}[dd]\ar@{.}[rr]&&d_{1,n-k}\ar@{.}[dd]\\
%&&&&&\\
%0\ar@{-}[rr]&&0&d_{n-k,1}\ar@{.}[rr]&&d_{n-k,n-k}
%}}\right)\]


qui est une matrice {\it diagonale} par blocs.

\end{remarque*}
\begin{exemple}
Soit $e_1,e_2,e_3$ la base canonique de $\R^3$. Soit $r_\theta$ la rotation d'axe $e_3$ et d'angle $\theta$. L'endomorphisme $r_\theta$ de $\R^3$ laisse invariants les sous-espaces :
\[\langle e_1,e_2 \rangle \text{ et } \langle e_3 \rangle\]
et sa matrice dans la base $e_1,e_2,e_3$ est la matrice :
\[\left( \begin{array}{ccc}
\cos \theta & -\sin \theta& 0\\
\sin \theta & \cos \theta & 0\\
0&0&1
\end{array}\right) .\]
\end{exemple}

Les droites invariantes sont appelées {\it droites propres}, ce sont les droites engendrées par les vecteurs propres.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Vecteurs propres}
\begin{definition}[vecteurs,valeurs propres, spectre]
Soit $u$ un endomorphisme de $E$. Un {\it vecteur propre} \index{vecteur propre}de $u$ est un vecteur {\bf non nul}  $x \in E\setminus\{0\}$ tel que :
\[u(x) = \lambda x\]
pour un certain scalaire $\lambda \in \Kk$. On dit que $\lambda$ est la {\it valeur propre} \index{valeur propre} de $u$ associée au vecteur propre $x$. On dit aussi que $x$ est un vecteur propre associé à la valeur propre $\lambda$.

Le spectre\index{spectre} de $u$ est l'ensemble des valeurs propres de $u$. Notation : $\Sp_\Kk(u)$ (ou $\Sp(u)$). 
\end{definition}

{\bf Version matricielle} :

Soit $A \in \mathcal{M}_n(\Kk)$.  Un {\it vecteur propre} de $A$ est un vecteur {\bf non nul}  $X \in \Kk^n\setminus\{0\}$ tel que :
\[A X = \lambda X\]
pour un certain scalaire $\lambda \in \Kk$. On dit que $\lambda$ est la {\it valeur propre} de $A$ associée au vecteur propre $X$. On dit aussi que $X$ est un vecteur propre associé à la valeur propre $\lambda$.

Le spectre de $A$ est l'ensemble des valeurs propres de $A$. Notation : $\Sp_\Kk(A)$ (ou $\Sp(A)$ si le corps où l'on se place est évident). 

\vskip 1cm
\begin{exemple}[s]

--- soient $E= \R^3$, $u=r_\theta$ la rotation d'axe $e_3$ et d'angle $\theta$. Alors $r_\theta(e_3)=e_3$. Donc $e_3$ est un vecteur propre de $r_\theta$ ; la valeur propre associée est $1$.

--- Soit $E=\Cc[X]_{\le n}$ l'espace des polynômes complexes de degré $\le n$. Soit $u=\partial : E \to E$, $P(X) \mapsto P'(X)$. Pour des raisons de degré, \[P' =\lambda P \implies \lambda =0 \text{ et } P \mbox{ constant}\]
de plus, tout polynôme constant non nul est un vecteur propre de $\partial$ de valeur propre associée $0$ ; donc $\Sp(\partial) = \{0\}$.

--- (Cet exemple est en dimension infinie) Soit $E =\mathcal{C}^\infty(\R)$ l'espace des fonctions infiniment dérivables de $\R$ dans $\R$. Soit $u =\partial 
 : E \to E$, $f \mapsto f'$.

Pour tout $\lambda \in \R$, posons \[e_\lambda : \R \to \R ,\, x \mapsto e^{\lambda x}.\]
On a : $e_\lambda' = \lambda e_\lambda$ donc chaque fonction $e_\lambda$ est un vecteur propre de $\partial$ de valeur propre associée $\lambda$.

--- Soit $A := \left(\begin{array}{cc}
0 & 1\\
1 &1
\end{array}\right)$. Le réel $\alpha :=\frac{1+\sqrt{5}}{2}$ est une valeur propre de $A$. En effet :
\[A.\left(\begin{array}{c}
1\\
\alpha
\end{array}\right) = \alpha \left(\begin{array}{c}
1\\
\alpha
\end{array}\right)\]
\exoo

--- Soit $A := \left(\begin{array}{cc}
0 & 1\\
-1 &-1
\end{array}\right)$. Le complexe $j :=-\frac{1}{2} + i \frac{\sqrt{3}}{2} = e^{i\frac{2\pi}{3}}$ est une valeur propre de $A$. En effet :

\[A \left(\begin{array}{c}
1\\
j
\end{array}\right) = j \left(\begin{array}{c}
1\\
j
\end{array}\right)\]
\exoo
 \end{exemple}

\og Comment trouver les valeurs propres d'un endomorphisme ou d'une matrice parmi tous les éléments de $\Kk$  ? \fg\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Polynôme caract\'eristique}

\begin{proposition}\label{pro:vprac}
Soient $A \in \mathcal{M}_n(\Kk)$ et $\lambda \in \Kk$. Alors :
\[\lambda \mbox{ est une valeur propre de $A$ } \iff \det (A-\lambda I_n) = 0 .\]
\end{proposition}

\begin{proof}
$\lambda$ n'est pas une valeur propre de $A$ \[\iff \Qq X \in \Kk^n \setminus\{0\},\, AX \neq \lambda X c'est-à-dire (A -\lambda I_n)X \neq 0\]
\[\iff \Ker(A-\lambda I_n) = \{0\}\]
\[\iff A- \lambda I_n \mbox{ injective}\]
\[\iff A -\lambda I_n \mbox{ inversible}\]
\[\iff \det (A-\lambda I_n) \neq 0 .\] 
\end{proof}

\begin{definition}[polynôme caractéristique]
Soit $ A \in \mathcal{M}_n(\Kk)$.
Le {\it polynôme caractéristique}\index{polynôme caractéristique} de $A$ est :
$\chi_A(X) := \det(XI_n -A).$ 
\end{definition}

\begin{remarque*}[s]
--- La matrice $XI_n -A$ est à coefficients dans $\Kk[X]$ donc son déterminant $\chi_A(X) \in \Kk[X]$. 

--- Pour tout $\lambda \in \Kk$, $\det (A- \lambda I_n) = (-1)^n\chi_A(\lambda)$.

\end{remarque*}

\begin{exemple}[s]

--- Si $n=2$ :

\[\Qq A =\left(\begin{array}{cc}
a & b\\
c &d
\end{array}\right),\, \chi_A(X) = X^2 -(a+d)X + (ad-bc)\]
\[= X^2 -(\tr A) X + \det A\]
où $\tr A := a+d$.

--- Si $n=3$, 

\[\Qq A =\left(\begin{array}{ccc}
a_{1,1}& a_{1,2} & a_{1,3}\\
a_{2,1}& a_{2,2} & a_{2,3}\\
a_{3,1} & a_{3,2} & a_{3,3}
\end{array}\right),\, \chi_A(X) = X^3 - (\tr A) X^2 + s_2 X -\det A\]
où $\tr A := a_{1,1}+ a_{2,2} + a_{3,3}$ et :
\[s_2:= \left|\begin{array}{cc}
a_{1,1}&a_{1,2}\\
a_{2,1}&a_{2,2}
\end{array}\right| + \left|\begin{array}{cc}
a_{2,2}&a_{2,3}\\
a_{3,2}&a_{3,3}
\end{array}\right| +\left|\begin{array}{cc}
a_{1,1}&a_{1,3}\\
a_{3,1}&a_{3,3}
\end{array}\right|\]
(c'est la trace de la comatrice de $A$).

--- $n$ quelconque :
\[\Qq A \in \mathcal{M}_n(\Kk),\, \chi_A(X) = X^n -s_1 X^{n-1} + ... + (-1)^ns_n\]
où pour tout $1 \le d\le n$, le coefficient devant $(-1)^d X^{n-d}$ est :
\[s_d:= \sum_{I \subset \{1,...,n\}\atop |I| = d} |A_I|\]
avec pour tout $I =\{k_1,...,k_d\}$, tel que $k_1 < ... <k_d$, 

[[ICI TABLEAU]]
%\[A_I := \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{a_{k_1,k_1}\ar@{.}[rr]\ar@{.}[dd]&&a_{k_1,k_d}\ar@{.}[dd]\\
%&&\\
%a_{k_d,k_1}\ar@{.}[rr]&&a_{k_d,k_d
%}
%}
%}\right) \in \mathcal{M}_{d}(\Kk)\]


(c'est la matrice obtenue en ne gardant de $A$ que les lignes et les colonnes $k_1,,,k_d$).

\begin{proof}
On pose $P(X_1,...,X_n):=\left | \begin{array}{cccc}
X_1 -a_{1,1}& -a_{1,2}&...&\\
-a_{2,1} & X_2-a_{2,2}&...&\\
\vdots & &\ddots &\\
&&&X_n-a_{n,n}
\end{array}\right |$. C'est un polynôme en les variables $X_1,...,X_n$ et à coefficients dans $\Kk$. On montre par récurrence sur $k$ que pour $1\le i_1 <...<i_k\le n$, le coefficient devant le monôme $X_{i_1}...X_{i_k}$ est :
\[(-1)^{n-k}\det A^I\]
où $I := \{i_1,...,i_k\}$ et $A^I$ est la matrice obtenue à partir de $A$ en retirant les lignes et les colonnes $i_1,...,i_k$ (il suffit de développer par rapport à la ligne $i_1$ puis $i_2,...$).

En particulier, \[P(X,...X) = \chi_A(X) = \sum_{k=0}^n  (-1)^{n-k}(\sum_{1 \le i_1 <...<i_k}|A^{\{i_1,...,i_k\}} |)X^k\]
\[= \sum_{k=0}^n (-1)^k (\sum_{1 \le i_1 <...<i_k}|A_{\{i_1,...,i_k\}} |)X^{n-k}.\] 
\end{proof}

{\bf \`A retenir : } le polynôme $\chi_A(X)$ est unitaire (c'est-à-dire son coefficient de plus haut degré est $1$)
 de degré $n$ et :
\[\begin{array}{l}
s_1 = \sum_{i=1}^n a_{i,i} = : \tr A\\
s_2 = \displaystyle \sum_{1 \le i < j \le n} \left|\begin{array}{cc}
a_{i,i}&a_{i,j}\\
a_{j,i}&a_{j,j}
\end{array}\right|\\
s_n = \det A.
\end{array}
\]\end{exemple}

\begin{definition}[deux définitions équivalentes de la trace]
Soit $A \in \mathcal{M}_n(\Kk)$. On définit la {\it trace}\index{trace} de $A$ par :
\[\tr A := \mbox{ $-$ le coefficient devant $X^{n-1}$ dans $\chi_A(X)$}\]
ou par :
\[\tr A := \mbox{ la somme des coefficients diagonaux de $A$.}\]
\end{definition}

\begin{theoreme}[polynôme caractéristique d'un produit]
Soient $m,n$ des entiers $\ge 1$. Si $A \in \mathcal{M}_{m,n}(\Kk)$ et $B \in \mathcal{M}_{n,m}(\Kk)$, alors :
\[AB \in \mathcal{M}_m(\Kk) \text{ et } BA \in \mathcal{M}_{n}(\Kk)\] 
et :
\[X^n\chi_{AB}(X) = X^m\chi_{BA}(X)\]
dans $\Kk[X]$.

En particulier, si $m=n$ alors :\[\chi_{AB} (X) = \chi_{BA}(X) .\]
\end{theoreme}
\begin{proof}
On pose :
\[M:=\left(\begin{array}{c|c}
XI_m & -A\\\hline
0 & I_n
\end{array}\right) \text{ et } N := \left(\begin{array}{c|c}
I_m & A\\\hline
B & XI_n
\end{array}\right) \; \in \mathcal{M}_{m+n}(\Kk) .\]
On a alors :
\[MN = \left(\begin{array}{c|c}
XI_m -AB& 0\\\hline
XB & XI_n
\end{array}\right)\]
donc :
\[\det (MN) = \det (X I_m -AB) \det (X I_n) \]
\[= X^n\chi_{AB}(X) .\] D'un autre côté, \[ NM = \left(\begin{array}{c|c}
XI_m & 0\\\hline
XB & XI_n-BA
\end{array}\right)\]
donc \[\det (NM) = \det (XI_m)\det(X I_n-BA) \]\[ = X^m \chi_{BA}(X) .\]
Or, $\det (MN) = \det (NM) = \det M \det N$.
\end{proof}

\begin{remarque*}
--- Si $A,A' \in \mathcal{M}_n(\Kk)$ sont des matrices semblables c'est-à-dire si \[\exists P \in \GL_n(\Kk),\, A = PA'P^{-1}\]
alors :
\[\chi_{A}(X) = \chi_{P(A'P^{-1})}(X)\]
\[=\chi_{(A'P^{-1})P}(X) \]
\[= \chi_{A'}(X)\]
autrement dit deux matrices semblables ont le même polynôme caractéristique.
\end{remarque*}

En conséquence, on peut définir le polynôme caractéristique d'un endomorphisme :
\begin{definition}
Supposons que $E$ est de dimension finie. Si $u$ est un endomorphisme de $E$, alors toutes les matrices de $u$ dans une base de $E$ sont semblables donc ont le même polynôme caractéistique. Ce polynôme caractéristique commun est le polynôme caractéristique de $u$, noté $\chi_u(X)$.
\end{definition}

Concrétement, si $(e)= e_1,...,e_n$ est une base de $E$, si $u$ est un endomorphisme de $E$, alors :
\[\chi_u(X) = \chi_A(X)\]
où $A:= \Mat(u)_{(e)}$.
\subsection*{Spectre et racines du polynôme caractéristique}

On peut réécrire la proposition \ref{pro:vprac} :

\begin{theoreme}
Soit $A \in \mathcal{M}_n(\Kk)$. Alors :
\[\Sp(A) = \left\{ \mbox{ valeurs propres de $A$ }\right\} = \left\{ \mbox{ racines de $\chi_A(X)$ }\right\}\]
\end{theoreme}

\begin{proof}
On a $\lambda$ valeur propre de $A$ $\iff$ $\det (A -\lambda I_n) = 0$ $\iff \chi_A(\lambda) = 0$.
\end{proof}

En particulier :

\begin{corollaire}
Si $A \in \mathcal{M}_n(\Kk)$, $A$ possède au plus $n$ valeurs propres. 
\end{corollaire}

(En effet, nous savons qu'un polynôme de degré $n$ a au plus $n$ racines).

\begin{exemple}[s]

--- Si $A = \left(\begin{array}{cc}
0 & 1\\
1 &1
\end{array}\right)$, alors \[\chi_A(X) = X^2 -X -1 = (X-\frac{1-\sqrt{5}}{2})(X-\frac{1+\sqrt{5}}{2})\]
donc $\Sp_\R(A) = \{\frac{1-\sqrt{5}}{2}, \frac{1+\sqrt{5}}{2}\}$ mais $\Sp_\Qq(A) = \varnothing$.

--- Si $A =  \left(\begin{array}{cc}
0 & 1\\
-1 &-1
\end{array}\right)$, alors :
\[\chi_A(X) = X^2 + X +1 = (X-j)(X-j^2)\]
où $j := -\frac{1}{2}+i\frac{\sqrt{3}}{2}$. Donc dans ce cas :
\[\Sp_\Cc(A) = \left\{j,j^2\right\}\]
mais $\Sp_\R(A) = \varnothing$.

--- Si $A= \left(\begin{array}{cc}
\cos \theta & -\sin \theta\\
\sin \theta &\cos \theta
\end{array}\right)$, alors :
\[\chi_A(X) = X^2-2\cos \theta X +1 = (X-e^{i\theta}) (X-e^{-i\theta})\]
et $\Sp_\Cc(A) = \{e^{-i\theta},e^{i\theta}\}$. (Si $\theta \neq 0 \mod \pi$, alors $\Sp_\R(A) =\varnothing$). 
\end{exemple}

{\bf Cas des matrices triangulaires :}

Soit

[[ICI TABLEAU]]

%\[T:= \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%t_{1,1}\ar@{.}[rrr]\ar@{.}[rrrddd]&& &t_{1,n}\ar@{.}[ddd]\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&\\
%&&&\\
%0\ar@{-}[rr]&&0&t_{n,n}}
%}\right) \]


une matrice triangulaire supérieure. Alors :

[[ICI TABLEAU]]
%
%\[\chi_T(X) = \left|\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%X-t_{1,1}\ar@{.}[rrr]\ar@{.}[rrrddd]&& &-t_{1,n}\ar@{.}[ddd]\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&\\
%&&&\\
%0\ar@{-}[rr]&&0&X-t_{n,n}
%}
%}\right| \]
%\[=\prod_{i=1}^n(X-t_{i,i})\]


donc $\Sp_\Kk(T) = \left\{t_{i,i} \mid 1\le i \le n\right\}$.

\subsection*{ Matrices compagnons :}

Soit 

[[ICI TABLEAU]]

%$P(X) : = X^n+c_{n-1}X^{n-1}+...+c_0 \in \Kk[X]$. On pose :
%\[C_P:=\left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%0\ar@{-}[rrr]\ar@{-}[rrrddd]&& &0\ar@{-}[ddd]&-c_{0}\ar@{.}[dddd]\\
%1\ar@{-}[rrrddd]&&&&\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&&\\
%&&&0&\\
%0\ar@{-}[rr]&&0&1&-c_{n-1}
%}
%}\right)  \in \mathcal{M}_n(\Kk) \]

c'est la {\it matrice compagnon} \index{matrice compagnon} du polynôme $P$.
\begin{proposition}
\[\chi_{C_P}(X) = P(X) .\]
\end{proposition}

\begin{proof}
Par récurrence sur $n \ge 1$. Si $n=1$ c'est évident car $P(X)= X +c_0$ et $C_P=(-c_0)$ (matrice $1 \times 1$).

Si 

[[ICI TABLEAU]]
%
%$P(X) = X^n +c_{n-1}X^{n-1}+...+c_0$, on a :
%\[\chi_{C_P}(X) = \left|\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%X\ar@{-}[rrrddd]&0\ar@{-}[rr]\ar@{-}[rrdd]& &0\ar@{-}[dd]&c_{0}\ar@{.}[dddd]\\
%-1\ar@{-}[rrrddd]&&&&\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&0&\\
%&&&X&\\
%0\ar@{-}[rr]&&0&-1&X+c_{n-1}
%}
%}\right|\]

(en développant par rapport à la première ligne)

[[ICI TABLEAU]]

%\[= X  \underbrace{
%\left|\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%X\ar@{-}[rrrddd]&0\ar@{-}[rr]\ar@{-}[rrdd]& &0\ar@{-}[dd]&c_{1}\ar@{.}[dddd]\\
%-1\ar@{-}[rrrddd]&&&&\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&0&\\
%&&&X&\\
%0\ar@{-}[rr]&&0&-1&X+c_{n-1}
%}
%}\right|}_{=X^{n-1}+c_{n-1}X^{n-2}+...+c_1 \atop \mathrm{ \, par\, hypoth\breve{e}se\,de\, r\acute{e}currence}} + (-1)^{n+1} c_0  \underbrace{\left|\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%-1\ar@{-}[rrrrdddd]&X\ar@{-}[rrrddd]&0\ar@{-}[rrdd]\ar@{-}[rr]&&0\ar@{-}[dd]\\
%0\ar@{-}[rrrddd]\ar@{-}[ddd]&&&&\\
%&&&&0\\
%&&&&X\\
%0\ar@{-}[rrr]&&&0&-1
%}
%}\right|}_{=(-1)^{n-1}}\]
%\[= X^n + c_{n-1}X^{n-1} +...+c_0\]
%\[= P(X)\]


(ce qui achève la récurrence).

\end{proof}

\begin{exemple}
Soit $J$ la matrice :

[[ICI TABLEAU]]

%\[\left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%0\ar@{-}[rrrrdddd]\ar@{-}[rrr]&&&0&1\\
%1\ar@{-}[rrrddd]&&&&0\ar@{-}[ddd]\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&&\\
%&&&&\\
%0\ar@{-}[rr]&&0&1&0
%}
%}\right)\in \mathcal{M}_n(\Kk)\]


alors $\chi_J(X) = X^n-1$ car $J=C_{X^n-1}$.
\end{exemple}
\vskip 1cm

\begin{exercicecours}[Polynômes de Tchébychev]
On rappelle le résultat suivant :

Pour tout $k$ entier $\ge 1$, il existe un polynôme en $t$, à coefficients rationnels, noté $T_k(t)$, tel que :
\[\Qq \theta \in \R ,\, \sin (k\theta) = \sin \theta \:  T_k(\cos \theta)\]
(en effet : \[\sin (k\theta) = \Im (e^{ik\theta})\]
\[= \Im \left( (\cos \theta + i \sin \theta )^k \right)\]
et on développe ...)

Par exemple, $\sin (2 \theta) = \sin \theta (2 \cos \theta)$ et $\sin (3 \theta) = \sin \theta (4 \cos^2 \theta -1)$ donc $T_2(t) =2t$ et $T_3(t) = 4t^2-1$. Plus généralement, $T_k(t) = 2^{k-1}t^{k-1} + ...$

Pour tout $n$ soit :


[[ICI TABLEAU]]
%
%\[V_n:= \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%0\ar@{-}[rrrrdddd]&1\ar@{-}[rrrddd]&0\ar@{-}[rrdd]\ar@{-}[rr] &&0\ar@{-}[dd]\\
%1\ar@{-}[rrrddd]&&&&\\
%0\ar@{-}[rrdd]\ar@{-}[dd]&&&&0\\
%&&&&1\\
%0\ar@{-}[rr]&&0&1&0
%}
%}\right) \in \mathcal{M}_n(\R)\]

alors, pour tout $n \ge 1$ :
\[\chi_{V_n}(X) = T_{n+1}(\frac{X}{2})\]
en particulier, \[\Sp_{\R}(V_n) = \left\{2\cos\biggm(\frac{k\pi}{n+1}\biggm) \mid 1 \le k \le n\right\} .\]

{\it Indications : vérifier que $\chi_{V_n}(X) = T_{n+1}(\frac{X}{2})$ pour $n=1,2$ et trouver une relation de récurrence d'ordre $2$ pour  $\chi_{V_n}(X)$ (en développant par rapport à une ligne ou une colonne) et une autre pour $T_{n}(X)$}.
\end{exercicecours}

Rappelons le 
\begin{theoreme}[fondamental de l'algèbre (ou théorème de d'Alembert)]
Tout polynôme complexe non constant admet une racine dans $\Cc$.
\end{theoreme}

{\it admettons ...}

\begin{corollaire}
Toute matrice $A \in \mathcal{M}_n(\Cc)$, $n \ge 1$, tout endomorphisme $u$ d'un espace vectoriel {\bf complexe} de {\bf dimension finie} admet au moins une valeur propre.
\end{corollaire}

\begin{proof}
Le polynôme caractéristique de $A$ (ou de $u$) est un polynôme complexe non constant donc admet (au moins) une racine $\lambda\in \Cc$. Alors, $\lambda$ est une valeur propre de $A$ (ou de $u$).
\end{proof}

\begin{corollaire}
Soit $A$ une matrice réelle. Alors, $A$ possède un sous-espace invariant de dimension $1$ ou $2$.
\end{corollaire}

\begin{proof}
Soit $n \ge 1$. Comme $A \in \mathcal{M}_n(\Rr) \subset \mathcal{M}_n(\Cc)$, $A$ possède une valeur propre $\lambda = a+ib \in \Cc$, $a,b$ réels, et un  vecteur propre associé $Z=X+iY \in\Cc^n \setminus\{0\}$ où $X,Y \in \Rr^n$. 

Alors :

\[A Z  = \lambda Z \iff AX + i AY = (aX -bY) + i (bX+aY)\]
\[\iff AX = aX-bY \text{ et } AY = bX +a Y\]
et en particulier le sous-espace (réel) $\langle X,Y \rangle$ est stable par $A$. Or $X$ ou $Y \neq 0$ donc $\langle X, Y \rangle$ est de dimension $1$ ou $2$. 
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Espaces propres}

\begin{definition}
Soit $u$ un endomorphisme de $E$. Soit $\lambda \in \Kk$. L'espace propre de $u$ \index{espace propre}  associé à $\lambda$ est le sous-espace $\Ker(u-\lambda \id_E)$ de $E$. On le note :
\[E_\lambda(u) := \Ker(u-\lambda \id_E)\]
\[= \{x \in E \mid u(x) = \lambda x\}.\]

Soit $A \in \mathcal{M}_n(\Kk)$. L'espace propre de $A$ associé à la valeur propre $\lambda$ est le sous-espace vectoriel de $\mathcal{M}_{n,1}(K)$ (l'espace des vecteurs colonnes à $n$ coefficients) défini par :
\[E_\lambda(A) := \{X \in {\cal M}_{1,n}(\Kk) \mid A X = \lambda X\} .\]
\end{definition}

\begin{remarque*}[s]

--- Si $\lambda$ est une valeur propre de $u$ (ou de $A$), l'espace propre associé $E_\lambda(u)$ (ou $E_\lambda(A)$ est de dimension $\ge 1$ ;

--- L'espace propre $E_\lambda(u)$ est laissé stable par  $u$. En effet :
\[x  \in \Ker (u-\lambda \id_E) \implies u(u(x)) = u(\lambda x) = \lambda u(x)\]
\[\implies u(x) \in \Ker (u-\lambda \id_E) .\]
\end{remarque*}

\begin{theoreme}
Soit $u$ un endomorphisme de $E$. Soient $\lambda_1,...,\lambda_r$ $r$ valeurs propres {\bf distinctes} de $u$. Alors les espaces propres associés $E_{\lambda_1},...,E_{\lambda_r}$ sont en somme directe c'est-à-dire :

\[E_{\lambda_1} + .... + E_{\lambda_r} = E_{\lambda_1} \oplus .... \oplus E_{\lambda_r} .\] 
\end{theoreme}

\begin{remarque*}
On en déduit que le nombre de valeurs propres est $\le \dim E$ car :
\[\dim E \ge \dim E_{\lambda_1} \oplus .... \oplus E_{\lambda_r} = \dim E_{\lambda_1} + ... +  \dim E_{\lambda_r} \ge r .\] 
\end{remarque*}

\subsection*{Rappels sur les sommes directes}

\begin{definition}
On dit que $E_1,...,E_r$ des sous-espaces de $E$ sont en somme directe \index{somme directe} si :
\[\Qq v_1 \in E_1,...,\Qq v_r \in E_r,\, v_1+...+v_r = 0 \implies v_1=...=v_r = 0\]
notation :
\[E_1+...+E_r=E_1\oplus...\oplus E_r .\]
\end{definition}

\begin{remarque*}
Si $r = 2$, $E_1$ et $E_2$ sont en somme directe si et seulement si $E_1 \cap E_2 =\{0\}$.
\end{remarque*}

\begin{exercicecours}
$E_1,..., E_r$ sont en somme directe $\iff$ :
\[\Qq 1 \le i \le r,\, E_i \cap (\sum_{j=1\atop j\neq i}^r E_j)=\{0\}.\]
\end{exercicecours}

\begin{exemple}
Si $e_1,...,e_n$ est une famille libre de $E$ (par exemple une base), alors les droites $\Kk e_1,...,\Kk e_n$ sont en somme directe. 
\end{exemple}

Il est facile de calculer la dimension d'une somme directe :

\begin{proposition}
Si $E_1,...,E_r$ sont des sous-espaces de $E$ en somme directe, alors :
\[\dim (E_1 + ...+E_r) = \dim E_1 +...+\dim E_r .\]
\end{proposition}

\begin{proof}
Soient $\mathcal{B}_1,...,\mathcal{B}_r$ des bases respectives de $E_1,...,E_r$. Alors les $\mathcal{B}_i$ sont deux à deux disjointes et $\mathcal{B}_1 \cup ... \cup \mathcal{B}_r$ est une base de $E_1+...+E_r$ donc :
\[\dim E_1 \oplus ... \oplus E_r = \biggm | \mathcal{B}_1 \cup ... \cup \mathcal{B}_r\biggm |  \]
\[= \biggm |\mathcal{B}_1 \biggm | + ... + \biggm |\mathcal{B}_r \biggm | \]
\[\dim E_1 +...+\dim E_r .\]
\end{proof}
\begin{center}
*
\end{center}
\begin{proof}[du théorème]
Par récurrence sur $r \ge 1$. Si $r=1$, il n'y a rien à démontrer.

Soient $v_1 \in E_{\lambda_1},...,v_r \in E_{\lambda_r}$ tels que 
\begin{equation}\label{eq:vp1}
v_1+...+v_r=0
\end{equation}
alors si on applique $u$, on trouve :
\begin{equation}
u(v_1)+...+u(v_r)=0
\end{equation}
\begin{equation}\label{eq:vp2}
\iff \lambda_1 v_1 +...+\lambda_r v_r =0
\end{equation}

mais alors (\ref{eq:vp1}) - $\lambda_1$ x (\ref{eq:vp2}) donne :
\[(\lambda_2-\lambda_1)v_2+...+(\lambda_r-\lambda_1)v_r = 0 \]
$\implies$(hypothèse de récurrence de rang $r-1$)$\implies$ :
\[(\lambda_2-\lambda_1)v_2 = ... = (\lambda_r-\lambda_1)v_r = 0\]
\[\implies v_2=...=v_r=0\]
car $\lambda_2-\lambda_1,...,\lambda_r-\lambda_1\neq 0$.

On a donc aussi : $v_1 = -v_2-...-v_r =0$.
\end{proof}

\begin{corollaire}
Soit $ u$ un endomorphisme de $E$. Si $E$ est de dimension $n$ et si le polynôme carctéristique $\chi_u(X)\in\Kk[X]$ admet $n$ racines distinctes dans $\Kk$, alors il existe une base de $E$ formée de vecteurs propres de $u$.
\end{corollaire}

\begin{proof}
Soient $\lambda_1,...,\lambda_n \in \Kk$ les $n-$racines distinctes de $\chi_u(X)$. Ce sont aussi $n$ valeurs propres de $u$. Notons $E_{\lambda_1}$,...,$E_{\lambda_n}$ les espaces propres associés. Alors : $E_{\lambda_1}+...+E_{\lambda_n} \subset E$ et :
\[\dim (E_{\lambda_1}+...+E_{\lambda_n}) = \dim (E_{\lambda_1}\oplus ... \oplus E_{\lambda_n})\]
\[= \dim E_{\lambda_1} +...+ \dim E_{\lambda_n} \ge n=\dim E\]
donc :

\[\Qq 1\le i \le n,\, \dim E_{\lambda_i} =1 \text{ et } E_{\lambda_1}\oplus ... \oplus E_{\lambda_n} = E.\]

Pour tout $i$, soit $e_i$ un vecteur non nul tel que :
\[E_{\lambda_i} = \Kk e_i\]
alors les vecteurs $e_i$ sont des vecteurs propres de $u$ (de valeurs propres $\lambda_i$) et \[\Kk e_1+...+\Kk e_n = \Kk e_1\oplus...\oplus\Kk e_n = E\]
signifie que les $e_i$ forment une base de $\Kk^n$.
\end{proof}

\begin{remarque*}
Bien entendu la réciproque est fausse car par exemple si $n \ge 2$, toute base de $E$ est formée de vecteurs propres de $\id_E$ et le polynôme caractéristiquede $\id_E$ est  $\chi_{\id_E}(X)=(X-1)^n$ qui n'a qu'une seule racine : $ 1$. 
\end{remarque*}

\begin{definition}[diagonalisable]
On dit qu'un endomorphisme $u$ de $E$ est diagonalisable \index{diagonalisable} s'il existe une base de $E$) formée de vecteurs propres de $u$.
\end{definition}
\begin{remarque*}
Si $u$ est diagonalisable et si on note $\lambda_1,...,\lambda_r$ ses valeurs propres distinctes, alors :
\[ \Ker (u -\lambda_1\id_E) \oplus .... \oplus \Ker (u-\lambda_r\id_E) = E\]
(car tout vecteur de $E$ est combinaison linéaire de vecteurs propres de $u$ donc est une somme de vecteurs appartenant aux espaces propres $\Ker (u-\lambda_i)$)
et réciproquement, si :
\[\Ker (u -\lambda_1\id_E) \oplus .... \oplus \Ker (u-\lambda_r\id_E) =E\]
alors, il existe une base de $E $ formée de vecteurs propres de $u$ (il suffit de mettre \og bout à bout \fg\ des bases des espaces propres $\Ker (u -\lambda_i\id_E)$).

En bref :
\[u \mbox{ est diagonalisable } \iff \Ker (u -\lambda_1\id_E) \oplus .... \oplus \Ker (u-\lambda_r\id_E) =E .\] 
\end{remarque*}

\begin{definition}
Si $A \in \mathcal{M}_n(\Kk)$, on dit que $A$ est diagonalisable sur $\Kk$ s'il existe une matrice inversible $P \in \GL_n(\Kk)$ et une matrice diagonale $D \in \mathcal{M}_n(\Kk)$ telle que : $A = PDP^{-1}$.
\end{definition}

\begin{remarque*}


Si 

[[ICI TABLEAU]]

%\[D=  \left(\raisebox{0.5\depth}{%
%\xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}% 
%\xymatrix{
%\lambda_1\ar@{.}[rrdd]& &\\
%&&\\
% &&\lambda_n
%}
%}\right)\;,\]


alors les $\chi_A(X) = \chi_D(X) = (X-\lambda_1)...(X-\lambda_n)$ et donc les $\lambda_i$ sont les valeurs propres de $A$ (et les racines de $\chi_A(X)$).

\end{remarque*}
{\it Diagonaliser}\index{ diagonaliser} une matrice $A \in \mathcal{M}_n(\Kk)$ signifie trouver, si elles existent, $P \in \GL_n(\Kk)$, $D \in\mathcal{M}_n(\Kk)$ diagonale telles que :
\[A = PDP^{-1} .\]


\begin{exemple}

--- Toute matrice diagonale est diagonalisable.

--- Toute matrice réelle $2 \times 2$ symétrique :
\[\left(\begin{array}{cc}
a & b \\
b & d
\end{array}\right)\]
est diagonalisable \exoo

--- {\bf projections :} \index{projection} On suppose que $E = F \oplus G$. On définit la projection sur $F$ suivant $G$ par :
\[p : E \to E \,,\, \underbrace{x}_{\in F} \oplus \underbrace{y}_{\in G} \mapsto x .\]
Il est facile de voir que :
\[F = \Ker (p -\id_E) = E_1(p) \text{ et } G = \Ker p = E_0(p)\]
donc $p$ est diagonalisable. Remarquer aussi que $p^2=p$. En fait, réciproquement, si $p$ est un endomorphisme de $E$ tel que $p^2=p$ alors $p$ est une projection sur un certain sous-espace suivant un autre certain sous-espace.

--- {\bf réflexions :} \index{réflexion} On suppose encore que $E =F \oplus G$. On définit la réflexion par rapport à $F$ suivant $G$ par :
\[r : E \to E \,,\, \underbrace{x}_{\in F} \oplus \underbrace{y}_{\in G} \mapsto x -y\]
c'est un endomorphisme de $E$ tel que : $r^2 =\id_E$. Il est facile de voir :

\[F =\Ker (r-\id_E) =E_1(r) \text{ et } G = \Ker(r+\id_E) =E_{-1}(r) .\]

Vérifier que si $r$ est un endomorphisme de $E$ vérifiant $r^2 =\id_E$ ($\Kk =\Qq,\Rr $ ou $\Cc$), alors $r$ est une réflexion par rapport à un certain sous-espace et suivant un certain autre sous-espace. 

--- La matrice de permutation circulaire 

[[ICI TABLEAU]]

%\[J =
%\left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%0\ar@{-}[rrrrdddd]\ar@{-}[rrr]&&&0&1\\
%1\ar@{-}[rrrddd]&&&&0\ar@{-}[ddd]\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&&\\
%&&&&\\
%0\ar@{-}[rr]&&0&1&0
%}
%}\right)\in \mathcal{M}_n(\Kk)\] 


est diagonalisable sur $\Cc$ de valeurs propres \[1,e^{i\frac{2\pi}{n}},...,e^{i\frac{2(n-1)\pi}{n}}\]
les racines $n-$ièmes de l'unité. Trouver une base de vecteurs propres \exoo.
\end{exemple}

\begin{proposition}
Soit $E$ un $\Kk-$espace vectoriel de base $e_1,...,e_n$. Soit $u$ un endomorphisme de $E$ et soit $A$ la matrice de $u$ dans la base $e_1,...,e_n$. Alors $u$ est diagonalisable sur $\Kk$ si et seulement si $A$ est diagonalisable sur $\Kk$. 
\end{proposition}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Un premier critère de diagonalisabilité}

\subsection*{Rappels sur les polynômes}

\begin{definition}
Un polynôme à coefficients dans $\Kk$ est une suite 
\[a_0,a_1,a_2,...\]
dont tous les termes sont nuls à partir d'un certain rang et qui est notée :
\[a_0 + a_1X + a_2X^2 + ...\]
Le degré du polynôme
\[P(X) = a_0 + a_1 X + ...\]
est le plus grand entier $n$, noté $\deg P$, tel que $a_n \neq 0$. On prend pour convention $\deg   0 = -\infty$.

Si $P(X) = a_0 + a_1 X + ...$ et $Q(X) = b_0 + b_1 X + ...$ sont des polynômes, on définit leur produit :
\[P(X) Q(X) := a_0b_0 + (a_0 b_1 + a_1b_0) X + ... + (a_0b_k + a_1 b_{k-1} + ... + a_k b_0)X^k + ...\]

On note $\Kk[X]$ la $\Kk-$algèbre des polynômes à coefficients dans $\Kk$.
\end{definition}

\begin{remarque*}[importante]
On peut attribuer une valeur à $X$ : 
si $P(X) = a_0 +...+ a_d X^d$, si $\lambda \in \Kk$, on définit :
\[P(\lambda):= a_0 + ... +a_d\lambda^d \in \Kk .\]

\end{remarque*}

\begin{remarque*}
Pour tous polynômes $P(X)$, $Q(X)$ à coefficients dans $\Kk$, $\deg (PQ) = \deg P + \deg Q$.
\end{remarque*}

\begin{proposition}[intégrité]

Soient $P(X),Q(X) \in \Kk[X]$. Si $P(X) Q(X) = 0$, alors $P(X)=0$ ou $Q(X) =0$.
\end{proposition}

\begin{proof}
Si $P(X) \neq 0$ et $Q(X) \neq 0$, alors, $\deg (PQ) = \deg P +\deg Q \ge 0$ donc $P(X)Q(X) \neq 0$.
\end{proof}
\subsubsection*{Divisibilité, racines, multiplicité}

\begin{definition}
Si $P(X),Q(X) \in \Kk[X]$, on dit que $Q$ divise $P$ (dans $\Kk[X]$) si \[P(X) = B(X)Q(X)\]
pour un certain polynôme $B(X) \in \Kk[X]$. Notation : \[Q | P\]
remarque : \[Q | P \implies \deg Q \le \deg P .\] 
\end{definition}
{\bf Formule de Taylor :}

Pour tout $P(X) \in \Kk[X]$, pour tout $\lambda \in \Kk$, il existe (une unique) suite $a_0,...,a_d$ telle que :
\[P(X) = a_0 + a_1 (X-\lambda) + ... + a_d (X-\lambda)^d\]

Bien entendu, $a_0 = P(\lambda)$. On dit que $\lambda$ est une racine de $P$ si $P(\lambda) = 0$ c'est-à-dire si $(X-\lambda) | P(X)$.

Si $P(X) \neq 0$, on appelle {\it multiplicité} de $\lambda$ dans $P$ le plus petit entier $i$ tel que $a_i \neq 0$.

Autrement dit la multiplicité de $\lambda $ dans $P$ est le plus grand entier $i$ tel que $(X-\lambda)^i | P(X)$.

{\bf Notation :} $\mult_\lambda P := $ la multiplicité de $\lambda$ dans $P$\index{$\mult_\lambda P$, multiplicité}. 

\begin{remarque*}
Soient $P \in \Kk[X]$, $\lambda \in \Kk$. On a l'équivalence :
\[\lambda \mbox{ racine de $P$ } \iff \mult_\lambda P \ge 1 .\]
\end{remarque*}
\begin{exercicecours}
{\bf Multiplicité du produit :} 
\[\Qq P,Q \in \Kk[X],\, \Qq \lambda \in \Kk,\, \mult_\lambda(PQ) = \mult_\lambda P + \mult_\lambda Q\]
\end{exercicecours}

\begin{remarque*}[multiplicité]
Si $\lambda_1,...,\lambda_r$ sont deux à deux distincts et si :
\[P(X) :=(X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r}\]
alors $m_i$ est la multiplicité de $\lambda_i$ dans $P(X)$, pour tout $i$. En effet, par exemple pour $i=1$,
d'après l'exercice précédent, 
\[\mult_{\lambda_1} P = m_1\underbrace{\mult_{\lambda_1}(X-\lambda_1)}_{=1} + ... + m_r\underbrace{ \mult_{\lambda_1}(X-\lambda_r)}_{= 0}\] 
\[=m_1 .\]
\end{remarque*}

\begin{exercicecours}

Si $\Kk = \Cc$ montrer que :
\[\sum_{\lambda \in \Cc}\mult_\lambda P = \deg P\]
pour tout polynôme non nul $P$.
\end{exercicecours}

\begin{definition}[scindé]
Un polynôme $P(X)$ est scindé \index{scindé} sur $\Kk$ si :
\[P(X) = a_d(X-\lambda_1)...(X-\lambda_d)\]
pour certains $\lambda_i \in \Kk$ et un $a_d \in \Kk$. Souvent, on regroupe les racines égales et on écrit :
\[P(X) = a_d(X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r}\]
avec les $\lambda_i$ deux à deux distinctes et des entiers $m_i  \ge 1$.
\end{definition}

\begin{exemple}
D'après le théorème de d'Alembert, tous les polynômes sont scindés sur $\Cc$. 
\end{exemple}
\begin{center}
*
\end{center}

Pour énoncer un premier critère de diagonalisation des endomorphismes on aura besoin du lemme suivant :

\begin{lemme}\label{lem:restr}
Soit $u$ un endomorphisme de $E$. On suppose $E$ de dimension finie et on suppose aussi qu'il existe un sous-espace $F$ de $E$ laissé stable  par $u$. Notons $\chi_{u\Res{F}}$ le polynôme caractéristique de la restriction à $F$. Alors :
\[\chi_{u\Res{F}}(X) |\chi_u(X)\]
dans $\Kk[X]$.
\end{lemme}

\begin{proof}
Soit $e_1,...,e_k$ une base de $F$ que l'on complète en une base de $E$ :
\[e_1,...,e_n\]
alors la matrice de $u$ dans la base $e_1,...,e_n$ est de la forme :

[[ICI TABLEAU]]

%\[\left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%\ar@{<->}_{k}[dd]\ar@{<->}^{k}[rr]&&\ar@{<->}^{n-k}[rr]\ar@{-}[dddd]&&\\
%&A && B &\\
%\ar@{<->}_{n-k}[dd]\ar@{-}[rrrr]&&&&\\
%&0 & &D&\\
%&&&&
%}}\right)
%\]


(où $A$ est la matrice de $u\Res{F}$ dans la base $e_1,..,e_k$). Mais alors :
\[\chi_u(X) = \left|\begin{array}{c|c}
XI_k-A & -B\\\hline
0 & XI_{n-k} - D
\end{array}\right| = \det (XI_k-A) \det(XI_{n-k}-D)\]
\[= \chi_{u\Res{F}} \det(XI_{n-k}-D) .\] 
\end{proof}

\begin{definition}[multiplicités algébrique et géométrique]
Soit $A\in \mathcal{M}_n(\Kk)$. Soit $\lambda \in \Kk$. On notera $m_a(\lambda)$\index{$m_a(\lambda)$} la multiplicité de $\lambda$ dans le polynôme caractéristique de $A$, $\chi_A(X)$ : \[m_a(\lambda) := \mbox{  le plus grand entier $m$ tel que $(X-\lambda)^m | \chi_A(X)$}\]c'est la multiplicité algébrique \index{multiplicité algébrique} de $\lambda$. On notera  :
\[ m_g(\lambda) := \dim_\Kk \Ker(A-\lambda I_n)\] \index{$m_g(\lambda)$} c'est la multiplicité géométrique \index{multiplicité géométrique} de $\lambda$.
\end{definition}

\begin{corollaire}
Soit $A \in \mathcal{M}_n(\Kk)$ ou soit $u$ un endomorphisme de $E$, si $E$ est de dimension finie. Pour tout $\lambda \in \Kk$,
\[m_g(\lambda) \le m_a(\lambda).\] 
\end{corollaire}

\begin{exemple}
Si 

[[ICI TABLEAU]]

%\[A = \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%1\ar@{-}[rrrrdddd]& 1\ar@{-}[rrrddd]& 0\ar@{-}[rr]\ar@{-}[rrdd]&&0\ar@{-}[dd]\\
%0\ar@{-}[ddd]\ar@{-}[rrrddd]&&&&\\
%&&&&0\\
%&& &&1\\
%0 \ar@{-}[rrr]&& &0&1}}\right) \in \mathcal{M}_n(\Kk) \]


alors $\chi_A(X) = (X-1)^n$ et $m_g(1)=1 < m_a(1)=n$. 

Si $A= I_n$, alors :
$\chi_A(X) = (X-1)^n$ et $m_g(1)=m_a(1)=n$.

\end{exemple}

\begin{proof}
Soit $\lambda$ une valeur propre de $u$. Posons $F:=\Ker (u-\lambda)$ l'espace propre associé.

Alors $F$ est stable par $u$ :

en effet, si $x \in F$, alors :
\[u(u(x)) = u(\lambda x) = \lambda u(x)\]
donc $u(x) \in F$.

Donc d'après le lemme \ref{lem:restr},
\[\chi_{u\Res{F}}(X) \biggm | \chi_u(X)\]
or :
\[\Qq x \in F\,, u(x) = \lambda x\]
donc $u\Res{F} = \lambda \id_F$ et \[\chi_{u\Res{F}}(X) = (X-\lambda)^{\dim F}\]
\[= (X-\lambda)^{m_g(\lambda)}\]
et finalement,
\[(X-\lambda)^{m_g(\lambda)} \biggm | \chi_u(X) \implies m_g(\lambda) \le m_a(\lambda) .\] 
\end{proof}

Voici un premier critère de diagonalisabilité :
\begin{theoreme}
Soit $A \in \mathcal{M}_n(K)$ (resp.\ $u$ un endomorphisme de $E$ avec $E$ de dimension finie). Alors :
\[A \mbox{ (resp.\ $u$) est diagonalisable sur $\Kk$ } \iff \left\{\begin{array}{l}
i)\; \chi_A(X) \mbox{ est scindé sur $\Kk$ ;}\\
\text{ et }\\
ii)\Qq \lambda,\mbox{ valeur propre de $A$, } m_a(\lambda) = m_g(\lambda).
\end{array}\right.\]
\end{theoreme}

\begin{proof}
$\mathbf \implies :$ Supposons $u$  diagonalisable. Soient $\lambda_1,..., \lambda_r$ les valeurs propres distinctes de $u$. Comme :
\[\Ker(u-\lambda_1\id_E) \oplus ... \oplus \Ker(u-\lambda_r\id_E) =E\,,\]
si on choisit des bases $\mathcal{B}_1$ de $\Ker(u-\lambda_1\id_E) $, ..., $\mathcal{B}_r$ de $\Ker(u-\lambda_r\id_E)$, on obtient une base $\mathcal{B}_1\cup ...\cup \mathcal{B}_r$ de $E$ dans laquelle la matrice de $u$ est :
\[\Mat(u) = \left(\begin{array}{c|c|c}
\lambda_1I_{n_1}& &\\\hline
&\ddots&\\\hline
&&\lambda_r I_{n_r}
\end{array}\right)\] où $n_i =m_g(\lambda_i)=\dim \Ker(u-\lambda_i\id_E)$ pour tout $i$. Donc :
\[\chi_u(X) = (X-\lambda_1)^{n_1}...(X-\lambda_r)^{n_r} .\]
Par conséquent le polynôme $\chi_u(X) $ est scindé sur $\Kk$ et :
\[\Qq i, \, m_a(\lambda_i) = n_i = m_g(\lambda_i) .\]

$\mathbf \Leftarrow$ :  Supposons que :
\[\chi_u(X) = (X-\lambda_1)^{n_1}...(X-\lambda_r)^{n_r}\]
pour certains $\lambda_i \in \Kk$, deux à deux distincts et certains entiers $n_i \ge 1$. 
Comme :
\[\Ker(u-\lambda_1\id_E) + ... + \Ker(u-\lambda_r\id_E) = \Ker(u-\lambda_1\id_E) \oplus ... \oplus \Ker(u-\lambda_r\id_E) \subset E\,,\]
on a :
\[m_g(\lambda_1) + ... + m_g(\lambda_r) = \dim \left(\Ker(u-\lambda_1\id_E) + ... + \Ker(u-\lambda_r\id_E)\right)\]
\[\le \dim E .\]

Or, pour tout $i$, $m_g(\lambda_i) = m_a(\lambda_i) = n_i$ et \[n_1 + ... + n_r =\deg \chi_u = \dim E\]
en conséquence : 
\[
 \dim \left(\Ker(u-\lambda_1\id_E) + ... + \Ker(u-\lambda_r\id_E)\right) = \dim E \]
et forcément,  \[\Ker(u-\lambda_1\id_E) + ... + \Ker(u-\lambda_r\id_E) = E .\]

\end{proof}

\begin{exemple}
Si $n \ge 2$, la matrice :

[[ICI TABLEAU]]

%\[
% \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%0\ar@{-}[dddd]\ar@{-}[rrrrdddd]& 1\ar@{-}[rrrddd]& 0 \ar@{-}[rr]\ar@{-}[rrdd]&&0\ar@{-}[dd]\\
%&&&&\\
%&&&&0\\
%&& &&1\\
%0 \ar@{-}[rrrr]&& &&0}}\right)
%\]

n'est jamais diagonalisable car $m_g(0) = 1 < m_a(0) =n$.
\end{exemple}

\subsection*{Méthode pour diagonaliser}
Soit $A$ une matrice carrée complexe $n \times n$. Pour la diagonaliser :

--- on calcule d'abord son polynôme caractéristique $\chi_A(X)$ ;

--- on cherche les racines de $\chi_A(X)$ : ce sont les valeurs propres de $A$ ;

--- pour toute valeur propre $\lambda$ de $A$, on cherche une base de $\Ker A-\lambda I_n$ c'est-à-dire on cherche une base de l'espace des solutions du système :
\[AX = \lambda X\]

--- si pour toute valeur propre $\lambda$ de $A$, $\dim \Ker (A -\lambda I_n) = m_a(\lambda)$, $A$ est diagonalisable et une réunion des bases des espaces propres forme une base de vecteurs propres.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Trigonalisation}
\begin{definition}
On dit qu'une matrice $A \in \mathcal{M}_n(K)$ (resp.\ un endomorphisme $u$ de $E$, si $E$ est de dimension finie) est {\it trigonalisable}\index{trigonalisable} sur $\Kk$ (on devrait dire triangularisable mais ce terme signifie déjà autre chose) si $A$ est semblable à une matrice triangulaire supérieure c'est-à-dire :
\[\exists P \in \GL_n(\Kk),\, T \in \mathcal{M}_n(\Kk),\, \Qq n \ge i > j \ge 1,\, T_{i,j}= 0 \text{ et } A = PTP^{-1}\]
(resp.\ il existe une base de $E$ où la matrice de $u$ est triangulaire supérieure). 
\end{definition}

\begin{exercicecours}
Toute matrice triangulaire inférieure est trigonalisable ({\it si $T$ est triangulaire inférieure, $w_0Tw_0^{-1}$ est triangulaire supérieure où : 

[[ICI TABLEAU]]

%\[w_0 := \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%0\ar@{-}[dd]\ar@{-}[rr]& &0\ar@{-}[lldd]& 1\ar@{-}[lllddd]\\
%&&&0\ar@{-}[dd]\ar@{-}[lldd]\\
%0&& &\\
%1&0\ar@{-}[rr] &&0}}\right).\] 

}
\end{exercicecours}
\begin{theoreme}
Soit $A \in \mathcal{M}_n(\Kk)$. Alors $A$ est trigonalisable $\iff$ $\chi_A(X)$ est scindé sur $\Kk$.
\end{theoreme}

\begin{proof}
{$\mathbf \implies :$} Deux matrices semblables ont le même polynôme caractéristique donc il suffit de montrer que $\chi_T(X)$ est scindé pour toute matrice triangulaire supérieure $T$ ; ce qui est facile.

{$\mathbf \Leftarrow :$} On raisonne aves $u$ un endomorphisme de $E$ (et on suppose $E$ de dimension finie). Par récurrence sur $\dim E$. Si $\dim E = 1$, il n'y a rien à démontrer. Si $\dim E > 1$, alors comme $\chi_u(X)$ est scindé, 
\[\chi_u(X) = (X-\lambda_1)...(X-\lambda_n)\]
où $n = \dim E$ et où les $\lambda_i \in \Kk$ ne sont pas forcément distincts. Soit $e_1$ un vecteur propre associé à la valeur propre $\lambda_1$. On complète $e_1$ en une base :
$e_1,...,e_n$. Dans cette base, la matrice de $u$ est de la forme :
\[\left(\begin{array}{c|c}
\lambda_1 & l\\\hline
0 & B
\end{array}\right) \]
où $B \in \mathcal{M}_{n-1}(\Kk)$ et $l \in \mathcal{M}_{1,n-1}(\Kk)$. En particulier, \[\chi_u(X) = (X-\lambda_1)\chi_B(X)\]
\[\implies \chi_B(X) = (X-\lambda_2)...(X-\lambda_n)\] est scindé sur $\Kk$. Par hypothèse de récurrence, il existe $S \in \mathcal{M}_{n-1}(\Kk)$ une matrice triangulaire supérieure et $Q \in \GL_{n-1}(\Kk)$ une matrice inversible telles que :
\[B = Q S Q^{-1}\]
alors, on peut vérifier que : 
\[\Mat(u) = \left(\begin{array}{c|c}
\lambda_1 & l\\\hline
0 & QSQ^{-1}
\end{array}\right) \]
\[= P \left(\begin{array}{c|c}
\lambda_1 & lQ\\\hline
0 & S
\end{array}\right) P^{-1}\]pour la matrice inversible :
\[P := \left(\begin{array}{c|c}
1 & 0\\\hline
0 & Q
\end{array}\right) .\]

Donc la matrice de $u$ dans la base $e_1,...,e_n$ est semblable à une matrice triangulaire supérieure :
\[\left(\begin{array}{c|c}
\lambda_1 & lQ\\\hline
0 & S
\end{array}\right) \]
donc $u$ est trigonalisable. 
\end{proof}

\begin{corollaire}
Sur $\Cc$ toutes les matrices sont trigonalisables.
\end{corollaire}

\subsection*{Relations entre les valeurs propres et les invariants}

Soit $A\in \mathcal{M}_n(\Cc)$.  Alors $A$ est semblable à une matrice triangulaire supérieure de la forme :

[[ICI TABLEAU]]

%\begin{equation}\label{eq:tri}
%\left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%\lambda_1\ar@{.}[rrrddd]\ar@{.}[rrr]& &&\ar@{.}[ddd]\\
%0\ar@{-}[rrdd]\ar@{-}[dd]&&&\\
%&&&\\
%0\ar@{-}[rr]&&0& \lambda_n}}\right)
%\end{equation}



donc : 
\[\chi_A(X) = (X-\lambda_1)...(X-\lambda_n)\]
 ainsi les coefficients diagonaux de (\ref{eq:tri}) ne dépendent que de $A$ :  

ce sont les valeurs propres de $A$ comptées avec leur multiplicité algébrique.

\begin{exercicecours}
Vérifier que 
\[\det A = \lambda_1...\lambda_n\]
\[\Qq k \ge 1, \, \tr A^k = \lambda_1^k + ... +\lambda_n^k .\]
\end{exercicecours}

On peut en déduire la belle formule suivante :

\[\exists \epsilon > 0,\, \Qq |t| < \epsilon\,,\]
la série $\displaystyle \sum_{k=1}^\infty \frac{\tr (A^k)}{k}t^k$ converge, la matrice $I_n - t A$ est inversible et :

\[e^{\left(\sum_{k=1}^\infty \frac{\tr A^k}{k}t^k\right)} = \frac{1}{\det(I_n -tA)}.\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Polynômes d'endomorphismes}

Soit $u$ un endomorphisme d'un espace vectoriel $E$ sur $\Kk$. Soit $A \in\mathcal{M}_n(\Kk)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Définition}
On remplace $X^k$ par $u^k$ (ou $A^k$) et $1$ par $\id_E$ (ou $I_n$).

Soit $P(X) = a_0 + a_1 X +a_2 X^2 + ... \in\Kk[X]$ un polynôme. On pose :
\[P(u) := a_0\id_E +a_1u + a_2 u^2 + ... \text{ et } P(A) := a_0 I_n +a_1 A +a_2 A^2 +...\] 

\begin{proposition}
L'application :
\[\Kk[X] \to \mathcal{M}_n(\Kk) \,,\, P(X) \mapsto P(A)\]

est un morphisme d'algèbres c'est-à-dire : c'est linéaire et :
\[\Qq P,Q\in \Kk[X]\,,\, (PQ)(A) = P(A)Q(A)\]

de même l'application :
\[\Kk[X] \to \End_\Kk(E) \,,\, P(X) \mapsto P(u)\]
est aussi un morphisme d'algèbres.
\end{proposition}

\begin{proof}
Si $P(X) = a_0 + a_1X +...$ et $Q(X) = b_0 + b_1X + ...$, alors $PQ(X) = a_0b_0 +(a_0 b_1 + a_1b_0) X + ...$. Donc :
\[(PQ)(A) = a_0b_0 I_n + (a_0b_1 + a_1b_0) A + ...\]
\[= (a_0 I_n + a_1 A +...)(b_0 I_n +b_1 A + ...) \]
\[= P(A) Q(A) .\]
\end{proof}

\begin{remarque*}[importante]
En particulier, pour tous $P,Q \in \Kk[X]$, les matrices $P(A)$ et $Q(A)$ commutent :
\[P(A) Q(A) = Q(A)P(A)\]
de même les endomorphismes $P(u)$ et $Q(u)$ commutent.
\end{remarque*}

\begin{exemple}
--- {\bf Polynôme d'une diagonale :}

[[ICI TABLEAU]]

%\[D:= \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%\lambda_1\ar@{.}[rrdd]& &\\
%&&\\
%&& \lambda_n}}\right)
%\]
on a :

[[ICI TABLEAU]]

%\[P(D) = \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%P(\lambda_1)\ar@{.}[rrdd]& &\\
%&&\\
%&& P(\lambda_n)}}\right)\]

pour tout polynôme $P(X) \in \Kk[X]$.

--- {\bf polynôme et conjugaison :} Si $Q$ est inversible, si $A =Q A'Q^{-1}$, alors pour tout polynôme $P(X) \in \Kk[X]$, $P(A) = Q P(A')Q^{-1}$.
\end{exemple}

\begin{exercicecours}
Montrer que plus généralement, pour une matrice triangulaire :

[[ICI TABLEAU]]

%\[T:=\left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%\lambda_1\ar@{.}[rrdd]\ar@{.}[rr]& &\ar@{.}[dd]\\
%&&\\
%&& \lambda_n}}\right)\]
on a :

[[ICI TABLEAU]]

%\[P(T) = \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%P(\lambda_1)\ar@{.}[rrdd]\ar@{.}[rr]& &\ar@{.}[dd]\\
%&&\\
%&& P(\lambda_n)}}\right)\]

pour tout polynôme $P(X)  \in \Kk[X]$ (les coefficients hors de la diagonale peuvent avoir une expression compliquée mais les coefficients diagonaux sont obtenus simplement en leur appliquant le polynôme $P$).
\end{exercicecours}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Th\'eorème de Cayley-Hamilton}

\begin{definition}
On dit qu'un polynôme $P(X)$ est un polynôme annulateur \index{polynôme annulateur} de la matrice $A$ ou de l'endomorphisme $u$ si $P(A) =0$, ou si $P(u) =0$. 
\end{definition}

\begin{exemple}
--- Si $p : E \to E$ est une projection, $X^2-X$ est un polynôme annulateur de $p$ car $p^2=p$.

--- Si $r : E \to E$ est une réflexion, $X^2-1$ est un polynôme annulateur de $r$ car $r^2 =\id_E$. 
\end{exemple}

Où chercher les valeurs propres, connaissant un polynôme annulateur mais ne connaissant pas le polynôme caractéristique ?
\begin{proposition}
Si $P$ est un polynôme annulateur de $u$, resp.\ de $A$, alors :
\[\Sp (u) \subset \{\mbox{ racines de $P$ }\}\]
resp.
\[\Sp (A) \subset \{\mbox{ racines de $P$ }\}.\]
\end{proposition}

\begin{proof}
Si $x$ est un vecteur propre de $u$ associé à une valeur propre $\lambda$, alors :

\[u(x) = \lambda x \implies \Qq k \ge 0\,, u^k(x) = \lambda^k x\]
 et plus généralement :
\[Q(u)(x) = Q(\lambda)x \]
pour tout polynôme $Q(X)$. En particulier : $P(u)(x)=0$ $\implies P(\lambda) x =0$ $\implies P(\lambda) = 0$ car $x \neq 0$.
\end{proof}


\begin{theoreme}[de Cayley-Hamilton]\index{Cayley-Hamilton}
Si $E$ est de dimension finie, \[\chi_u(u) = 0\]

de même $\chi_A(A)  =0$.
\end{theoreme}
\begin{exemple}
--- Si :

[[ICI TABLEAU]]

%\[N:= \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%0\ar@{-}[dddd]\ar@{-}[rrrrdddd]& 1\ar@{-}[rrrddd]& 0 \ar@{-}[rr]\ar@{-}[rrdd]&&0\ar@{-}[dd]\\
%&&&&\\
%&&&&0\\
%&& &&1\\
%0 \ar@{-}[rrrr]&& &&0}}\right)
% \text{ et } J:= \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%0\ar@{-}[rrrrdddd]\ar@{-}[rrr]&&&0&1\\
%1\ar@{-}[rrrddd]&&&&0\ar@{-}[ddd]\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&&\\
%&&&&\\
%0\ar@{-}[rr]&&0&1&0
%}
%}\right) \; \in \mathcal{M}_n(\Kk)\;,\]



alors : $\chi_N(X) = X^n$ et $\chi_J(X) = X^n -1$ et on a bien $N^n=0$ et $J^n = I_n$.
\end{exemple}

\begin{proof}[(s) du théorème]

{\bf 1ère démonstration (algébrique) :}

Notons $B(X)\in \mathcal{M}_n(\Kk[X])$ la transposée de la comatrice de $XI_n -A$. Tous les coefficients de la matrice $B(X)$ sont des polynômes à coefficients dans $\Kk$ de degré $\le n-1$. Il existe donc des matrices :
\[B_0,...,B_{n-1} \in \mathcal{M}_n(\Kk)\]
telles que :
\[B(X) = B_0 + XB_1+...+X^{n-1}B_{n-1} .\]

On a alors :
\[B(X)(XI_n-A) = \det(XI_n-A)I_n\]
\[\iff (B_0 + XB_1+...+X^{n-1}B_{n-1})(XI_n-A) = \chi_A(X)I_n\]
(on développe la partie gauche)
\[
\iff -B_0A +X(B_0 -B_1A) + X^2(B_1-B_2A) + ... + X^{n-1}(B_{n-2}-B_{n-1}A) +X^nB_{n-1} \]
\begin{eqnarray}\label{eq:ch}= \chi_A(X)I_n
\end{eqnarray}

Notons $c_0,...,c_n \in \Kk$ les coefficients du polynôme caractéristique :
\[\chi_A(X) = c_0 + ...+c_nX^n\]
($c_0 = \pm \det A, c_n =1$)
On a donc d'après (\ref{eq:ch}) :

\[-B_0A = c_0I_n\]
\[B_0-B_1A = c_1I_n\]
\[...\]
\[B_{n-1} = c_n I_n\]
et donc :
\[\chi_A(A) = c_0 I_n + c_1A + ...+ c_n A^n \]
\[= -B_0A +(B_0-B_1A)A + (B_1-B_2A)A^2+...+(B_{n-2}A^{n-1}-B_{n-1})A^{n-1} + B_{n-1}A^n\]
\[=0\]
car \og tout se simplifie \fg\ .

{\bf 2ème démonstration (avec les matrices compagnons) :}
On suppose $E$ de dimension finie $n$.
Soit $u$ un endomorphisme de $E$. Soit $v$ un vecteur non nul de $E$. Soit $1 \le k \le n$ le plus grand entier tel que la famille :
\[v, u(v),...,u^{k-1}(v)\]
soit libre. Alors forcément, la famille 
\[v, u(v),...,u^{k-1}(v),u^{k}(v)\]
est liée et \[u^k(v) +c_{k-1}u^{k-1}(v) + ... +c_0v =0 \] pour certains coefficients $c_0,..., c_{k-1 }\in \Kk$.

Posons : $F: =\langle v, u(v),...,u^{k-1}(v) \rangle$. C'est un sous-espace vectoriel de $E$ (de dimension $k$) stable par $u$. De plus la matrice de la restriction $u\Res{F}$ dans la base 
\[v, u(v),...,u^{k-1}(v)\]
est la matrice :

[[ICI TABLEAU]]

%\[A:= \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%0\ar@{-}[rrr]\ar@{-}[rrrddd]&& &0\ar@{-}[ddd]&-c_{0}\ar@{.}[dddd]\\
%1\ar@{-}[rrrddd]&&&&\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&&\\
%&&&0&\\
%0\ar@{-}[rr]&&0&1&-c_{n-1}
%}
%}\right)\]


C'est une matrice compagon donc :
\[\chi_A(X)= X^k + c_{k-1}X^{k-1}+...+c_0 .\]


D'après le lemme \ref{lem:restr}, $\chi_A(X)$ divise $\chi_u(X)$ c'est-à-dire :
\[\chi_u(X)= Q(X)\chi_A(X)\]
pour un certain polynôme $Q(X) \in \Kk[X]$. On a alors :

\[\chi_u(u)(v)= Q(u)\chi_A(u)(v)\]
\[= Q(u)(u^k(v)+c_{k-1}u^{k-1}(v)+...+c_0v)\]
\[=Q(u)(0)=0\]
finalement $\chi_u(u)(v)=0$ pour tout vecteur $v$ de $E$ et $\chi_u(u)=0$.

{\bf 3ème démonstration (par les matrices triangulaires) :}  

Supposons que $T$ est une matrice triangulaire :

[[ICI TABLEAU]]

%\[T=\left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%t_1\ar@{.}[rrdd]\ar@{.}[rr]& &\ar@{.}[dd]\\
%&&\\
%&& t_n}}\right) .\]

Soient \[e_1= \left(\begin{array}{c}
1\\0\\
\vdots\\
0
\end{array}\right) ,...,e_n = \left(\begin{array}{c}
0\\
\vdots\\
0\\
1
\end{array}\right)\]
les vecteurs de la base canonique de $\Kk^n$. On pose aussi : 
\[V_k := \langle e_1,...,e_k\rangle \]
si $1\le  k  \le n$ et $V_0 := 0$. On a alors :
\[\Qq 1 \le k \le n , \, (T-t_kI_n)(V_k) \subset V_{k-1} \]
donc :
\[(T-t_1I_n)...(T-t_nI_n)(\Kk^n) = (T-t_1I_n)...\underbrace{(T-t_nI_n)(V_n)}_{\subset V_{n-1}}\]
\[\subset (T-t_1I_n)...\underbrace{(T-t_{n-1}I_n)(V_{n-1})}_{\subset V_{n-2}}\]
\[\subset (T-t_1I_n)...\underbrace{(T-t_{n-2}I_n)(V_{n-2})}_{\subset V_{n-3}}\]
\[... \subset (T-t_1I_n)(V_1) \subset V_0 = 0\]
donc : $(T-t_1I_n)...(T-t_nI_n)=0$.

Or, $\chi_T(X) = (X-t_1)...(X-t_n)$. Donc :
\[\chi_T(T) = (T-t_1I_n)...(T-t_nI_n) =0 .\]

Soit $A \in \mathcal{M}_n(\Cc)$. On sait que $A$ est trigonalisable c'est-à-dire :
\[\exists P \in \GL_n(\Kk),\, \exists T \mbox{ triangulaire supérieure, } A = PTP^{-1} .\]
Mais alors $\chi_T(X) = \chi_A(X)$ et :
\[\chi_A(A) = P \chi_A(T)P^{-1} = P\chi_T(T)P^{-1} = 0 .\]
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Polynômes annulateurs}

Un {\it polynôme annulateur}\index{polynôme annulateur} d'un endomorphisme $u$ de $E$ est un polynôme $P\in \Kk[X]$ tel que $P(u) = 0$. Par exemple, en dimension finie : $\chi_u(X)$. Un {\it polynôme minimal} de $u$ \index{ polynôme minimal} est un polynôme annulateur de $u$, non nul,  de degré minimal.

\begin{exemple}
Des polynômes minimaux des matrices :

[[ICI TABLEAU]]

%\[O,I_n,N:= \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%0\ar@{-}[dddd]\ar@{-}[rrrrdddd]& 1\ar@{-}[rrrddd]& 0 \ar@{-}[rr]\ar@{-}[rrdd]&&0\ar@{-}[dd]\\
%&&&&\\
%&&&&0\\
%&& &&1\\
%0 \ar@{-}[rrrr]&& &&0}}\right) \; \in \mathcal{M}_n(\Kk),\]


sont respectivement : $X,X-1,X^n$.
\end{exemple} 

\begin{remarque*}

{\bf Rappels sur la division euclidienne :}

Soient $P,Q$ deux polynômes dans $\Kk[X]$. Si $Q \neq 0$, alors il existe un unique couple $(B,R)$ tels que :
\[B,R \in \Kk[X], \, P=BQ+R \text{ et } \deg R < \deg Q\]
($R$ peut éventuellement être  nul).

\begin{proof}
{\bf Unicité : } si $B_0Q+R_0 = B_1Q+R_1 = P$ et $\deg R_{0,1} < \deg Q$ , alors $R_0-R_1 = (B_0-B_1)Q$ et $\deg (R_0-R_1) < \deg Q$ ; donc forcément, $R_0 -R_1 =0$ et $R_0 = R_1 \implies B_0 =B_1$.

{\bf Existence : } On raisonne par récurrence sur le degré de $P$. Si $\deg P < \deg Q$, il suffit de choisir $B=0$ et $ R =P$. Sinon :
\[P = a_0 + ... + a_pX^p,\, Q = b_0 +...+b_qX^q\]
avec $a_i ,b_j \in \Kk,\, a_p,b_q \neq 0,\, p \ge q $.Il suffit alors d'appliquer l'hypothèse de récurrence au polynôme 
\[P-\frac{a_p}{b_q}X^{p-q}Q\]
dont le degré est $< \deg P$.
\end{proof}

\end{remarque*}

\begin{center}
*
\end{center}

\begin{proposition}
Soit $m_u(X)$ un polynôme minimal de $u$. Alors, $m_u$ {\sc divise tous les polynômes annulateurs de $u$.} 
\end{proposition}

\begin{proof}
Si $P(u) = 0$, on fait la division euclidienne de $P$ par $m_u$ :
\[P = Bm_u + R\]
où $\deg R < \deg m_u$. On a : 
\[0 = P(u)=\underbrace{B(u)m_u(u)}_{=0} + R(u) \implies R(u) = 0\]
et $R(X)$ est un polynôme annulateur de $u$ de degré $<\deg m_u$. Forcément, $R=0$ et $m_u(X) divise P(X)$. 
\end{proof}

Il existe donc au plus un {\it unique} polynôme minimal {\it unitaire} (c'est-à-dire son coefficient de plus haut degré vaut $1$) de $u$ \exoo c'est LE polynôme minimal de $u$.

\begin{remarque*}
Si $E$ est de dimension finie, $\chi_u(X)$ est un polynôme annulateur de $u$ (non nul) donc dans ce cas, le polynôme minimal existe toujours de plus :
\[m_u(X) \mbox{ divise } \chi_u(X)\]
dans $\Kk[X]$. 
\end{remarque*}

On définit de même les polynômes annulateurs et le polynôme minimal d'une matrice $A \in \mathcal{M}_n(\Kk)$.

\begin{exercicecours}
Si $E$ est de dimension finie, le polynôme minimal de $u$ co\"{i}ncide avec le polynôme minimal de sa matrice dans n'importe quelle base de $E$.
\end{exercicecours}

\begin{proposition}
Soit $P$ un polynôme annulateur de $u$ un endomorphisme de $E$. Alors, pour tout $\lambda \in \Sp(u)$, $P(\lambda) =0$. En particulier si le polynôme minimal $m_u$ existe, $m_u(\lambda) = 0$ pour toute valeur propre $\lambda$ de $u$. 
\end{proposition}

\begin{proof}
Si $u(x) =\lambda x$, $0 \neq x \in E$. Alors, $0 =P(u) (x) = P(\lambda)x \implies P(\lambda) =0$.
\end{proof}

\begin{proposition}
Les racines de $m_u(X)$ sont exactement les valeurs propres de $u$ c'est-à-dire (si $m_u(X)$ existe) : \[\Qq \lambda \in \Kk ,\;\; m_u(\lambda)=0 \iff \lambda \in \Sp(u) .\]

\end{proposition}

\begin{proof}
Il suffit de démontrer que si $m_u(\lambda) = 0$, alors $\lambda$ est une valeur propre de $u$. Or dans ce cas, $m_u(X) = (X-\lambda)Q(X)$ pour un certain polynôme $Q(X)$ de degré $< \deg m_u(X)$. Donc :
\[0 =m_u(u) = (u-\lambda\id_E)Q(u) .\]
Forcément $Q(u) \neq 0$ par minimalité de $m_u$. Donc $u-\lambda\id_E$ n'est pas injective et donc $\lambda $ est une valeur propre de $u$.
\end{proof}

{\bf Comment trouver le polynôme minimal d'une matrice ?}

\begin{theoreme}
Soit $A \in \mathcal{M}_n(\Kk)$. On suppose que le polynôme caractéristique est scindé :
\[\chi_A(X) = (X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r}\]
où $m_1,...,m_r \ge 1$, $\lambda_1,...,\lambda_r \in \Kk$, sont deux à deux distincts. Alors :
\[m_A(X) = (X-\lambda_1)^{k_1}...(X-\lambda_r)^{k_r}\]
pour certains entiers : $1 \le k_i\le m_i$, $i=1,...,r$. 
\end{theoreme}

\begin{proof}
On note $k_1,...,k_r$ les multiplicités de $m_A(X)$ en les valeurs propres $\lambda_1,...,\lambda_r$. On a déjà vu que $1 \le k_i$ car $m_A(\lambda_i)=0$. On a aussi $k_i \le m_i$, la multiplicité de $\lambda_i$ dans $\chi_A(X)$. Il reste donc à démontrer le lemme suivant :
\begin{lemme}\label{lem:div}
On suppose que le polynôme $P(X)$ divise le produit 
\[(X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r}\]
dans $\Kk[X]$ pour certains $\lambda_i \in \Kk$ deux à deux distincts et certains entiers $m_i \ge 1$. Alors si on note $k_1,...,k_r$ les multiplicités respectives des $\lambda_1,...,\lambda_r$ dans $P(X)$, on a :
\[P(X) = a_d(X-\lambda_1)^{k_1}...(X-\lambda_r)^{k_r}\]
où $a_d$ est le coefficient dominant de $P$.
\end{lemme}
 \begin{proof}[du lemme]
On peut supposer $P$ unitaire c'est-à-dire $a_d =1$. On raisonne par récurrence sur $r \ge 0$. Si $r=0$, il n'y a rien à montrer. Notons  $Q(X) \in \Kk[X]$ le quotient par $P(X)$ : 
\[(X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r} = P(X) Q(X) .\]

La multiplicité de $\lambda_1$ dans $Q(X)$ est : $m_1-k_1$. Donc :
\[P(X) = (X-\lambda_1)^{k_1}\tilde{P}(X) \text{ et } Q(X) = (X-\lambda_1)^{m_1-k_1}\tilde{Q}(X)\]
d'où : 
\[(X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r} = (X-\lambda_1)^{m_1}\tilde{P}(X)\tilde{Q}(X)\]
\[\iff (X-\lambda_1)^{m_2}...(X-\lambda_r)^{m_r} = \tilde{P}(X)\tilde{Q}(X) \]
et on applique l'hypothèse de récurrence.

\end{proof}
\begin{remarque*}
Un cas particulier important à retenir :
les diviseurs unitaires de $(X-\lambda)^n$ sont les $(X-\lambda)^d$ avec $0\le d \le n$ (pour tous $n \ge 0, \lambda \in \Kk$).
\end{remarque*}

\end{proof}

\begin{exercicecours}
\[
\begin{array}{|c|c|c|c|}
\hline
A & \left(\begin{array}{cccc}
0&1&0&0\\
0&0&0&0\\
0&0&0&1\\
0&0&0&0
\end{array}\right) & \left(\begin{array}{cccc}
0&1&0&0\\
0&0&1&0\\
0&0&0&0\\
0&0&0&0
\end{array}\right) & \left(\begin{array}{cccc}
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
0&0&0&0
\end{array}\right) \\\hline
\chi_A(X) & X^4 & X^4 & X^4\\\hline
m_A(X)&X^2&X^3& X^4\\\hline
\end{array}\]
\end{exercicecours}

\begin{exercicecours}[Polynôme minimal d'une diagonale]
Soit 

[[ICI TABLEAU]]

%\[
%D:= \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%\lambda_1\ar@{.}[rrdd]& &\\
%&&\\
%&& \lambda_n}}\right)\]

alors $m_D(X) = \prod_{\lambda \in \Sp(D)}(X-\lambda)$ où $\Sp(D) = \{\lambda_1,...,\lambda_n\}$ et les valeurs propres sont comptées {\it sans multiplicité}.

\end{exercicecours}
{\bf Nouveau critère de diagonalisabilité}

On dit qu'un polynôme $P(X) \in \Kk[X]$ est {\it scindé à racines simples dans $\Kk$} s'il se factorise en :
\[P(X) = a_d (X-\lambda_1)...(X-\lambda_r)\]
où $0 \neq a_d \in \Kk$ et $\lambda_1,...,\lambda_r\in\Kk$ sont deux à deux distincts. 

\begin{theoreme}
Une matrice $A \in \mathcal{M}_n(\Kk)$ est diagonalisable sur $\Kk$ si et seulement si son polynôme minimal est scindé à racines simples sur $\Kk$.
\end{theoreme}

\begin{proof}
{\boldmath $\implies$ :} Si $A$ est diagonalisable, $A$ est semblable à une diagonale. Or deux matrices semblables ont le même polynôme minimal \exoo. Donc il suffit de calculer le polynôme minimal d'une matrice diagonale ce qui est l'objet d'un exercice précédent.

{\boldmath $\Leftarrow$ : } Si $m_A(X) = (X-\lambda_1)...(X-\lambda_r)$ avec $\lambda_1,...,\lambda_r\in \Kk$ deux à deux distincts, la décomposition en éléments simples de la fraction $\frac{1}{m_A(X)}$ donne :

\[\frac{1}{m_A(X)} = \frac{a_1}{X-\lambda_1} +... +\frac{a_r}{X-\lambda_r}\]
où $a_i = \frac{1}{m_A'(\lambda_i)}$ pour tout $i$.

Donc \[1  = a_1 Q_1(X) + ... + a_r Q_r(X)\]
où pur tout $i$ : \[Q_i(X) = \frac{m_A(X)}{X-\lambda_i} = \prod_{j=1\atop j\neq i}^r(X-\lambda_j) \]
est un polynôme. Si on applique cette égalité à la matrice $A$, on trouve :
\[I_n = a_1Q_1(A) +..+a_rQ_r(A)\]
donc si $Z \in \Kk^n$, on a :
\[Z = a_1Q_1(A)(Z) +..+a_rQ_r(A)(Z)\]
or, pour tout $1 \le i \le r$, $Q_i(A)(Z) \in \Ker (A -\lambda_iI_n)$ car :
\[(A-\lambda_iI_n)(Q_i(A)(Z)) = m_A(A)(Z) = 0 .\]

Par conséquent \[\oplus_{i=1}^r \Ker (A-\lambda_iI_n) = \Kk^n\]
et $A$ est diagonalisable.

{\it Remarque : } on peut utiliser aussi le lemme des noyaux.   Si $m_A(X) = (X-\lambda_1)...(X-\lambda_r)$ avec $\lambda_1,...,\lambda_r\in \Kk$ deux à deux distincts, on a :
\[m_A(A) = 0 \iff \Kk^n=\Ker m_A(A) = \Ker (A-\lambda_1I_n) \oplus .... \oplus \Ker (A-\lambda_rI_n)\]
car les polynômes $X-\lambda_i$ sont deux à deux premiers entre eux. En effet si $D(X)$ divise $X-\lambda_i$ et $X-\lambda_j$, $i\neq j$ dans $\Kk[X]$, alors $D(X)$ divise $X-\lambda_i - (X-\lambda_j) = \lambda_j -\lambda_i\in \Kk \setminus \{0\}$ donc $D(X)$ est constant. Donc $A$ est diagonalisable.

\end{proof}

\begin{lemme}[des noyaux]
Soit $u$ un endomorphisme de $E$.

Soient $P(X), Q(X)$ des polynômes premiers entre eux. ALors :
\[\Ker ((PQ)(u)) = \Ker (P(u)) \oplus \Ker (Q(u)) .\]

Généralisation : soient $P_1,...,P_r$ des polynômes deux à deux premiers entre eux. Alors :
\[\Ker (P_1...P_r)(u) = \Ker(P_1(u)) \oplus ... \oplus \Ker (P_r(u)) \]

(énoncés similaires avec des matrices)
\end{lemme}
\begin{proof}
\begin{remarque*}

{\bf Rappels : } On dit que $P(X)$ et $Q(X)$ sont {\it premiers entre eux}\index{premiers entre eux} si :
\[D(X) \in \Kk[X] \mbox{ divise } P(X) \text{ et } Q(X) \mbox{ dans }\Kk[X] \implies D(X) \mbox{constant !}\]

En particulier, sur $\Cc$, deux polynômes sont premiers entre eux si et seulement s'ils n'ont pas de racine commune.

\begin{proposition}
Soient $P,Q \in \Kk[X]$ alors :
\[P,Q \mbox{ sont premiers entre eux } \iff \; \exists A,B \in \Kk[X],\, AP + BQ =1 .\]
\end{proposition}
\begin{proof}
{\boldmath $\Leftarrow$ :} \exoo
{\boldmath $\implies$ :} Soient $D \in \Kk[X]$ un polynôme non nul de degré minimal parmi les polynômes de la forme \[AP + BQ \,,\; A,B \in \Kk[X] .\]

Il suffit de montrer que $D$ est constant. On a donc $D=AP + BQ$. On fait la division euclidienne de $P$ par $D$ :
\[P= CD+R \]
pour un certain $C \in \Kk[X]$ et un certain $R \in \Kk[X]$ de degré $< \deg D$. Mais alors :
\[R = (1-CA)P +(-CB)Q\]
donc par minimalité du degré de $D$, $R= 0$ et $D$ divise $P$. De même $D$ divise $Q$ donc $D$ est constant $ =d \in K\setminus\{0\}$. D'où :
\[1 = \frac{A}{d}P+\frac{B}{d}Q .\]
\end{proof}
\end{remarque*}





On écrit $1 = AP+BQ$ pour certains polynômes $A,B \in \Kk[X]$. On a donc :
\[\id_E = P(u)A(u)+ Q(u)B(u) .\]
Soit $x \in \Ker((PQ)(u))$, alors : \[x = P(u)A(u)(x) + Q(u)B(u)(x) .\] Or, \[P(u)Q(u)B(u)(x) = B(u)P(u)Q(u)(x) = 0.\]

 Donc $Q(u)B(u)(x) \in \Ker (P(u))$. De même, $P(u)A(u)(x) \in \Ker (Q(u))$. Donc :
\[x \in \Ker (P(u))  + \Ker (Q(u)) .\]
Réciproquement, il est clair que \[\Ker (P(u)) \subset \Ker ((PQ)(u)) \text{ et } \Ker (Q(u)) \subset \Ker ((PQ)(u)) .\]

 Donc :
\[\Ker(PQ)(u) = \Ker P(u) + \Ker Q(u)\]
montrons que cette somme est directe :
soit $x\in \Ker P(u) \cap \Ker Q(u)$. Alors :
\[x = A(u)P(u)(x) + B(u)Q(u)(x) = 0 .\]

Pour le cas général : on raisonne par récurrence sur $r$ :

Montrons d'abord que :
\[\Ker (P_1(u)) + ... + \Ker (P_r(u)) =\Ker ((P_1...P_r)(u)) .\]

Soit $x \in \Ker((P_1...P_r)(u))$. Alors comme $P_1$ et $P_2$ sont premiers entre eux, on a :
\[1 = AP_1 + BP_2\] pour certains polynômes $A,B \in \Kk[X]$. Donc en appliquant cette égalié à $u$ :
\[x = A(u)P_1(u)(x) + B(u)P_2(u)(x)\] 
or : $A(u)P_1(u)(x) \in \Ker (P_2...P_r)(u)$ car :
\[(P_2...P_r)(u)A(u)P_1(u)(x) = A(u)(P_1....P_r)(u)(x) =0 .\]
Donc par hypothèse de récurrence :

\[A(u)P_1(u)(x) \in \Ker (P_2(u)) + ... + \Ker (P_r(u))\]
et de même :
\[B(u)P_2(u)(x) \in \Ker (P_1(u)) + \Ker (P_3(u)) +... + \Ker (P_r(u))\]
et donc :
\[x = A(u)P_1(u)(x) + B(u)P_2(u)(x) \in \Ker (P_1(u))  +... + \Ker (P_r(u)) .\]

Il reste à montrer que cette somme est directe :

Supposons que \[x_1 +...+x_r =0\]
pour certains $x_1 \in \Ker (P_1(u)), ... ,x_r \in \Ker((P_r(u))$. si on applique $P_1(u)$, on trouve :
\[P_1(u)(x_2) +... + P_1(u)(x_r) = 0\]

Or, $P_1(u)(x_2) \in \Ker (P_2(u)), ... ,P_1(u)(x_r) \in \Ker((P_r(u))$ donc par hypothèse de récurrence :
\[P_1(u)(x_2) = ... = P_1(u)(x_r)=0\]
Or : $\Ker P_1(u) \cap \Ker P_i(u) =0$ si $i >1$ car $P_1$ et $P_i$ sont premiers entre eux !. Donc :
\[x_2 = ... = x_r =0 \]
et forcément, $x_1=0$.
\end{proof}

\begin{corollaire}
Une matrice $A \in \mathcal{M}_n(\Kk)$ est diagonalisable sur $\Kk$ si et seulement si elle admet un polynôme annulateur scindé à racines simples sur $\Kk$.
\end{corollaire}

\begin{corollaire}
Soit $u$ un endomorphisme de $E$ diagonalisable sur $\Kk$. Si $F$ est un sous-espace de $E$ stable par $u$, alors la restriction $u\Res{F}$ est encore diagonalisable.
\end{corollaire}

\begin{proof}
En effet, 
\[m_u(u) = 0 \implies m_u (u\Res{F}) = 0 \implies m_{u\Res{F}} \mbox{ divise } m_u\]
mais si $m_u$ est scindé à racines simples sur $\Kk$, tous ses diviseurs le sont aussi (voir le lemme \ref{lem:div}). 
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{D\'ecomposition spectrale}

Soient $E = \Kk-$espace vectoriel de dimension finie et $u \in \mathcal{L}(E)$.

{\bf Objectif :} (si $E$ est de dimension finie) construire une base $\mathcal{B}$ telle que :
$$\Mat(u)_\mathcal{B} = \left(\begin{array}{cccc}
T_1 & & &\\
 & T_2 & &\\
&&\ddots&\\
&&&T_p

\end{array}\right)
$$ où les $T_i$ sont des blocs triangulaires supérieures avec diagonale constante.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sous-espaces caract\'eristiques}

\begin{definition}
Soit $\lambda \in\Kk$.
Un vecteur propre généralisé \index{vecteur propre généralisé} de $u$ de poids \index{poids} $\lambda$ est un vecteur $v \in E$ tel que :
\[(u-\lambda\id_E)^m v =0\]
pour un certain entier $m \ge 0$. Le plus petit entier $m$ de la sorte est appelé la hauteur \index{hauteur} de $v$.
\end{definition}

En particulier, les vecteurs propres sont des vecteurs propres généralisés de hauteur $1$. Il est bien pratique de considérer le vecteur nul comme un vecteur propre généralisé de hauteur $0$ pour tout $\lambda \in \Kk$.

\begin{exemple}
Soit $E:=\mathcal{C}^\infty(\Rr)$ le $\Rr-$espace vectoriel des fonctions réelles infiniment dérivables. 
Considérons l'endomorphisme de dérivation $u:= D : E \to E$, $f \mapsto f'$. Les vecteurs propres associés à $\lambda \in \Rr$ sont les fonctions (non nulles) proportionnelles à $e^{\lambda x}$ et les vecteurs propres généralisés sont les fonctions de la forme $p(x)e^{\lambda x}$ pour un certain polynôme $p(x)$ (en effet, si $f =e^{\lambda x}g$, alors :
\[(D-\lambda\id_E)^m (f)= e^{\lambda x}g^{(m)} = 0 \iff g^{(m)} = 0\]
\[\iff g \mbox{ est un polynôme de degré }\le m-1 .\]  La hauteur d'une telle fonction $e^{\lambda x}p(x)$ est $\deg p +1$. En particulier, les polynômes sont les vecteurs propres généralisés associés à $0$.
\end{exemple}

\begin{remarque*}


Si $v$ est un vecteur propre généralisé de hauteur $m$ associé à $\lambda \in \Kk$, alors \[(u-\lambda\id_E)^{m-1}v\] est un vecteur propre de poids (c'est-à-dire de valeur propre)  $\lambda$. Donc $\lambda$ est une racine du polynôme caractéristique (si $E$ est de dimension finie).
\end{remarque*}

\begin{exercicecours}[important]
L'ensemble des vecteurs propres généralisés de poids $\lambda$ et de hauteur $\le m$ est un sous-espace de $E$, stable par $u$ : c'est exactement $\Ker(u-\lambda\id_E)^m$.  
\end{exercicecours}

On a une cha\^ine croissante de sous-espaces stables :
\[\Ker(u-\lambda\id_E) \subset \Ker(u-\lambda\id_E)^2 \subset ...  \]

\begin{definition}
Soit $\lambda \in \Kk$. 
Le sous-espace caractéristique \index{sous-espace caractéristique} de $u$ de poids $\lambda$ est la réunion :
\[E^\lambda(u):= \cup_{n=1}^\infty \Ker(u-\lambda\id_E)^n\]
c'est-à-dire :
\[E^\lambda(u) = \{v \in E \mid \exists m \ge 0,\, (u-\lambda\id_E)^m(v) =0\} \]
c'est un sous-espace de $E$ stable par $u$.
\end{definition}

\begin{remarque*}

La suite des dimensions est croissante :
\[ \dim \Ker(u-\lambda\id_E) \le \dim \Ker(u-\lambda\id_E)^2 \le ...\]

En dimension finie, cette suite est stationnaire donc il existe un entier $m$ tel que : $E^\lambda(u) =\Ker(u-\lambda\id_E)^m$.

\end{remarque*}

Nous allons maintenant voir pourquoi cette notion de sous-espace caractéristique
est importante.

Rappelons que pour tout $\lambda$, le sous-espace $E^\lambda(u)$ est stable par $u$ et donc par tout polynôme en $u$.

\begin{lemme}
i) Si $\dim E^\lambda(u) < \infty$, alors il existe une base de $E^\lambda(u)$ où la matrice de la restriction $u\Res{E^\lambda(u)}$ est triangulaire supérieure avec $\lambda$ sur la diagonale :

[[ICI TABLEAU]]

%\[ \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%\lambda\ar@{-}[rrdd]\ar@{.}[rr]& &\ar@{.}[dd]\\
%&&\\
%&& \lambda}}\right) .\]

ii) Pour tous $\mu \neq \lambda$, $(u-\mu\id_E)^m$ est injectif sur $E^\lambda(u)$.
\end{lemme}

\begin{proof}
i) Soit $k :=\dim(E^\lambda)$.

Notons $V_i := \Ker (u-\lambda\id_E)^i$ pour tout $i \ge 0$. (Donc $V_0=\{0\}$).

Soit $m \ge 0$ le plus petit entier tel que $V_m =V_{m+1}$; Alors :
\[0 = V_0 \substr V_1 \substr ... \substr V_m = V_{m+1}\]
de plus :

\[v \in V_{m+2} \iff (u-\lambda\id_E)^{m+2} v = 0\]
\[\iff (u-\lambda\id_E) v \in V_{m+1}\]
\[\iff (u-\lambda \id_E) v \in V_m \]
\[\iff v \in V_{m+1}\]
donc $V_{m}= V_{m+1}= V_{m+2} = V_{m+3} = ... = E^\lambda$.

Soit $e_1,...,e_{k_1}$ une base de $V_1 = \Ker (u-\lambda\id_E)$ que l'on complète en une base $e_1,...,e_{k_2}$ de $V_2 = \Ker (u-\lambda\id_E)^2$, que l'on complète en ......etc, que l'on complète en $e_1,...,e_{k_m}$ une base de $E^\lambda$.

On a alors : $k_1 < k_2 < ... <k_m=k$ et pour tout $ 0 \le i \le m$ :
\[V_i =\langle e_1,...,e_{k_i}\rangle.\]
Or  pour tout $i \ge 1$ :
\[(u-\lambda\id_E) (V_i) \subset V_{i-1}\]
en particulier, \[ \Qq k_{i-1} < j \le k_{i},\,  u(e_j) = \lambda e_j \mod \langle e_1 ,..., e_{k_{i-1}}\rangle\]
et la matrice de la restriction \[u\Res{E^\lambda}\] dans la base $e_1,...e_{k_m}$ est triangulaire de la forme :

[[ICI TABLEAU]]

%\[B = \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%\lambda\ar@{-}[rrdd]\ar@{.}[rr]& &\ar@{.}[dd]\\
%&&\\
%&& \lambda}}\right) .\]


ii) Il suffit de montrer que $(u-\mu\id_E)$ est injectif sur $E^\lambda(u)$ c'est-à-dire :
\[\Ker(u-\mu\id_E) \cap E^\lambda(u) = 0\]
or, si $(u-\mu \id_E)(x) = 0$ et $x \in E^\lambda(u)$, alors :
\[(u-\lambda\id_E) (x) = (u-\mu\id_E)(x) + (\mu -\lambda)x\]
\[ = (\mu -\lambda) x\]
\[\Qq l \ge 0, \, (u-\lambda\id_E)^l(x) =(\mu-\lambda)^l x\]
\[\implies (\mu-\lambda)^l x = 0\]
pour $l$ assez grand car $x \in E^\lambda(u)$. Donc $x =0$, car $\mu \neq \lambda$.
\end{proof}

\begin{proposition}
Si $E$ est de dimension finie, alors le sous-espace caractéristique de $u$ de poids $\lambda$ est de dimension la multiplicité de $\lambda$ dans le polynôme caractéristique $\chi_u(X)$ :
\[\dim E^\lambda(u)= m_a(\lambda).\]
\end{proposition}
\begin{proof}
Soit $e_1,...,e_k$ une base de $E^\lambda(u)=:E^\lambda$ où la matrice de la restriction $u\Res{E^\lambda(u)}$ est de la forme :

[[ICI TABLEAU]]
%
%\[B = \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%\lambda\ar@{-}[rrdd]\ar@{.}[rr]& &\ar@{.}[dd]\\
%&&\\
%&& \lambda}}\right) .\]

Donc $\chi_{u\Res{E^\lambda}}(X) = (X-\lambda)^{k}$.

 On complète la base $e_1,...,e_k$ en :
\[e_1,...,e_k,e_{k+1},...,e_n\]
une base de $E$. 

Remarquons que $E^\lambda$ est stable par $u$ en effet :
\[(u-\lambda\id_E)^m(v) = 0 \implies (u-\lambda\id_E)^m.u (v) = u. (u-\lambda\id_E)^m (v) = 0 .\]


Dans cette base, la matrice de $u$ est de la forme :

\[\left(
\begin{array}{c|c}
B & ?\\\hline
0 & D
\end{array}
\right)\]
où $D \in \mathcal{M}_{n-k}(\Kk)$.

Donc :
\[\chi_u(X) = (X-\lambda)^k \chi_D(X)\]
il reste donc à montrer que $\chi_D(\lambda) \neq 0$. Sinon, il existerait $0\neq w \in \langle e_{k+1},...,e_n\rangle$ tel que : $D w = \lambda w$.

Mais alors : \[u(w) = \lambda w + y \]
avec $y \in E^\lambda$. Donc : \[(u-\lambda\id_E) w \in E^\lambda = \Ker(u-\lambda\id_E)^m\]
\[\implies (u-\lambda\id_E)^{m+1} w = 0\]
\[\implies w \in E^\lambda \cap \langle e_{k+1} ,..., e_n \rangle \]
\[\implies w =0\]
contradiction !
\end{proof}

\begin{proposition}
Les sous-espaces caractéristiques de poids distincts $\lambda_1,...,\lambda_r$ sont en somme directe
\end{proposition}

\begin{proof}
Soient $v_1,...,v_r$ tels que :
\[v_1+...+v_r =0\]
et $v_i \in E^{\lambda_i}$ pour tout $i$.

Pour tout $i$, il existe un entier $k_i$ tel que :
\[v_i \in  \Ker (u-\lambda_i\id_E)^{k_i}\]
il suffit donc de vérifier que les polynômes $(X-\lambda_i)^{k_i}$ sont deux à deux premiers entre eux car alors : les sous-espaces $\Ker(u-\lambda_i\id_E)^{k_i}$ sont en somme directe d'après le lemme des noyaux et $v_1=...=v_r = 0$. Nous allons montrer que $P(X) = (X-\lambda)^m$, $Q(X)=(X-\mu)^n$, $m,n$ entiers $\ge 1$, $\lambda\neq\mu \in \Kk$ sont premiers entre eux. Soit $c:= \frac{1}{\mu -\lambda}$. On a :
\[1 = c((X-\lambda) - (X-\mu))\]
en élevant à la puissance $m+n-1$ :
\[1 = c^{m+n-1} (r(X) (X-\lambda)^m +s(X)(X-\mu)^n)\]
pour certains polynômes $r(X),s(X) \in \Kk[X]$ de degrés respectifs $\le n-1$ et $\le m-1$ \exoo (utiliser la formule du binôme). Donc, $P(X)$ et $Q(X)$ sont premiers entre eux.  
\end{proof}

\begin{theoreme}
Supposons $E$ de dimension finie. Si le polynôme caractéristique de $u$ est scindé sur $\Kk$, alors :
\[E = \oplus_{i=1}^r E^{\lambda_i}\]
où $\lambda_1,...,\lambda_r$ sont les racines distinctes de $\chi_u(X)$.
\end{theoreme}

\begin{proof}
On a déjà vu que la somme est directe. Si $\chi_u(X) = (X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r}$, alors d'après le théorème de Cayley-Hamilton, $\chi_u(u) = 0$ donc :
\[E = \oplus_{i}\Ker(u-\lambda_i\id_E)^{m_i} \subset \oplus_{i}E^{\lambda_i}.\]
\end{proof}

\subsection*{Interprétation géométrique des multiplicités du polynôme minimal}

Supposons que $E$ est de dimension finie et que le polynôme caractéristique de $u$ est scindé sur $\Kk$. Alors, comme le polynôme minimal $m_u(X)$ de $u$ divise $\chi_u(X)$, $m_u(X)$ est aussi scindé sur $\Kk$. Factorisons-le :
\[m_u(X) = (X-\lambda_1)^{k_1}...(X-\lambda_r)^{k_r}\]
pour certains $\lambda_1,...,\lambda_r \in \Kk$ deux à deux distincts et certains entiers $k_i \ge 1$.

\begin{theoreme}
Pour tout $1 \le i \le r$, $k_i$ est aussi le plus petit entier $m$ tel que \[\Ker (u-\lambda_i\id_E)^{m} = \Ker (u-\lambda_i\id_E)^{m+1} (= E^{\lambda_i})\]
\end{theoreme}

\begin{proof}
Notons $m_i$ le plus petit entier $m$ tel que \[\Ker (u-\lambda_i\id_E)^{m} = \Ker (u-\lambda_i\id_E)^{m+1}\,,\]  pour tout $i$. Alors :
\[E = \oplus_i E^{\lambda_i} = \oplus_i (\Ker (u-\lambda_i\id_E)^{m_i})\]
donc : \[(u-\lambda_1\id_E)^{m_1}...(u-\lambda_r\id_E)^{m_r} = 0\]
et le polynôme $(X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r}$ annule $u$ donc :
\[m_u(X) \mbox{ divise }(X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r} \]
et $k_i \le m_i$ pour tout $i$.

D'un autre côté, $m_u(u)=0 \implies$
\[\oplus_{i=1}^r \Ker (u-\lambda_i\id_E)^{k_i} =E.\]

Soit $x \in E^{\lambda_i}$. Il existe $x_1 \in \Ker (u-\lambda_1)^{k_1},..., x_r \in \Ker(u-\lambda_r)^{k_r}$ tels que :
\[x = x_1 +... + x_r\]
\[\implies x-x_i \in E^{\lambda_i} \cap \left(\oplus_{j=1\atop j \neq i}^r \Ker (u-\lambda_j\id_E)^{k_j}\right)\]
\[\subset E^{\lambda_i} \cap \left(\oplus_{j=1\atop j \neq i}^r E^{\lambda_j} \right)= \{0\}\] 
Donc $x = x_i \in \Ker (u-\lambda_i\id_E)^{k_i}$. D'où : \[E^{\lambda_i} =\Ker (u-\lambda_i\id_E)^{m_i} \subset \Ker (u-\lambda_i\id_E)^{k_i}\]
et donc $k_i \ge m_i$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Projecteurs spectraux}

Supposons $E$ de dimension finie $n$ et le polynôme $\chi_u(X)$ scindé sur $\Kk$ :
$$\chi_u = (X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r}$$
avec $\lambda_i\in \Kk$ deux à deux distincts, $1 \le m_i$ et $m_1+...m_r=n$.
Rappelons que \[E= \oplus_{i=1}^r E^{\lambda_i}.\]
\begin{definition}
Pour toute valeur propre $\lambda_i$, on note $\pi_{\lambda_i}$ ou $\pi_i$ la projection sur le sous-espace $E^{\lambda_i}$ parallèlement au sous-espace :
\[\oplus_{j=1\atop j\neq i}^r E^{\lambda_j}\]
autrement dit si $x= x_1+...+x_r$ où chaque $x_i \in E^{\lambda_i}$, $\pi_i(x) = x_i$, autrement dit (encore) :
\[\pi_i(x) = x \mbox{ si $x \in E^{\lambda_i}$ et 0 si $x \in E^{\lambda_j}$, $i \neq j$.
}\]
Les $\pi_i$ sont les projecteurs spectraux de $u$.

\end{definition}

{\bf Propriétés :} 

--- les $\pi_i$ sont linéaires ;

--- $\pi_1+...+\pi_r =\id_E$ ;
 
--- $\Qq i\neq j,\, \pi_i\pi_j =0$ ;

--- $ \Qq i, \pi_i^2=\pi_i$ ;

--- $\Im \pi_i = E^{\lambda_i}$ ;

--- $\Ker \pi_i = \oplus_{1 \le j \le r \atop j \neq i}E^{\lambda_j}$.

\begin{remarque*}
Si $m_u(X) = (X-\lambda_1)^{k_1}...(X-\lambda_r)^{k_r}$, alors \[E^{\lambda_i} = \Ker(u-\lambda_i\id_E)^{k_i}\]
pour tout $i$.
\end{remarque*}

\begin{proposition}
Les projecteurs spectraux sont des polynômes en $u$.
\end{proposition}

\begin{proof}
En effet, soit $1 \le i \le r$. Posons :
\[Q_i(X):=\frac{m_u(X)}{(X-\lambda_i)^{k_i}} = \prod_{j=1\atop j \neq i}^r(X-\lambda_j)^{k_j} \in \Kk[X] .\]
Comme $Q_i(\lambda_i)\neq 0$, \[Q_i(X) = a_0 + a_1 (X-\lambda_i) + a_2 (X-\lambda_i)^2 +...\]
pour certains coefficients $a_0,a_1,a_2,... \in \Kk$ tels que $a_0\neq 0$. On peut alors trouver \[U_i(X) = b_0 + b_1(X-\lambda_i) + ... + b_{k_i-1}(X-\lambda_i)^{k_i-1} \in \Kk[X]\]
un polynôme de degré $< k_i$ tel que :
\[1 = (a_0 + a_1 (X-\lambda_i) + a_2 (X-\lambda_i)^2 +...)(b_0 + b_1(X-\lambda_i) + ... + b_{k_i-1}(X-\lambda_i)^{k_i-1}) \mod (X-\lambda_i)^{k_i}\]
\[\mbox{c'est-à-dire : } 1= Q_i(X)U_i(X) \mod (X-\lambda_i)^{k_i} .\]


Il suffit alors de remarquer que : \[\pi_i = (U_iQ_i)(u)\]
en effet :
\[\mbox{ si } x \in E^{\lambda_i} =\Ker (u-\lambda_i\id_E)^{k_i},\mbox{ alors } (U_iQ_i)(u)(x) = \id_E(x) = x\]
\[\mbox{ si } x \in E^{\lambda_j} = \Ker (u-\lambda_j\id_E)^{k_j},\,j\neq i,\mbox{ alors } (U_iQ_i)(u)(x) =0.\]  

\begin{remarque*}
Le polynôme $1-U_1(X)Q_1(X) - ... -U_r(X)Q_r(X)$ est de degré $< k_1+...+k_r$. Or, pour tout $i$, la multiplicité de $\lambda_i$ dans le polynôme :
\[1-U_1(X)Q_1(X) - ... -U_r(X)Q_r(X)\] 
est $\ge k_i$ (car si $j \neq i$, $(X-\lambda_i)^{k_i}$ divise $Q_j( X)$), donc : 
\[0 = 1-U_1(X)Q_1(X) - ... -U_r(X)Q_r(X)\]
\[\iff 1 =U_1(X)Q_1(X) + ... +U_r(X)Q_r(X)\]
\[\iff \frac{1}{m_u(X)} = \frac{U_1(X)}{(X-\lambda_1)^{k_1}} + ... + \frac{U_r(X)}{(X-\lambda_1)^{k_r}} . \]
\end{remarque*}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Décomposition de Dunford-Jordan}
Un endomorphisme $N$ de $E$ est nilpotent si $N^k=0$ pour un certain $k \ge 0$.
\begin{theoreme}
Soit $u \in \mathcal{L}(E)$ tel que $\chi_u$ est scindé sur $\Kk$. Alors il existe un unique couple $(d,n)$ tels que :

0) $d,n \in \mathcal{L}(E)$ ;

i) $d$ diagonalisable, $n$ nilpotent ;

ii) $dn=nd$ ;

iii) $u=d+n$.

De plus, $d,n$ sont des polynômes en $u$.
\end{theoreme}

Cette décomposition \[u=d+n\]
est appelée décomposition de Dunford-Jordan.

\begin{remarque*}
Même énoncé avec une matrice $A$ à la place de $u$.
\end{remarque*}

\begin{proof}

soient $\pi_i$ les projecteurs spectraux de $u$.

--- {\bf existence :} $d:= \lambda_1 \pi_1 + ... + \lambda_r \pi_r$, $n:= u - d$.

Pour tout $x \in E^{\lambda_i}$, $d(x) =\lambda_i x$. Donc 
\[E^{\lambda_i} \subset \Ker (d-\lambda_i\id_E) \]
et : \[E = \oplus_i \Ker(d-\lambda_i\id_E)\]
et $d$ est diagonalisable avec les mêmes valeurs propres que $u$.

Pour tout $x \in E^{\lambda_i}$, \[n(x) = u(x) - d(x) \]\[= (u-\lambda_i\id_E)(x)\] et par récurrence :
\[n^k(x) = (u-\lambda_i)^k(x) .\]
Donc si $k \ge \max_{{ 1 \le i \le r}}{\{k_i\}}$, $n^k(x) = 0$.

On a construit $d$ et $n$ comme des polynômes en $u$ ce qui est important pour la suite.

\begin{lemme}
Soit $(d_\alpha)_{\alpha\in \mathcal{A}}$ une famille d'endomorphismes de $E$ diagonalisables qui commutent deux à deux. Si $E$ est de dimension finie alors il existe une base commune de diagonalisation.
\end{lemme}

\begin{proof}
Si tous les $d_\alpha$ sont des homothéties, c'est évident. Sinon on raisonne par récurrence sur $\dim E$ et on choisit un $d_{\alpha_0}$ qui n'est pas une homothétie. Soient $\lambda_1,...,\lambda_r$ ses valeurs propres distinctes. Alors pour tout $i$, $V_i:=\Ker (d_{\alpha_0}-\lambda_i)$ est un sous-espace de $E$ de dimension $< \dim E$ (car $d_{\alpha_0}$ n'est pas une homothétie) et chaque $V_i$ est stable par $d_\alpha$ pour tout $\alpha$ (car $d_\alpha d_{\alpha_0} = d_{\alpha_0}d_\alpha$). Par hypothèse de récurrence il existe une base $\mathcal{B}_i$ de $V_i$ formée de vecteurs propres communs à tous les $d_\alpha$. La réunion :
\[\mathcal{B}_1 \cup ... \cup \mathcal{B}_r\]
est alors une base de $V_1 \oplus...\oplus V_r =E$, une base de diagonalisation pour tous les $d_\alpha$.  
\end{proof}

--- {\bf unicité :} supposons que $u = d'+n'$ avec $d'$ diagonalisable, $n'$ nilpotent et $d'n'=n'd'$. Alors : $ d'-d =n-n'$. Or $n'$ commute avec $d'$ et $n'$ donc avec $u=d'+n'$ et avec $n$ qui est un polynôme en $u$. On en déduit que $n'-n$ est nilpotent \exoo.

De même, $d'$ commute avec $u$ donc $d'$ laisse stable chaque $E^{\lambda_i}=\Ker (u-\lambda_i\id_E)^{k_i}$. Mais alors \[d'\Res{E^{\lambda_i}}\]
est diagonalisable et :
\[(d'-d)\Res{E^{\lambda_i}} = (d'-\lambda_i\id_E)\Res{E^{\lambda_i}}\]
est diagonalisable et nilpotent donc nul (nilpotent $\implies$ la seule valeur propre est $0$ et diagonalisable avec pour seule valeur propre $0$ $\implies$ nul).
Donc $d'=d$ sur chaque $E^{\lambda_i}$, comme $E= \oplus_i E^{\lambda_i}$, par linéarité, $d'=d$. On a aussi : $n'=u-d'=u-d=n$.

\end{proof}
{\bf  \`A retenir :} 

--- $d = \lambda_1 \pi_1 + ... \lambda_r \pi_r$ et les valeurs propres de $u$ sont les valeurs propres de $d$.

--- diagonalisable + nilpotent $\implies$ nul.
\begin{exercicecours}
$\chi_u(X)= \chi_d(X)$.
\end{exercicecours}

\begin{proposition}

Soit $u$ un endomorphisme avec une décomposition de Dunford-Jordan : $u= d+n$, $d $ diagonalisable, $n$ nilpotent et $dn=nd$. Alors :

--- $u$ diagonalisable $\iff$ $u=d$ $\iff$ $n=0$ ;

--- $u$ nilpotent $\iff$ $u=n$ $\iff$ $d=0$.
\end{proposition}

\begin{proof}
C'est une conséquence directe de la décomposition de Dunford-Jordan.
\end{proof}

\begin{exemple}


[[ICI TABLEAU]]
%
%--- si $A =\left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%\lambda\ar@{-}[rrdd]\ar@{.}[rr]& &\ar@{.}[dd]\\
%&&\\
%&& \lambda}}\right)$, $D =\lambda I_n$ et $N = \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%0\ar@{-}[rrdd]\ar@{.}[rr]& &\ar@{.}[dd]\\
%&&\\
%&& 0}}\right)$ ;



--- {\bf ATTENTION !} si $u = \left(\begin{array}{cc}
1 & 3\\
0 & 2
\end{array}\right)$, $d = u$, $n =0$.

\end{exemple}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Calcul pratique des projecteurs spectraux}
\subsection{Méthode}
Soit $u \in \mathcal{L}(E)$. Supposons que $Q\ne 0$ est un polynôme annulateur de $u$ scindé sur $\Kk$ (en particulier $\chi_u$ est scindé !) (Plus le degré de $Q$ est bas moins compliqués sont les calculs).


{\bf 1ère étape :} Factoriser $Q$ : 
$$Q = (X-\lambda_1)^{l_1}...(X-\lambda_r)^{l_r}$$
$\lambda_i$ deux à deux $\neq$ et $l_i \ge 1$.

{\bf 2ème étape :} Décomposer $\frac{1}{Q}$ en éléments simples :
\[(*)\;\;\frac{1}{Q}= \frac{R_1(X)}{(X-\lambda_1)^{l_1}} + ...\]
où $R_i(X)$ : polynômes de degré $< l_i$ (une telle décomposition est unique).

{\bf 3ème étape :} $\pi_{\lambda_i} = R_i(u)Q_i(u)$ où $Q_i(X) : = \frac{Q(X)}{(X-\lambda_i)^{l_i}} = \prod_{1 \le j \le r \atop j \neq i} (X-\lambda_j)^{l_j}$.

{\bf justification : } la décomposition $(*)$ multipliée par $Q$ donne une relation de Bézout :
\[ 1 = R_i(X)Q_i(X) + (X-\lambda_i)^{l_i}S(X)\]
pour un certain polynôme $S(X)$.

\subsection{Exemples}

a) cas où $u$ diagonalisable avec seulement $2$ valeurs propres :

si \[A=\left(\begin{array}{ccc}
1&1&1\\
1&1&1\\
1&1&1
\end{array}\right)\]
alors on sait que $A$ est diagonalisable et que ses valeurs propres sont $0,3$. Les projecteurs spectraux associés $\pi_0$, $\pi_1$ vérifient :
\[\pi_0+\pi_3 = I_3 \text{ et } 3\pi_3 =D=A\]
donc : \[\pi_3=\frac{1}{3} A \text{ et } \pi_0 = I_3-\frac{1}{3}A \]

 b) \[A:=\left(\begin{array}{ccc}
1 & 0 & -3\\
1&-1&-6\\
-1&2&5
\end{array}\right)\]
\[\chi_A(X) = m_A(X)= (X-1)(X-2)^2\]
\[\frac{1}{m_A(X)} = \frac{1}{X-1}+\frac {3-X}{(X-2)^2} \]
\[\iff 1 = \frac{m_A(X)}{X-1} + \frac{m_A(X)(3-X)}{(X-2)^2}\]
donc :
\[\pi_1 = \left(\frac{m_A(X)}{X-1}\right)(A) = (A-2
I_3)^2 \text{ et } \pi_2 = \left(\frac{m_A(X)(3-X)}{(X-2)^2}\right)(A) =  -A^2+4A-3I_3
.\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Réduction de Jordan}


Nous allons montrer que toute matrice dont le polynôme caractéristique est scindé est semblable à une matrice diagonale par blocs avec des blocs \og presque \fg\ diagonaux. 
\subsection{Blocs de Jordan}
\begin{definition}
Un bloc de Jordan \index{bloc de Jordan} est une matrice de la forme :

[[ICI TABLEAU]]

%\[J_{\lambda,n} := \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%\lambda \ar@{-}[rrrddd]&1\ar@{-}[rrdd] &0\ar@{-}[rd]\ar@{-}[r]&0\ar@{-}[d]\\
%0 \ar@{-}[rrdd]\ar@{-}[dd]&&&0\\
%&&&1\\
%0\ar@{-}[rr]&& 0&\lambda
%}
%}\right) \in \mathcal{M}_n(\Kk)\]

où $\lambda \in \Kk, n \ge 0$.
\end{definition}

On a :

[[ICI TABLEAU]]

%\[(J_{\lambda,n}-\lambda I_n)^k = \bordermatrix{
%& & &k\atop |&&&&\cr
%&0&\cdots &0 &1 &0 &\cdots & 0\cr
%&\vdots & & & \ddots &\ddots &&\vdots\cr
%&\vdots& & &  &\ddots&\ddots& \vdots\cr
%&\vdots&&& &&\ddots& 1\cr
%&\vdots&&& &&&0\cr
%&\vdots&&&&&&\vdots\cr
%&0&\cdots&\cdots&\cdots&\cdots&\cdots&0\cr} \mbox{ si $0\le k\le n-1$}\]


\[\text{ et } (J_{\lambda,n}-\lambda I_n)^k = 0 \mbox{ si $n < k$.}\]

On a aussi $J_{\lambda,n}-\mu I_n$ inversible si $\mu \neq \lambda$.

\begin{exercicecours}
Le polynôme caractéristique et le polynôme minimal d'un bloc de Jordan sont égaux à $(X-\lambda)^n$.
\end{exercicecours}

\begin{definition}
Une matrice de Jordan \index{matrice de Jordan} est une matrice diagonale par blocs de la forme :
\[\left(\begin{array}{c|c|c}
J_1&&\\\hline
&\ddots&\\\hline
&&J_r
\end{array}\right)\]
où les $J_i$ sont des blocs de Jordan.
\end{definition}

\begin{exercicecours}
Une matrice de Jordan est diagonalisable si et seulement si ses blocs sont tous de taille $1$.
\end{exercicecours}

\subsection{Matrices nilpotentes}

Supposons que $E$ est un $\Kk-$espace vectoriel de dimension $n$.
Soit $u$ un endomorphisme de $E$.

\begin{definition}
Soit $e \in E$. On appelle hauteur de $e$, notée $h(e)$, le plus petit entier $m \ge 0$ tel que $u^m(e)=0$.  
\end{definition}

\begin{lemme}
Si $e \in E$ un vecteur de hauteur $m$. Alors 
\[e,u(e),...,u^{m-1}(e)\]
sont linéairement indépendants.
\end{lemme}

\begin{proof}
Supposons que \[\lambda_0 e + ... + \lambda_{m-1}u^{m-1}(e) = 0\]
et que $\lambda_k$ est le premier coefficient $\neq 0$, $0 \le k \le m-1$. Alors si on applique $u^{m-k-1}$, on trouve :
\[\lambda_ku^{m-1}(e) =0\]
\[\implies \lambda_k = 0\]
car $u^{m-1}(e) \neq 0$ {\it absurdo}. 
\end{proof}

\begin{corollaire}
On a forcément, $u^{\dim E} = 0$ pour tout endomorphisme nilpotent de $E$.
\end{corollaire}

\begin{definition}
On dit que le sous-espace $\langle e,u(e),...,u^{m-1}(e)\rangle$ est le sous-espace cyclique \index{cyclique}
 de $u$ engendré par $e$.\end{definition}


Un sous-espace cyclique est invariant par $u$ \exoo et la restriction de de $u$  au sous-espace cyclique :
\[\langle e,u(e),...,u^{m-1}(e)\rangle\]a pour matrice

[[ICI TABLEAU]]

%\[J_{0,n}=\left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%0\ar@{-}[rrrddd]&1\ar@{-}[rrdd] &0\ar@{-}[rd]\ar@{-}[r]&0\ar@{-}[d]\\
%0 \ar@{-}[rrdd]\ar@{-}[dd]&&&0\\
%&&&1\\
%0\ar@{-}[rr]&& 0&0
%}
%}\right) \in \mathcal{M}_n(\Kk)\]

dans la base \[u^{m-1}(e),...,u(e),e .\] 


\begin{remarque*}[importante]
Soit $e$ un vecteur de hauteur $m$. Un vecteur $x $ du sous-espace cyclique 
\[\langle e,u(e),...,u^{m-1}(e)\rangle\]

qui n'est pas dans l'image de $u$ est de hauteur $m$.

En effet, si \[x = \lambda_0 e + ... + \lambda_{m-1}u^{m-1}(e)\]
avec $\lambda_0 \neq 0$, $u^m(x) = 0$ et $u^{m-1}(x) = \lambda_0u^{m-1}(e) \neq 0$.
\end{remarque*}

\begin{theoreme}\label{thm:jnilpo}
L'espace $E$ est une {\bf somme directe} de sous-espaces cycliques de l'opérateur $u$ :
\[E = E_1 \oplus ... \oplus E_r .\] En particulier, il existe une base de $E$ où la matrice de $u$ est une matrice de Jordan de la forme :
\[ \left(\begin{array}{c|c|c}
J_{0,n_1}&&\\\hline
&\ddots&\\\hline
&&J_{0,n_r}
\end{array}\right).\]

Et le nombre $r$ de composantes est $r = \dim \Ker u$
\end{theoreme}
\begin{proof}
Supposons que $E$ est une somme directe de sous-espaces cycliques 
\[E_i = \langle e_i,...,u^{n_i-1}(e_i)\rangle\]
alors, la matrice de $u$ dans la base :
\[u^{n_1-1}(e_1),...,e_1,u^{n_2-1}(e_2),...,e_2,...,u^{n_r-1}(e_r),...,e_r\]
est de la forme 
\[\left(\begin{array}{c|c|c}
J_{0,n_1}&&\\\hline
&\ddots&\\\hline
&&J_{0,n_r}
\end{array}\right)\]
donc :
\[\rg u = \rg J_{0,n_1} + ... + \rg J_{0,n_r} \]
\[ (n_1 -1) + ... + (n_r -1) = \dim E -r
\]
%\[\iff r = \dim E - \rg u = \dim \Ker u .\]

Démontrons par récurrence sur $n = \dim E \ge 0$ que $E$ est une somme directe de sous-espaces cycliques.
Si $ n = 0$, il n'y a rien à montrer. Supposons que $n >0$.

Comme $u$ n'est pas surjective \exoo, il existe un sous-espace de $E$, disons $H$, de dimension $n-1$ tel que :
\[\Im u \subset H .\]

Ce $H$ est stable par $u$. Par hypothèse de récurrence,
\[H = H_1 \oplus ... \oplus H_r\]
où les $H_i$ sont des sous-espaces cycliques de $u\Res{H}$ (donc de $u$). On choisit un vecteur $e \in E \setminus H$.

On a :
\[u(e) = u_1 +...+u_r \; , \Qq i,\, u_i \in H_i .\]
Si pour un certain $i$, $u_i = u(v_i)$, avec $v_i \in H_i$, alors on remplace $e$ par $e-v_i \in E \setminus H$. On peut donc supposer que pour tout $i = 1$ à $r$, $u_i = 0$ ou $u_i \in H_i \setminus u(H_i)$. C'est-à-dire : $u_i = 0$ ou $H_i$ est cyclique engendré par $u_i$.
 

Si $u(e) = 0$, alors :
\[E = \Kk e \oplus H_1 \oplus ... \oplus H_r\]
est une décomposition de $E$ en sous-espaces cycliques.

Si $u(e) \neq 0$, alors :
\[0 < h(u(e)) = \max_{i}h(u_i) \exoo .\]
Quitte à renuméroter, on peut supposer que \[h(u(e)) = h(u_1) = : m.\]
Mais alors : $h(e) = m+1$. Vérifions que 
\[E = \langle e,u(e) ,...,u^{m}(e)\rangle \oplus H_2 \oplus ... \oplus H_r .\]

Comme $h(u_1) = \dim H_1 = m $, on a : 
\[\dim E = \dim H + 1 = (m+1) + \dim H_2 + ... + \dim H_r\]
et il suffit de démontrer que 
\[\langle e, ..., u^{m}(e) \rangle \cap (H_2 \oplus ... \oplus H_r) = 0 .\]

Si $\lambda_0 e + ... + \lambda_m u^m(e) \in H_2 \oplus ... \oplus H_r$, alors, comme $e \not\in \Im u$, $\lambda_0 = 0$.

Or, $u(e) = u_1 + ... +u_r$ donc :
\[\lambda_1u(e) + ... + \lambda_m u^m(e) = \lambda_1 u_1 + ... + \lambda_mu^{m-1}(u_1) \mod H_2 \oplus ... \oplus H_r\]
\[\implies \lambda_1 u_1 + ... + \lambda_mu^{m-1}(u_1) \in H_1 \cap (H_2 \oplus ... \oplus H_r) = 0\]
\[\implies \lambda_1 = ... = \lambda_m =0\]
car $h(u_1) = m$. 
\end{proof}

\subsection{Réduction de Jordan}

\begin{theoreme}
Soit $u$ un endomorphisme de $E$ dont le polynôme caractéristique,  $\chi_u(X)$ est {\bf scindé} sur $\Kk$.

Existence : il existe une base de $E$ où la matrice de $u$ est de Jordan c'est-à-dire  :
\[ \Mat (u) = \left(\begin{array}{c|c|c}
J_1&&\\\hline
&\ddots&\\\hline
&&J_r
\end{array}\right)\]
où les $J_i$ sont des blocs de Jordan.

Version matricielle  : si $A \in \mathcal{M}_n(\Kk)$ a son polynôme caractéristique scindé sur $\Kk$, alors, $A$ est semblable (sur $\Kk$) à une matrice de Jordan.

Unicité : le nombre de blocs de Jordan de la forme $J_{\lambda,m}$ noté :
\[ \Qq \lambda \in \Kk, \Qq m \ge 1 ,\; N_{\lambda,m} := \{1 \le i \le r \mid J_i = J_{\lambda,m} \} \]
ne dépend que de $u$ (ou de $A$) :

les $\lambda$ qui apparaissent sont les valeurs propres de $u$ (ou de $A$) et plus précisément, on a :

\[N_{\lambda,m} = \rg(u-\lambda\id_E)^{m+1}-2\rg(u-\lambda\id_E)^m+\rg(u-\lambda\id_E)^{m-1}\]
pour tout $\lambda \in \Kk$ et tout $m \ge 1$.

 
\end{theoreme}

\begin{remarque*}
En particulier, ce théorème s'applique à TOUTES les matrices complexes.
\end{remarque*}


\begin{proof}
Existence : notons $E^{\lambda_1},...,E^{\lambda_r}$ les sous-espaces propres généralisés de $u$. Alors chaque $E^{\lambda_i}$ est stable par $u$ et $E$ se décompose en :
\[E = E^{\lambda_1} \oplus ... \oplus E^{\lambda_r} .\]

De plus , pour tout $i$, \[u\Res{E^{\lambda_i}} -\lambda_i\id_{E^{\lambda_i}}\]
est nilpotent. On peut donc appliquer le théorème \ref{thm:jnilpo} à $u\Res{E^{\lambda_i}} -\lambda_i\id_{E^{\lambda_i}}$ pour tout $i$. Et on remarque que :

\[\left(\begin{array}{c|c|c}
J_{0,n_1}&&\\\hline
&\ddots&\\\hline
&&J_{0,n_r}
\end{array}\right) + \lambda I_{n_1+...+n_r} = \left(\begin{array}{c|c|c}
J_{\lambda,n_1}&&\\\hline
&\ddots&\\\hline
&&J_{\lambda,n_r}
\end{array}\right).\]

Unicité : remarquons que :
\[\rg (J_{\lambda,n} -\mu I_n)^k = \left\{ \begin{array}{cl}
n & \si \mu \neq \lambda\\
n-k & \si \mu = \lambda \text{ et } 0 \le k \le n-1\\
0 & \si \mu = \lambda \text{ et } n \le k
\end{array}\right.\]

donc si la matrice de $u$ dans une certaine base est une matrice de Jordan :
\[
\left(\begin{array}{c|c|c}
J_{\lambda_1,n_1}&&\\\hline
&\ddots&\\\hline
&&J_{\lambda_r,n_r}
\end{array}\right)\]
alors :
 
\[\rg(u-\lambda\id_E)^k = \sum_{q=1}^r \rg ( J_{\lambda_q,n_q}-\lambda I_{n_q})^k\]
\[= \sum_{q=1\atop \lambda_q = \lambda , n_q > k}^r (n_q-k) + \sum_{q=1 \atop \lambda_q \neq \lambda}^r n_q\]
d'où :\[ \rg(u-\lambda\id_E)^{k-1} - \rg(u-\lambda\id_E)^{k } = \sum_{q=1 \atop \lambda_q = \lambda , n_q >k -1 }^r ((n_q-(k-1))-(n_q-k))  \]
\[ =  \sum_{q=1 \atop \lambda_q = \lambda , n_q \ge k }^r 1 \]
et finalement :
\[  (\rg(u-\lambda\id_E)^{k-1} - \rg(u-\lambda\id_E)^k) -  (\rg(u-\lambda\id_E)^k - \rg(u-\lambda\id_E)^{k+1}) =  \sum_{q=1 \atop \lambda_q = \lambda , n_q =k }^r 1 \]
\[\iff \rg(u-\lambda\id_E)^{k+1}-2\rg(u-\lambda\id_E)^k+\rg(u-\lambda\id_E)^{k-1} = N_{\lambda,k} .\]
\end{proof}

{\bf Applications}

--- Si $A \in \mathcal{M}_n(\Cc)$, alors $A$ est semblable à ${}^t A$. En effet, il suffit de le vérifier lorsque $A$ est un bloc de Jordan \exoo.

--- Si $N\in \mathcal{M}_4(\Kk)$ est nilpotente, alors $N$ est semblable à une et une seule  des $5$ matrices suivantes  :
\[0, \, \left(\begin{array}{cccc}
0&1&0&0\\
0&0&0&0\\
0&0&0&0\\
0&0&0&0
\end{array}\right) ,\,\left(\begin{array}{cccc}
0&1&0&0\\
0&0&0&0\\
0&0&0&1\\
0&0&0&0
\end{array}\right),\,\left(\begin{array}{cccc}
0&1&0&0\\
0&0&1&0\\
0&0&0&0\\
0&0&0&0
\end{array}\right),\, \left(\begin{array}{cccc}
0&1&0&0\\
0&0&1&0\\
0&0&0&1\\
0&0&0&0
\end{array}\right). \] 

Il y a une infinité de matrices nilpotentes $4 \times 4$ mais il n'y en a que $5$ à similitude près.

--- Si $A \in \mathcal{M}_3(\Kk)$ a pour polynôme caractéristique : $ \chi_A(X) = (X-1)(X-2)^2$ alors $A$ est semblable 
\[\mbox{ à } \left(\begin{array}{ccc}
1&0&0\\
0&2&0\\
0&0&2
\end{array}\right) \mbox{ ou à } \left(\begin{array}{cccc}
1&0&0\\
0&2&1\\
0&0&2
\end{array}\right) .\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Puissances}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}

--- Problème : résoudre \[
\left\{ \begin{array}{l}
u_{n}=au_{n-1}+ bv_{n-1} \\ v_n =cv_{n-1} + dv_{n-1}
\end{array}\right. \]où $a,b,c,d$ sont fixés ou bien :

\[u_n = au_{n-1} + bu_{n-2}\]
où $a,b$ sont fixés.

Ces deux problèmes se réduisent au calcul de $A^k$ où :
\[A = \left(\begin{array}{cc}
a & b \\
c& d
\end{array}\right)\mbox{ ou } \left(\begin{array}{cc}
0 & 1 \\
b& a
\end{array}\right).\]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cas diagonalisable}

--- cas diagonal : soient $\lambda_1,...,\lambda_n \in \Kk$, alors :
\[\Qq k \ge 0,\; \left(\begin{array}{ccc}
\lambda_1 && \\
&\ddots & \\
&&\lambda_n
\end{array}\right)^k = \left(\begin{array}{ccc}
\lambda_1^k & &\\
&\ddots &\\
&&\lambda_n^k
\end{array}\right).\] 



--- cas diagonalisable : si $A = PDP^{-1}$ avec $A,P,D\in \mathcal{M}_n(\Kk)$, $D$ diagonale, $P$ inversible, alors :
\[A^k=PD^kP^{-1}.\]

C'est encore plus simple avec les projecteurs spectraux :
\[\mbox{ si } A = \lambda_1\pi_1 + ... + \lambda_r \pi_r\]
où les $\lambda_i$ sont les valeurs propres de $A$ et les $\pi_i$ les projecteurs spectraux associés, alors :
\[\Qq k \ge 0, \,A^k = \lambda_1^k \pi_1 + ... +\lambda_r^k\pi_r \]
c'est vrai aussi pour $k$ entier négatif lorsque tous les $\lambda_i$ sont non nuls.
\begin{exemple}
Si 

[[ICI TABLEAU]]

%\[A := \left( \raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%1\ar@{-}[rrdd]\ar@{-}[dd]\ar@{-}[rr] &&1\ar@{-}[dd]\\
%&&\\
%1\ar@{-}[rr]&&1
%}
%}
%\right) \in \mathcal{M}_n(\Qq)\] 

alors  les valeurs propres de $A$ sont $0$ et $n$, et :
\[A = n \pi_n \implies \Qq k,\,A^k = n^k \pi_n = n^{k-1}A .\] 
\end{exemple}

\begin{exercicecours}
--- Si $A= \left(\begin{array}{cc}
\cos t & -\sin t\\
\sin t & \cos t
\end{array}\right)$, alors : \[A = e^{-it}\pi_- + e^{it }\pi_+\]
où :
\[\pi_- = \frac{1}{2}\left(\begin{array}{cc}
1& -i\\
i & 1
\end{array}\right)\mbox{ et }\pi_+ = \frac{1}{2}\left(\begin{array}{cc}
1& i\\
-i & 1
\end{array}\right) .\]

Vérifier alors que :
\[A^k = e^{-ikt}\pi_- + e^{ikt} \pi_+ = \left(\begin{array}{cc}
\cos kt & -\sin kt\\
\sin kt & \cos kt
\end{array}\right).\]


--- Si $A = \left(\begin{array}{cc}
0& 1\\
1 & 1
\end{array}\right)$ alors: 
\[\Qq k \in \Zz, A^k = \frac{1}{\sqrt{5}}\left(\begin{array}{cc}
\alpha^{k-1} -{\alpha'}^{k-1} & \alpha^{k} -{\alpha'}^{k}\\
\alpha^{k} -{\alpha'}^{k}& \alpha^{k+1} -{\alpha'}^{k+1}
\end{array}\right)\]
où $\alpha := \frac{1+\sqrt{5}}{2}$ et $\alpha' := \frac{1-\sqrt{5}}{2}$.
\end{exercicecours}

\begin{exercicecours}
Soit $A:= \left( \begin{array}{ccc}
0&1&1\\
0 &0&1\\
1 &0 &0
\end{array}
\right)$. Soit $\rho$ l'unique valeur propre réelle de $A$ et $\pi_\rho$ le projecteur spectral associé.

a) Vérifier que $\pi_\rho = \frac{1}{3\rho^2-1}\left( \begin{array}{ccc}
\rho^2&\rho&\rho^3\\
1 &\rho^{-1}&\rho\\
\rho &1 &\rho^2
\end{array}
\right)$.

b) Vérifier que les deux valeurs propres complexes conjuguées de $A$ sont de module $< 1$ et en déduire que :

\[\rho^2 =\lim_{n \infty}\frac{A^n_{1,3}}{A^n_{1,2}}\]
(voir page \pageref{exdupilote})

\begin{exercicecours}
Si $A:= \left(\begin{array}{ccc}
1 & 0 & -3\\
1&-1&-6\\
-1&2&5
\end{array}\right)$, alors :
\[\chi_A(X) = m_A(X)= (X-1)(X-2)^2.\]

Vérifier que :
\[\pi_1 =\left(\begin{array}{ccc}
4 & -6& -6\\
2&-3&-3\\
0&0&0
\end{array}\right) \text{ et } \pi_2 = \left(\begin{array}{ccc}
-3& 6 & 6\\
-2&4&3\\
0&0&1\end{array}\right)\]

et en déduire que pour tout $n \ge 0$ :
\[A^n = 2^{n-1}\left(\begin{array}{ccc}
3n-6 & -6n+12 & -9n+12\\
3n-4&-6n+8&-9n+6\\
-n&2n&3n+2
\end{array}\right)+ \left(\begin{array}{ccc}
4 & -6& -6\\
2&-3&-3\\
0&0&0
\end{array}\right).
\]
\end{exercicecours}

\end{exercicecours}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cas général}

\begin{definition}
Pour tous entiers $n,k$, on définit le coefficient binomial\index{coefficient binomial, $n \choose k$, $C_n^k$} par :
\[{n \choose k} := C_n^k := \frac{n(n-1) ...(n-k+1)}{k!}\]
si $k \ge n$ et $ [{n \choose k} := C_n^k:=0$ si $k >n$.

Les $n \choose k$ sont des entiers.
\end{definition}

\begin{proposition}
Soient $A,B \in\mathcal{M}_n(\Kk)$ deux matrices qui commutent. Alors :
\[\Qq k \ge 0,\, (A+B)^k = \sum_{j=0}^k {k \choose j} A^jB^{k-j} .\]
\end{proposition}


En particulier, si $A =D+N$ avec $D$ diagonalisable et $N$ nilpotente qui commutent :
\[A^k = \sum_{j=0}^k{ k \choose j} D^{k-j}N^{j} .\]

\begin{proposition}\label{pro:formulpuis}
Soit $A\in \mathcal{M}_n(\Kk)$. On suppose que le polynôme minimal de $A$ est scindé :
\[m_A(X) = (X-\lambda_1)^{k_1}... (X-\lambda_r)^k_r\]
les $\lambda_i$ étant deux à deux distincts. Notons $\pi_1,...,\pi_r$ les projecteurs spectraux associés aux valeurs propres $\lambda_1,...,\lambda_r$. Alors :

\[\Qq k \ge 0,\, A^k = \sum_{i = 1}^r \left( \sum_{j=0}^{\min \{k,k_i-1\}} {k \choose j} \lambda_i^{k-j}(A-\lambda_iI_n)^j\right) \pi_i .\]
\end{proposition}

\begin{proof}
Il suffit de vérifier cette formule sur chaque sous-espace caractéristique $E^{\lambda_i}$. Or si $x \in E^{\lambda_i}$, $Ax = \lambda_i x + (A-\lambda_iI_n)x$ et $(A-\lambda_i)^{k_i}x = 0$.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Suites récurrentes}

\begin{theoreme}
Soient $a_1,...,a_p \in \Cc$. On suppose $a_p \not=0$.

On note $P(X) : = X^p -a_1X^{p-1} - ... - a_p$, $\lambda_1,...,\lambda_r$ ses racines ({\it s.e. } distinctes) et $k_1,...,k_r$ leurs multiplicités respectives.

Alors les suites vérifiant :
$$\Qq n \ge p ,\; u_n= a_1 u_{n-1} +...+ a_pu_{n-p}$$

sont les suites de la forme :

$$u_n = P_1(n) \lambda_1^n +... + P_r(n)\lambda_r^n$$
où $P_i$ sont des polynômes de degré $< k_i$.
\end{theoreme}

--- rem : $P_i$ peuvent être déterminés par $u_0,...,u_{p-1}$.

\begin{exemple}
Si $p =1$,
\[\Qq n \ge 1,\, u_n = a_1u_{n-1} \iff \Qq n \ge 1, \, u_n = u_0 a_1^n .\]

Si $p =2 $, \[\Qq n \ge 2 ,\, u_n = a_1 u_{n-1} + a_2u_{n-2} \iff \Qq n \ge 2,\, u_n = \alpha_1 \lambda_1^n + \alpha_2 \lambda_2^n\]
pour certains $\alpha_1,\alpha_2$ si $X^2 -a_1X-a_2 = (X-\lambda_1)(X-\lambda_2)$ avec $\lambda_1 \neq \lambda_2$ et :
\[\Qq n \ge 2 ,\, u_n = a_1 u_{n-1} + a_2u_{n-2} \iff \Qq n \ge 2,\, u_n = (\alpha n+ \beta) \lambda^n \]
pour certains $\alpha,\beta$ si $X^2 -a_1X-a_2 = (X-\lambda)^2$.
\end{exemple}

\begin{exercicecours}
Soit $(u_n)$ la suite définie par :
\[u_0 = 0, \, u_1=1, \Qq n \ge 2 ,\, u_n=u_{n-1}+u_{n-2}\]
alors :
\[\Qq n \ge 0, \, u_n = \frac{1}{\sqrt{5}} \left({\left(\frac{1+\sqrt{5}}{2}\right)^n -\left(\frac{1-\sqrt{5}}{2}\right)^n }\right). \]
\end{exercicecours}

\begin{proof}
Si $u_n = n^k\lambda_i^n$, avec $0 \le k < k_i$, alors pour tout $n \ge p$ :
\[u_n -a_1u_{n-1}-...-a_pu_{n-p} = n^k\lambda_i^n - a_1 (n-1)^k \lambda_i^{n-1} - ... - a_p (n-p)^k\lambda_i^{n-p}\]
\[= \lambda_i^{n-p}\left((n-p)^k P(\lambda_i) + (n-p)^{k-1}\lambda_iP'(\lambda_i) + ... + \lambda_i^kP^{(k)}(\lambda_i)\right) \]
\[= 0 .\]

Réciproquement, si :
\[\Qq n \ge p ,\; u_n= a_1 u_{n-1} +...+ a_pu_{n-p}\]
alors on pose :
\[X_n:=\left(\begin{array}{c}
u_{n-p+1}\\
\vdots\\
u_n
\end{array}\right) \in \Kk^p .\]
On a alors :
\[\Qq n\ge p,\, X_n = A X_{n-1}\]
où $A\in \mathcal{M}_p(\Kk)$ est la transposée de la matrice compagnon du polynôme $P(X)$ :

[[ICI TABLEAU]]

%\[A= \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%0\ar@{-}[ddd]\ar@{-}[rrrddd]&1\ar@{-}[rrrddd]&0\ar@{-}[rr]\ar@{-}[rrdd] &&0\ar@{-}[dd]\\
%&&&&\\
%&&&&0\\
%0\ar@{-}[rrr]&&&0&1\\
%a_p\ar@{.}[rrr]&&&&a_1
%}
%}\right)  \]


donc :
\[\chi_A(X) = (X-\lambda_1)^{k_1}...(X-\lambda_r)^{k_r} .\]
Notons $\pi_1,...\pi_r$ les projecteurs spectraux correspondants. D'après la proposition \ref{pro:formulpuis}, 
\[\Qq n \ge p,\, A^n =  \sum_{i = 1}^r \left( \sum_{j=0}^{\min \{n,k_i-1\}} {n \choose j} \lambda_i^{n-j}(A-\lambda_iI_n)^j \right) \pi_i \]
\[= \sum_{i = 1}^r \lambda_i^n \left(\sum_{j=0}^{k_i-1} \frac{{n \choose j}}{\lambda_i^j} (A-\lambda_iI_n)^j \right)\pi_i .\]

Or, \[ \Qq n \ge p,\, X_n = A^{n-p+1}X_{p-1}\]
\[ = A^nX_0\]
si on pose $X_0 : = A^{1-p}X_{p-1}$. 

Donc, $u_n$ est la dernière composante du vecteur :
\[\sum_{i = 1}^r \lambda_i^n \sum_{j=0}^{k_i-1} {n \choose j}\frac{(A-\lambda_iI_n)^j\pi_i(X_0)}{\lambda_i^j}  \]
et il suffit de remarquer que si $0 \le j \le k_i-1$, \[{n \choose j} = \frac{n(n-1)...(n-j+1)}{j!}\] est un polynôme en $n$ de degré $< k_i$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Exponentielle}
Dans ce chapitre, les matrices sont complexes !

Motivation : système différentiel linéaire + formule de Taylor


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exponentielle complexe}

Rappelons que :

--- toute série numérique $\sum_{k=0}^\infty a_k$ à termes réels positifs (ou nuls) converge (dans $\R$) si et seulement si la suite de ses sommes partielles $\sum_{k=0}^Na_k$  est bornée.

--- toute série de nombres complexes $\sum_{k=0}^\infty z_k$ absolument convergente converge
\[\mbox{c'est-à-dire : } \sum_{k=0}^\infty |z_k| < \infty \, \implies \, \sum_{k=0}^\infty z_k \mbox{ converge dans $\Cc$.}\]

--- Pour tout nombre complexe :

\[ \exp z := e^z := \sum_{k=0}^\infty \frac{{z^k}}{k!}\]
et : \[e^0 =1, e^{z+z'} = e^z e^{z'} \; (\Qq z,z' \in \Cc) .\]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Suites de matrices}

\begin{definition}
On dit qu'une suite $(A_k)_{k \in \N}$ de matrices complexes converge vers une matrice $A$ si pour tous $i,j$ la suite des coefficients $A_{k,i,j}$ converge vers le coefficient $A_{i,j}$ dans $\Cc$.
\end{definition}
On pose :
\[
||| A ||| : = \max_i\sum_j|a_{i,j}|\]
pour toute matrice $A$.


--- propriétés : c'est une norme multiplicative ! c'est-à-dire : pour toutes matrices $A,B \in \mathcal{M}_n(\Cc)$, pour tout $\lambda \in \Cc$, on a :

i) $|||A||| = 0 \iff A =0$ ;

ii) $|||A+B||| \le |||A ||| + |||B|||$ ;

iii) $ |||\lambda A||| = |\lambda| |||A|||$ ;

iv) $|||AB||| \le |||A||| |||B|||$.


\begin{remarque*}
Si $A = (a_{i,j})_{1 \le i,j \le n}$, alors pour tous $i,j$,   $|a_{i,j}| \le |||A|||$. On en déduit qu'une suite de matrices $(A_k)_{k \in \N}$ converge vers une matrice $A$ si :
\[\lim_{k \to \infty} |||A_k-A||| = 0.\]
\end{remarque*}

\begin{exercicecours}
En déduire que si $(A_k)$ et $(B_k)$ sont des suites de matrices qui convergent vers $A$ et $B$, alors : \[\lim_{k \to \infty} A_k B_k  =AB .\]
\end{exercicecours}

\begin{exercicecours}
On pose pour tout $X=\left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array} \right)\in \Kk^n$ : $||X|| := \max_{i=1}^n|x_i|$. Alors :
\[|||A||| = \max_{X \in \Kk^n \atop X \neq 0} \frac{||AX||}{||X||}\]
pour toute matrice $A \in \mathcal{M}_n(\Cc).$ 
\end{exercicecours}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Définition de  $\exp(A)$}

\begin{theoreme}
Pour toute matrice $A \in \mathcal{M}_n(\Cc)$, la série :
\[\sum_{k=0}^\infty \frac{A^k}{k!}\]
converge dans $\mathcal{M}_n(\Cc)$. On note :
\[\exp A := e^A := \sum_{k=0}^\infty \frac{A^k}{k!}\]
sa limite. C'est la matrice exponentielle de $A$\index{exponentielle d'une matrice}.
\end{theoreme}

\begin{proof}
Il suffit de démontrer que les séries de coefficients convergent. Or pour tous $i,j$, on a :
\[\sum_{k=0}^\infty\left|\frac{A^k_{i,j}}{k!}\right| \le \sum_{k=0}^\infty\left|\frac{|||A^k|||}{k!}\right|\]
\[\le \sum_{k=0}^\infty \frac{|||A|||^k}{k!}\]
\[= e^{|||A|||} < \infty .\]

Donc pour tous $i,j$, la série $\sum_{k=0}^\infty \frac{A^k_{i,j}}{k!}$ converge dans $\Cc$.
\end{proof}

\begin{exercicecours}
Pour une matrice diagonale 

[[ICI TABLEAUX]]

%$D:= \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%\lambda_1\ar@{.}[rrdd]& &\\
%&&\\
%&& \lambda_n}}\right)$, on a \[\exp D = \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%e^{\lambda_1}\ar@{.}[rrdd]& &\\
%&&\\
%&& e^{\lambda_n}
%}}\right).\]


\end{exercicecours}

\begin{theoreme}[propriétés de l'exponentielle]
On a : \[\exp 0 = I_n \text{ et } \exp (A+B) = \exp A \exp B\]
pour toutes matrices $A$ et $B$ qui commutent. En particulier, pour tout $A \in \mathcal{M}_n(\Cc)$, la matrice $\exp A$ est inversible d'inverse $\exp (-A)$. On a aussi : \[\exp (kA) = (\exp A)^k \] pour tout $k\in \Zz$.
\end{theoreme}

\begin{remarque*}
Attention ! si $A,B$ ne commutent pas, en général $\exp (A+B) \neq \exp A \exp B$.
\end{remarque*}

\begin{remarque*}
En fait l'application : $\exp : \mathcal{M}_n(\Cc) \to \GL_n(\Cc)$ est surjective.
\end{remarque*}


\begin{proof}
Montrons que $\exp (A+B) = \exp A \exp B$ :

\[\Qq m \ge 0 \;\; : \left( \sum_{i=0}^m \frac{A^i}{i!} \right)\left(\sum_{j=0}^m\frac{B^j}{j!}\right)  - \sum_{k=0}^m \frac{(A+B)^k}{k!} \]
\[ = \sum_{0\le i,j \le m }\frac{A^i}{i!}\frac{B^j}{j!} - \sum_{k=0}^m \sum_{i,j \ge 0\atop i+j =k  } \frac{A^i}{i!}\frac{B^j}{j!}\]
\[=\sum_{0\le i,j \le m }\frac{A^i}{i!}\frac{B^j}{j!} - \sum_{0\le i,j \le m \atop i +j \le m}\frac{A^i}{i!}\frac{B^j}{j!}  \]
\[= \sum_{0\le i,j \le m \atop i+j > m }\frac{A^i}{i!}\frac{B^j}{j!} \]
donc :
\[\Qq m\ge 0,\;||| \left( \sum_{i=0}^m \frac{A^i}{i!} \right)\left(\sum_{j=0}^m\frac{B^j}{j!}\right)  - \sum_{k=0}^m \frac{(A+B)^k}{k!} ||| \]\[\le  |||\sum_{0\le i,j \le m \atop i+j > m }\frac{A^i}{i!}\frac{B^j}{j!}||| \]
\[\le \sum_{0\le i,j \le m \atop i+j > m }|||\frac{A^i}{i!}\frac{B^j}{j!}|||\]
\[\le \sum_{0\le i,j \le m \atop i+j > m }\frac{|||A|||^i}{i!}\frac{|||B|||^j}{j!}\]
\[\le \left( \sum_{i=0}^m \frac{|||A|||^i}{i!} \right)\left(\sum_{j=0}^m\frac{|||B|||^j}{j!}\right)  - \sum_{k=0}^m \frac{(|||A|||+|||B|||)^k}{k!}\]
et si \og on fait tendre $m$ vers $+\infty$ \fg\, on trouve :
\[|||\exp A \exp B -\exp (A+B)||| \le e^{|||A|||} e^{|||B|||} -e^{|||A||| + |||B|||} = 0\]
donc : $\exp A \exp B = \exp (A+B)$.
\end{proof}
\begin{exercicecours}
Vérifier que : \[\exp \biggm (t.\left(\begin{array}{cc}
0 & -1\\
 1& 0
\end{array}\right)\biggm ) = \left(\begin{array}{cc}
\cos t &- \sin t \\
\sin t & \cos t
\end{array}\right)\] pour tout $t$ réel.

\end{exercicecours}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Méthode de calcul}

\begin{exercicecours}
Si $P \in \GL_n(\Cc), D\in \mathcal{M}_n(\Cc)$ (par exemple $D$ diagonale), alors :
\[\exp (PDP^{-1}) = P\exp D P^{-1} .\]

En déduire que pour toute matrice $A \in \mathcal{M}_n(\Cc)$, 
\[\det \exp A = e^{\tr A} .\]
 \end{exercicecours}

\begin{proposition}\label{pro:formulexpo}
Soit $A \in \mathcal{M}_n(\Cc)$. Soit \[m_A(X)  = (X-\lambda_1)^{k_1}... (X-\lambda_r)^{k_r}\]
le polynôme minimal de $A$, les $\lambda_i$ étant les valeurs propres deux à deux distinctes de $A$. Notons $\pi_1,...\pi_r$ les projecteurs spectraux associés aux $\lambda_i$.

Alors :
\[\exp (tA) = \sum_{i=1}^re^{t\lambda_i}\left(\sum_{j=0}^{k_i-1}t^j\frac{(A-\lambda_iI_n)^j}{j!}\right) \pi_i .\] 

En particulier, $\exp A$ est un polynôme en $A$.
\end{proposition}

\begin{remarque*}
Si $A$ est diagonalisable, alors, pour tout $t \in \Cc$, $\exp (tA) = e^{t\lambda_1}\pi_1 + ... +e^{t\lambda_r} \pi_r$.
\end{remarque*}

\begin{proof}
On décompose $A$ en : \[A = D+N \]
avec $D$ diagonalisable et $N$ nilpotente qui commutent. Alors : 
\[\exp A = \exp D \exp N.\]

Or, \[ D= \sum_{i=1}^r \lambda_i \pi_i \implies \Qq k \ge 0,\; \frac{D^k}{k!} = \sum_{i=1}^r\frac{\lambda_i^k}{k!}\pi_i\]
et on en déduit que :
\[\exp D = \sum_{k=0}^\infty \frac{D^k}{k!} = \sum_{i=1}^r \left(\sum_{k=0}^\infty \frac{\lambda_i^k}{k!} \right)\pi_i\]
\[= \sum_{i=1}^r e^{\lambda_i}\pi_i .\] 

D'un autre côté, on a :
\[N = \sum_{i=1}^r N\pi_i\]
\[= \sum_{i=1}^r (A-\lambda_iI_n)\pi_i\]
\[\implies \, \Qq k \ge 0\, N^k = \sum_{i=1}^r (A-\lambda_iI_n)^k\pi_i \]
or : $\Qq k \ge k_i,\, (A-\lambda_iI_n)^k\pi_i = 0$ \exoo. 

Donc :
 \[\exp N = \sum_{k=0}^\infty \frac{N^k}{k!} \]
\[= \sum_{i=1}^r \sum_{k=0}^{k_i-1} \frac{(A-\lambda_iI_n)^k}{k!} \pi_i.\]
\end{proof}


\begin{exemple}
Si \[ A:=\left(\begin{array}{ccc}
1 & 0 & -3\\
1&-1&-6\\
-1&2&5
\end{array}\right)\]

alors \[\exp({tA}) = e^{2t}(I_3 + t(A-2I_3))\pi_2 + e^t \pi_1\]

\[ = e^{2t} \left(\begin{array}{ccc}
3t-3 & -6t+6 & -9t+6\\
3t-2&-6t+4&-9t+3\\
-t&2t&3t+1
\end{array}\right) + e^t  \left(\begin{array}{ccc}
4 & -6& -6\\
2&-3&-3\\
0&0&0
\end{array}\right) .\]
\end{exemple}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Équations différentielles}

\subsection{Dérivation des matrices}

On dit qu'une fonction $f$ définie sur un intervalle ouvert $I$ de $\R$ et à valeurs dans $\Cc$ est dérivable en $t_0 \in I$ si la limite :
\[\lim_{t \to t_0 \atop t \neq t_0}\frac{f(t)-f(t_0)}{t-t_0}\]
existe dans $\Cc$. On dit que $f$ est dérivable sur $I$ si elle l'est en tout $t_0 \in I$. 

\begin{definition}
Soient $a_{i,j} : I \to \Cc$, $i,j$, des fonctions dérivables sur un intervalle $I$ de $\R$. On dit que la matrice $A(t) := (a_{i,j}(t))_{1\le i \le p, 1 \le j \le q}$ est dérivable et on note $A'(t) := (a'_{i,j}(t))_{1\le i \le p, 1 \le j \le q}$.
\end{definition}

\begin{exercicecours}
Vérifier que si pour tout $t \in I$, $A(t) \in \mathcal{M}_{p,q}(\Cc)$ et $B(t) \in \mathcal{M}_{q,r}(\Cc)$ et si les matrices $A$ et $B$ sont dérivables sur $I$, alors le produit aussi et on a :
\[\Qq t \in I ,\; (AB)'(t) = A'(t)B(t) + A(t) B'(t) . \]
\end{exercicecours}


\begin{proposition}
Soit $A \in \mathcal{M}_n(\Cc)$. La matrice :
\[t \mapsto \exp (tA)\]
est dérivable sur $\R$ et on a :
\[\Qq t\ \in \Rr, \, (\exp (t A)) ' = A \exp(tA) =  \exp (tA) A .\]
\end{proposition}

\begin{proof}
Soient $\pi_1,...,\pi_r$ les projecteurs spectraux de $A$. Alors d'après la proposition \ref{pro:formulexpo}, on a :
\[\exp (tA) = \sum_{i=1}^r\sum_{k=0}^\infty e^{t\lambda_i}\frac{t^k}{k!} (A-\lambda_i)^k\pi_i\]
(la somme sur $k$ est en fait finie car $(A-\lambda_iI_n)^k\pi_i = 0$ pour $k$ assez grand). Donc :
\[t \mapsto \exp (tA) \]
et dérivable de dérivée :
\[(\exp (tA))' = \sum_{i=1}^r \sum_{k=0}^\infty e^{t\lambda_i}(\lambda_i \frac{t^k}{k!} + k \frac{t^{k-1}}{k!}) (A-\lambda_iI_n)^k\pi_i\]
\[ = \sum_{i=1}^r \sum_{k=0}^\infty e^{t\lambda_i}\lambda_i \frac{t^k}{k!}(A-\lambda_iI_n)^k\pi_i + \sum_{i=1}^r \sum_{k=0}^\infty e^{t\lambda_i} \frac{t^{k}}{k!}(A-\lambda_iI_n)^{k+1}\pi_i\]
\[=\sum_{i=1}^r \sum_{k=0}^\infty e^{t\lambda_i} \frac{t^k}{k!}(\lambda_iI_n + (A-\lambda_iI_n)) (A-\lambda_iI_n)^k\pi_i \]
\[= A \exp (tA) .\] 
\end{proof}

\subsection{\'Equations différentielles linéaires à coefficients constants}

Ce sont les équations de la forme :
\[Y'(t) =AY(t) \]
où $A \in \mathcal{M}_n(\Cc)$ est une matrice {\bf constante} et $Y(t)=\left(\begin{array}{c}
y_1(t)\\
\vdots\\
y_n(t)
\end{array}\right)$ est un vecteur inconnu dont les coordonnées sont des fonctions dérivables.
 
--- cas homogène :

\begin{theoreme}
\[Y' = AY \iff Y(t) = \exp(tA)Y(0)\]
en particulier les solutions sont définies sur $\R$ tout entier.
\end{theoreme}

\begin{proof}
D'un côté, le membre de droite est bien solution de l'équation $Y' = AY$ \exoo. Réciproquement, si on pose $Z(t) : = \exp(-tA)Y(t)$, alors :
\[Z'(t) = \exp (-tA) (Y'-AY) = 0\]
Donc sur $\R$, $Z$ est constante et $Z(t) = Z(0) = Y(0)$ pour tout $t$. 
\end{proof}

\begin{exemple}
Le système :
\[\left\{\begin{array}{lcl}
x'_1(t) &= & x_1(t) -3x_3(t)\\
x'_2(t) & = & x_1(t) - x_2(t) - 6x_3(t)\\
x_3'(t) & = & -x_1(t)+2x_2(t) + 5x_3(t)
\end{array}\right.\]
avec pour \og conditions initiales\fg\ $x_1(0) = 1,x_2(0) = 1, x_3(0) = 0$ a pour solution :
\[\left(\begin{array}{c}
x_1(t)\\
x_2(t)\\
x_3(t)
\end{array}\right) = \exp ( tA) \left(\begin{array}{c}
1\\
1\\
0
\end{array}\right)\]

où $A$ est la matrice $\left(\begin{array}{ccc}
1 & 0 & -3\\
1&-1&-6\\
-1&2&5
\end{array}\right)$. 
On trouve alors :
\[\begin{array}{l}
x_1(t) = (-3t+3)e^{2t}-2e^t\,,\\
x_2(t) = (-3t+2)e^{2t}-e^t\,,\\
x_3(t) = te^{2t}.
\end{array}\]
\end{exemple}

--- solutions de l'équation différentielle linéaire d'ordre $p$ à coefficients constants.


\begin{corollaire}
Soient $a_1,...,a_p \in \Cc$ tels que $a_p\neq 0$. On suppose que :
\[\chi(X):= X^p+a_1X^{p-1}+... +a_p =(X-\lambda_1)^{m_1}...(X-\lambda_r)^{m_r}\]
pour certains $\lambda_i \in \Cc$ deux à deux distincts et certains entiers $m_i \ge 1$.

Alors :
\[(E) \; y^{(p)} +a_1y^{(p-1)} + ... +a_p y = 0 \iff \Qq t\in \Rr, \, y(t) = \sum_{i=1}^r e^{\lambda_it}P_i(t)\]
pour certains polynômes $P_i$ de degré $< m_i$ (pour tout $i$).
\end{corollaire}

\begin{remarque*}
On peut déterminer les $P_i$ en fonction des valeurs $y(0),...,y^{(p-1)}(0)$.
\end{remarque*}


\begin{proof}
{\boldmath $\implies \,:$} On pose $Y(t) :=\left(\begin{array}{c}
y(t)\\
y'(t)\\
\vdots\\
y^{(p-1)}(t)
\end{array}
\right) \in \Cc^p$ pour tout $t$. Alors : \[(E) \iff Y'(t) = AY(t)\]
où $A$ est la matrice

[[ICI TABLEAU]]

%\[ \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%0\ar@{-}[ddd]\ar@{-}[rrrddd]&1\ar@{-}[rrrddd]&0\ar@{-}[rr]\ar@{-}[rrdd] &&0\ar@{-}[dd]\\
%&&&&\\
%&&&&0\\
%0\ar@{-}[rrr]&&&0&1\\
%-a_p\ar@{.}[rrr]&&&&-a_1
%}
%}\right) .\]


Remarquons que $\chi(X) = \chi_A(X) = m_A(X)$.

On a donc \[Y(t) = \exp (tA) Y(0) \]
avec :\[\exp(tA) := \sum_{i=1}^r e^{t\lambda_i}\left(\sum_{k=0}^{m_i-1}\frac{t^k}{k!}(A-\lambda_i I_p)^k \right)\pi_i  \]
où les $\pi_i$ sont les projecteurs spectraux associés aux $\lambda_i$.

Or $y(t)$ est le premier coefficient de $Y(t)$ donc :
\[y(t) = \sum_{i=1}^r e^{\lambda_i t } \underbrace{\sum_{k=0}^{m_i-1} \frac{t^k}{k!} \left((A-\lambda_iI_n)^k\pi_i ( Y(0))\right)_1}_{=:P_i(t)} .\]

{\boldmath $ \Leftarrow :$} Il suffit de vérifier que $y: t \mapsto e^{\lambda_i t}P_i(t)$ est solution de $(E)$ pour tout polynôme de degré $< m_i$. Or pour une telle fonction $y$, on a (en posant $a_0:=1$) :
\[\Qq t \in \Rr,\, y^{(p)}(t) +a_1y^{(p-1)}(t)+ ... +a_p y(t) = \sum_{k=0}^p a_k\sum_{j=0}^k { k \choose j} \lambda_i^{k-j}e^{\lambda_i t} P_i^{(j)}(t)\]
\[ = \sum_{j=0 \atop j < m_i }^p e^{\lambda_i t} P_i^{(j)}(t)\underbrace{\sum_{k=j}^p a_k\frac{k!}{(k-j)!} \lambda_i^{k-j}}_{= \chi^{(j)}(\lambda_i) = 0} \]
\[ = 0 .\]
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Groupe orthogonal}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrices orthogonales}

\begin{definition}
 Une matrice $A$ est orthogonale si ${}^t\!AA=I_n$.


\end{definition}

\begin{exemple}
\[\frac{1}{\sqrt{2}}\left(\begin{array}{cc}
1&-1\\
1&1
\end{array}\right) \text{ et } \frac{1}{3}\left(\begin{array}{ccc}
2&-2&1\\
1&2&2\\
2&1&-2
\end{array}\right)\] sont orthogonales.\end{exemple}

\begin{remarque*}
$A$ orthogonale $\iff$ $A$ inversible et $A^{-1} = {}^tA$ $\iff$ $A{}^t\!A = I_n$.
\end{remarque*}

\begin{remarque*}
Si $A$ est orthogonale, alors $(\det A)^2 = \det ({}^t\!AA) = 1$ donc $\det A = \pm 1$.
\end{remarque*}

\begin{definition}
On note $O_n(\Rr)$ l'ensemble des matrices $n \times n$ réelles orthogonales et $SO_n(\Rr)$ l'ensemble des matrices $n \times n$ réelles orthogonales de déterminant $1$. Les éléments de $SO_n(\Rr)$ sont les rotations de $\Rr^n$.
 \end{definition}

\begin{remarque*}
$I_n \in O_n(\Rr)$, $\Qq A \in O_n(\Rr),\, A^{-1} \in O_n(\Rr)$ $\Qq A,B \in  O_n(\Rr),\, AB \in O_n(\Rr)$ donc $O_n(\Rr)$ est un sous-groupe de $GL_n(\Rr)$. De même $SO_n(\Rr)$ est un sous-groupe de $GL_n(\Rr)$.
\end{remarque*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Produit scalaire}

\begin{definition}
Le produit scalaire standard sur $\Rr^n$ est l'application :
\[\Rr^n \times \Rr^n \to \Rr \;\; (X,Y) \mapsto {}^t\!XY \]

(si $X=\left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array}\right)$ et $Y=\left(\begin{array}{c}
y_1\\
\vdots\\
y_n
\end{array}\right)$, alors $\langle X, Y\rangle = x_1y_1+...+x_ny_n$).\end{definition}

{\it Propriétés :}

\[\Qq x,y,z \in \Rr^n,\, \langle x, y+z \rangle = \langle x,y \rangle + \langle x , z \rangle\]
\[\Qq x,y,z \in \Rr^n,\, \langle x+y, z \rangle = \langle x,z \rangle + \langle y , z \rangle\]
\[\Qq x,y \in \Rr^n,\,\Qq t \in \Rr,\, \langle x, ty \rangle = \langle tx,y \rangle = t\langle x , y\rangle\]
\[\Qq x,y \in \Rr^n,\, \langle x ,y \rangle = \langle y, x \rangle \]
\[\Qq x \in \Rr^n,\, \langle x, x \rangle \ge 0 \text{ et } \langle x, x \rangle = 0 \iff x =0. \]


\begin{definition}
On pose pour tout $x \in \Rr^n$, $||x|| = \sqrt{\langle x , x \rangle}$.
\end{definition}

{ \it Nouvelle caractérisation de la transposée}

\begin{proposition}
Pour tous $X,Y\in \Rr^n$, pour toute matrice réelle $A \in \mathcal{M}_n(\Rr)$, on :
\[\langle AX,Y\rangle = \langle X, {}^t\!AY \rangle .\]
\end{proposition}

\begin{proof}
Évident !
\end{proof}



\begin{proposition}
Soit $A \in \mathcal{M}_n(\Rr)$. Alors sont équivalentes :

i) $A$ est orthogonale ;

ii) $\Qq X \in \Rr^n \,,\, ||AX || = ||X|| $ ;

iii) $\Qq X,Y \in \Rr^n,\, \langle AX , AY \rangle = \langle X , Y \rangle$.
\end{proposition}


\begin{proof}
{ \boldmath$i) \implies ii)$ :} Si $A$ est orthogonale et si $X \in \Rr^n$, alors : 
\[||AX||^2 =\langle AX,AX\rangle = \langle X , {}^t\!AA X \rangle = \langle X, X\rangle.  \]

{\boldmath $ii) \implies iii) $ :} si $\Qq X \in \Rr^n \,,\, ||AX || = ||X||$, alors pour tout $X,Y\in \Rr^n$ :

\[||A(X+Y)||^2 = ||X+Y||^2 \]
\[\iff \langle AX + AY , AX +AY \rangle = \langle X+Y , X+Y \rangle \]
\[ \iff \langle AX , AX \rangle + 2 \langle AX , AY \rangle + \langle AY , AY \rangle = \langle X , X \rangle + 2 \langle X , Y\rangle + \langle Y , Y \rangle\] 
\[\iff \langle AX , AY \rangle = \langle X ,Y \rangle .\]

{\boldmath $iii) \implies i)$ :}

On a :
\[\Qq X,Y \in \Rr^n, \, \langle AX, AY \rangle = \langle X,Y\rangle \]
\[\iff \Qq X,Y \in \Rr^n, \, \langle {}^t\!AAX, Y \rangle = \langle X,Y\rangle \]
\[\iff \Qq X,Y \in \Rr^n, \, \langle {}^t\!AAX-X, Y \rangle = 0 \]
en particulier si $Y = {}^t\!AAX -X$, on trouve : \[|| {}^t\!AAX -X||^2 \]
donc ${}^t\!AAX =X$ pour tout $X \in \Rr^n$ d'où ${}^t\! AA = I_n$.


\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Réflexions orthogonales}

\begin{definition}
Une réflexion orthogonale est une matrice $R \in \mathcal{M}_n(\Rr)$ telle que :
\[i) R = {}^t\!R \,,\, ii) R^2 =I_n ,\,, iii) \dim \Ker (R-I_n)= n-1.\]
\end{definition}

En particulier, les réflexions orthogonales sont des matrices orthogonales.

\begin{exemple}
$R=\left(\begin{array}{cc}
1 & 0\\
0& -1
\end{array}\right)$.
\end{exemple}

\begin{definition}
Soit $v= \left(\begin{array}{c}
v_1\\
\vdots\\
v_n
\end{array}\right) \in \Rr^n$ tel que $||v|| =1$. On définit :
\[G(v_1,...,v_n) := \biggm( v_iv_j\biggm)_{1 \le i,j\le n} \in \mathcal{M}_n(\Rr) \]
et :
\[R_v := I_n -2G(v_1,...,v_n). \]
\end{definition}


\begin{proposition}
La matrice $R_v$ est une réflexion orthogonale et :
\[\Qq X \in \Rr^n,\, R_v(X) = X -2\langle v, X\rangle v .\]
\end{proposition}

\begin{proof}
Si $G= [G(v_1,...,v_n)$ avec $v_1,...,v_n \in \Rr$ tels que $v_1^+...+v_n^2 =1$, alors : $G^2=G$ et ${}^t\! G = G$. Donc $R_v$ est orthogonale car ${}^t\! R_v = R_v$, $R_v^2 =I_n -4G+4G^2 = I_n$ et $R_v - I_n = -2G$ qui est une matrice de rang $1$ donc de noyau de dimension $n-1$.

De plus, si $ X=\left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array}\right) $ et si $ R_v(X)=\left(\begin{array}{c}
y_1\\
\vdots\\
y_n
\end{array}\right)$, alors :
\[y_i = x_i - 2\sum_{j=1}^nv_iv_jx_j = x_i - 2\langle v,X\rangle v_i\]
pour tout $1 \le i \le n$ d'où : $R_v-X) = X-2\langle v , X \rangle v$.

\end{proof}


\begin{lemme}
Soient $x,x' \in \Rr^n$ tels que $||x||=||x'|| =1$. Alors il existe $R \in O_n(\Rr)$ tel que $Rx = x'$.
\end{lemme}

\begin{proof}
Si $x \neq x'$, il suffit de prendre $R =R_v$ où $v:=\frac{x-x'}{||x-x'||}$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Réduction des matrices orthogonales}



\subsection{ \boldmath $O_2(\Rr)$}

Soit $\theta \in \Rr$. On notera $\Delta_\theta : = \Rr \left(\begin{array}{c}
\cos \theta\\
\sin \theta
\end{array}\right)$ la droite du plan $\Rr^2$ passant par $0$ et qui fait un angle $\theta$ avec \og l'axe des abscisses\fg\ .

\begin{exercicecours}
Vérifier que les matrices \[\rho_t := \left( 
\begin{array}{cc}
\cos t & -\sin t \\ 
\sin t & \cos t
    \end{array}
 \right) \text{ et } R_t : = \left(
\begin{array}{cc}
\cos t & \sin t \\ 
\sin t & -\cos t
       \end{array}
 \right)\] sont orthogonales. Ce sont respectivement la matrice de la rotation (de centre $0$) et d'angle $t$ et la matrice de la réflexion orthogonale par rapport à l'axe $\Delta_{\frac{t}{2}}$. Remarquer que $\det \rho_t= -\det R_t= 1$.
\end{exercicecours}



Nous allons voir que ce sont les seules matrices orthogonales $2 \times 2$.



\begin{proposition}
 \[O_2(\R) =\left\lbrace {\left( 
\begin{array}{cc}
\cos t & -\sin t \\ 
\sin t & \cos t
                         \end{array}
                         \right) \mid t \in \Rr} \right\rbrace  \cup \left\lbrace {\left( 
\begin{array}{cc}
\cos t & \sin t \\ 
\sin t & -\cos t
                         \end{array}
                         \right) \mid t \in \Rr  }
\right\rbrace \]

\[SO_2(\Rr) = \left\lbrace {\left(
\begin{array}{cc}
\cos t & -\sin t \\ 
\sin t & \cos t
                         \end{array}
                         \right) \mid t \in \Rr} \right\rbrace  .\]
\end{proposition}



\begin{proof}
\[\left(\begin{array}{cc}
a & b\\
c & d
\end{array}\right) \in O_2(\Rr) \iff \left\{\begin{array}{l}
a^2+c^2 =1\\
ab+cd = 0\\
b^2 +d^2 =1
\end{array}\right.\]
donc il existe $\theta \in \Rr$ tel que $a = \cos \theta$, $c= \sin \theta$. De plus $\left|\begin{array}{cc}
a & b\\
c & d
\end{array}\right| = ad-bc :\epsilon = \pm1$.

On a donc :
\[\left\{\begin{array}{l}
\cos \theta d - \sin \theta b = \epsilon\\
\sin \theta d + \cos \theta b = 0
\end{array}\right.\]
\[\iff \left\{\begin{array}{l}
d = \epsilon \cos \theta\\
b = -\epsilon \sin \theta
\end{array}\right..\]
\end{proof}

\begin{exercicecours}\label{exo:symrot}
\[\Qq t,t'\in \Rr,\, \rho_t\rho_{t'} = \rho_{t+t'} \;,\; R_{t'}R_{t}=\rho_{2(t'-t)}\]
\end{exercicecours}

\begin{exercicecours}
En déduire que $O_2(\Rr)$ est engendré par les réflexions orthogonales.
\end{exercicecours}

\begin{exercicecours}
On a un isomorphisme de groupes : 
\[S^1 \to SO_2(\Rr)\]
\[e^{it } \mapsto \rho_t\]
en particulier, $SO_2(\Rr)$ est commutatif !
\end{exercicecours}



\begin{remarque*}
Pour une rotation $r \in SO_2(\Rr)$, $r =\rho_t \implies \tr r = 2 \cos t$.
\end{remarque*}

\begin{exercicecours}
Pour tout $t$, $R_t = \rho_{\frac{t}{2}} \left(\begin{array}{cc}
1 & 0\\
0& -1
\end{array}\right) \rho_{-\frac{t}{2}}$. 
\end{exercicecours}

\subsection{\boldmath $O_3(\Rr)$}

\begin{theoreme}
Soit $A \in O_3(\Rr)$. Il existe $\epsilon = \pm 1$, $t \in \Rr$ et $P\in O_3(\Rr)$ tels que :
\[A=P\left(\begin{array}{ccc}
\epsilon & 0 & 0\\
0 & \cos t & -sin t\\
0 & \sin t & \cos t
\end{array}\right)P^{-1}\] 
(remarque : alors $\epsilon = \det A$ et $\cos \theta = \frac{\tr A -1}{2}$).
\end{theoreme}

\begin{proof}
Supposons que $\det A =1$. Comme le polynôme caractéristique de $A$ est réell de degré $3$, il admet $3$racines réelles $ \lambda_1,\lambda_2,\lambda_3$ ou une racine réelle $\lambda$ et deux racines complexes conjuguées $\mu,\overline{\mu}$. Dans le prmier cas $\lambda_1\lambda_2\lambda_3 =1$ et dans le second : $\lambda|\mu|^2 = 1$. Dans les deux cas, $A$ admet au moins une valeur propre réelle $\lambda >0$. Il existe alors $v \in \Rr^3$ de norme $1$ tel que : $Av=\lambda v$.Comme $||Av|| = ||v||$, on a :
\[||\lambda v || = \lambda =1\]
donc $1$ est valeur propre de $A$.

Il existe $P\in O_3(\Rr)$ tel que $P\left(\begin{array}{c}
1\\
0\\
0
\end{array}\right) = v$. Mais alors :
\[Av=v\]

\[\iff AP\left(\begin{array}{c}
1\\
0\\
0
\end{array}\right) = P\left(\begin{array}{c}
1\\
0\\
0
\end{array}\right) \]

\[P^{-1} AP \left(\begin{array}{c}
1\\
0\\
0
\end{array}\right)  = \left(\begin{array}{c}
1\\
0\\
0
\end{array}\right) .\]

Posons $B:=P^{-1} AP$. La matrice $B$ est orthogonale car $P$ et $A$ le sont et comme \[B\left(\begin{array}{c}
1\\
0\\
0
\end{array}\right) = \left(\begin{array}{c}
1\\
0\\
0
\end{array}\right) \,,\]

$B$ est de la forme :
\[\left(\begin{array}{ccc}
1 & \alpha & \beta\\
0 & a &b\\
0& c & d
\end{array}\right) .\]

Donc : ${}^t\!BB = I_3 \implies \alpha =\beta = 0$ et $\left(\begin{array}{cc}
a & b\\
c&d
\end{array}\right) \in SO_2(\Rr)$ (car $\det B=1$).
 
Or, on a déjà vu que :

\[SO_2(\Rr) = \left\lbrace {\left(
\begin{array}{cc}
\cos t & -\sin t \\ 
\sin t & \cos t
                         \end{array}
                         \right) \mid t \in \Rr} \right\rbrace  .\]




Si $\det A=-1$, on est ramené au cas précédent avec $-A$ à la place de $A$.
\end{proof}

\begin{definition}
Soit $I_3 \neq A \in SO_3(\Rr)$. On appelle axe de la rotation \index{axe d'une rotation} $A$ la droite : $\Ker (A -I_3)$ (c'est bien une droite \exoo ).
\end{definition}

\begin{exercicecours}
Soient $t,t' \in \Rr$. S'il existe $P \in O_3(\Rr)$ tel que :
\[ \left(\begin{array}{ccc}
1 & 0 & 0\\
0 & \cos t & -sin t\\
0 & \sin t & \cos t
\end{array}\right) =P\left(\begin{array}{ccc}
1 & 0 & 0\\
0 & \cos t' & -sin t'\\
0 & \sin t' & \cos t'
\end{array}\right)P^{-1}\]
 alors $t = \pm t' \mod 2\pi$ ({\it indication : calculer la trace }).

\og Réciproquement \fg\ : \[\left(\begin{array}{ccc}
1 & 0 & 0\\
0 & \cos t & -sin t\\
0 & \sin t & \cos t
\end{array}\right) = P\left(\begin{array}{ccc}
1 & 0 & 0\\
0 & \cos (-t) & -sin(-t) \\
0 & \sin (-t) & \cos (-t)
\end{array}\right)P^{-1}\] 

avec $P=\left(\begin{array}{ccc}
1&0&0\\
0&1&0\\
0&0&-1
\end{array}\right)$. 
\end{exercicecours}

\subsection{Cas général}





\begin{theoreme}
Soit $A \in O_n(\Rr)$. Alors il existe une matrice orthogonale $P$ telle que :
\[A = P RP^{-1}\]
où $R$ est de la forme :

[[ICI TABLEAU]]

%\[R= \left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%1\ar@{.}[rrdd]& &&&&&&&\\
%&&&&&&&&\\
%&& 1&&&&&&\\
%&&&-1\ar@{.}[rrdd]&&&&&\\
%&&&&&&&&\\
%&&&&& -1&&&&\\
%&&&&&& \rho_{\theta_1} \ar@{.}[rrdd]&&\\
%&&&&&&&&\\
%&&&&&&&&\rho_{\theta_r}
%}
%}\right)\]


où $\rho_{\theta_i} = \left(\begin{array}{cc}
\cos \theta_i & -\sin \theta_i\\
\sin \theta_i & \cos \theta_i
\end{array}\right)$.
\end{theoreme}

\begin{remarque*}
Si le nombre de $-1$ dans la matrice réduite est impair, $\det A = -1$, $\det A = 1$ sinon. 
\end{remarque*}

{\it Cas où $n=2,3$ :}
Si $n=2$, $A = \rho_\theta$ ou $A = P\left(\begin{array}{cc}
1 & 0\\
0 & -1
\end{array}\right) P^{-1}$ pour une certaine matrice orthogonale $P$.

Si $n=3$, alors : 
\[A=P\left(\begin{array}{ccc}
\epsilon &0&0\\
0&\cos \theta & -\sin \theta \\
0&\sin \theta & \cos \theta
\end{array}\right)P^{-1}\]
pour une certaine matrice orthogonale $P$ et $\epsilon = \pm 1$. 

\begin{proof} On raisonne par récurrence sur $n$. Notons $e_1,...,e_n$ la base canonique de $\Rr^n$. Soit $A \in O_n(\Rr)$. Soit $\lambda \in \Cc$ une valeur propre de $A$. Si $\lambda \in \Rr$, alors $\lambda =\pm 1$. Soit $v$ un vecteur propre de norme $1$ associé. Soit $P \in O_n(\Rr)$ tel que $Pe_1 = v$. Alors :
\[P^{-1} A P e_1 = e_1\]
donc $P^{-1} A P$ est orthogonale de la forme :

\[\left(\begin{array}{cccc}
1 & 0 & ...& 0\\
0 & b_{1,1}&...&\\
\vdots &...&&\\
0&...&&b_{n-1,n-1}
\end{array}\right)\] où la matrice $(b_{i,j})_{1 \le i,j\le n-1} \in O_{n-1}(\Rr)$. Il suffit d'appliquer l'hypothèse de récurrence à cette matrice.

Supposons maintenant que $A$ n'a pas de valeur propre réelle. Soit $\lambda :=a +ib \in \Cc$ une valeur propre complexe (non réelle).  Soit $Z \in \Cc^n$ un vecteur propre de $A$ associé. Il existe $X,Y \in \Rr^n$ tels que $Z = X+iY$. Alors :
\[AX = aX - bY \text{ et } AY = bX +a Y\]

Comme $A$ est orthogonale, on a aussi :

\[\langle A X ,AX \rangle = \langle X, X \rangle \implies (a^2-1)||X||^2 +b^2||Y||^2 - 2ab \langle X,Y\rangle =0\]
\[\langle A Y ,AY \rangle = \langle Y, Y \rangle \implies (a^2-1)||Y||^2 +b^2||X||^2 + 2ab \langle X,Y\rangle =0\]
\[\langle A X ,AY \rangle = \langle X, Y \rangle \implies (a^2-b^2-1)\langle X,Y\rangle +ab ||X||^2 -ab||Y||^2  =0 .\]

On en déduit : $a^2 + b^2 =1$ et :
\[\langle X,Y\rangle = \frac{b}{2a}(||Y||^2-||X||^2) = -\frac{a}{2b}(||Y||^2-||X||^2)\]
D'où : si $||Y||^2\neq ||X||^2$ : $2b^2 = -2a^2 $ et $a=b=0$ absurde donc $||Y||^2 = ||X||^2$ et $\langle X,Y\rangle =0$.

On peut supposer que $||X|| = ||Y|| =1$. Soit $R_1 \in O_n(\Rr)$ tel que $R_1 X = e_1$. On pose :
\[R_2:=\left\{\begin{array}{l}
I_n \si R_1 Y = e_2\\
R_{\frac{e_2-R_1Y}{||e_2-R_1Y||}} \si R_1 Y \neq e_2.
\end{array} \right.\]
Comme $\langle R_1 Y , e_1\rangle = \langle R_1 Y , R_1 X \rangle = \langle Y, X\rangle =0$, on a $R_2R_1 X = e_1$. On a aussi, $R_2R_1 Y = e_2$.

Donc $(R_2R_1)^{-1} AR_2R_1$ est une matrice orthogonale de la forme :
\[\left(\begin{array}{cc}
U&V\\
0&Z
\end{array}\right)\]
où $U \in \mathcal{M}_2(\Rr)$, $Z \in \mathcal{M}_{n-2}(\Rr)$, $V \in \mathcal{M}_{2,n-2}(\Rr)$.

Comme $ (R_2R_1)^{-1} AR_2R_1$ est orthogonale, on a :
\[U \in O_2(\Rr) \,,\, Z \in O_{n-2}(\Rr)\,,\, {}^t\!V U =0 \implies  V =0 .\]

Il reste à appliquer l'hypothèse de récurrence à $Z$.

\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Les quaternions}\index{quaternions}

Rappelons que :
\[\Cc \iso \left\{\left(\begin{array}{cc}
a & -b\\
b & a
\end{array}\right) \in \mathcal{M}_2(\Rr)\right\} \;\; a+ib \mapsto \left(\begin{array}{cc}
a & -b\\
b & a
\end{array}\right) .\]


Sur le même modèle, on peut construire l'algèbre des quaternions à partir de $\Cc$.

\subsection{Définitions}

\begin{definition}
Soit \[\Hh:=\left\{\left(\begin{array}{cc}
a & b\\
-\overline{b} & \overline{a}
\end{array}\right) \in\mathcal{M}_2(\Cc)\right\}.\]
\end{definition}

\begin{exemple}
Par exemple : \[1:=I_2 ,\, I:=\left(\begin{array}{cc}
i & 0\\
0 & -i
\end{array}\right)\,,\, J:=\left(\begin{array}{cc}
0 & 1\\
-1 & 0
\end{array}\right)\,,\, K:=\left(\begin{array}{cc}
0 & i\\
i & 0
\end{array}\right) \; \in \Hh.\]
\end{exemple}

\begin{proposition}
$\Hh$ est une sous-$\Rr-$algèbre de $\mathcal{M}_2(\Cc)$ c'est-à-dire :
$I_2 \in \Hh$ et $\Hh$ est stable par addition, par multiplication par un scalaire réel et par multiplication.
\end{proposition}

\begin{proof}
Pour la stabilité par multiplication, on vérifie que :

\[\left(\begin{array}{cc}
a & b\\
-\overline{b} & \overline{a}
\end{array}\right)\left(\begin{array}{cc}
a' & b'\\
-\overline{b'} & \overline{a'}
\end{array}\right) = \left(\begin{array}{cc}
aa'-b\overline{b'} & ab'+\overline{a'}b\\
-\overline{ab'+\overline{a'}b} & \overline{aa'-b\overline{b'}}
\end{array}\right) .\]
\end{proof}

\begin{exercicecours}
Vérifier que \[\Hh = \Rr1 \oplus \Rr I \oplus \Rr J \oplus \Rr K .\]
\end{exercicecours}

{\it Table de mulitplications :}
\[I^2 = J^2 = K^2 = -1\]
\[IJ=-JI = K\,,\, JK = -KJ = I \,,\, KI = -IK = J\]
\[IJK = -1\]
\exoo

\begin{remarque*}
Si $s \in \Rr,\vec{v}=\left(\begin{array}{c}
v_1\\
v_2\\
v_3
\end{array}\right)\in \Rr^3$, alors on pose :
\[[s,\vec{v}] := s+v_1I+v_2J+v_3K \in\Hh
 .\]

On a alors pour $s,t \in \Rr$, $\vec{v},\,\vec{w}\in\Rr^3$ :
\[[s,\vec{v}][t,\vec{w}] = [st-\vec{v}.\vec{w},t\vec{v} + s\vec{w} +\vec{v}???\vec{t}??? \vec{w}] .\]
\end{remarque*}

\begin{definition}
On appelle quaternions purs \index{quaternions purs} les quaternions de la forme :
\[xI + y J + z K \,,\, x,y,z \in \Rr\]

et on note $\Hh'$ les sous-espace des quaternions purs.
\end{definition}

On identifiera  $\Rr$ avec $\Rr I_2 \subset \Hh$.

\begin{exercicecours}
Pour tout $q \in \Hh$ :
\[q \in \Rr \iff q^2 \in \Rr_+\]
\[q \in \Hh' \iff q^2 \in \Rr_-\]
\end{exercicecours}



\begin{remarque*}
Pour tout $q = \left(\begin{array}{cc}
a & b\\
-\overline{b} & \overline{a}
\end{array}\right) \in \Hh$, $\det q = |a|^2+|b|^2$. Donc, $q \neq 0 \implies q$ inversible dans $\mathcal{M}_2(\Cc)$.
\end{remarque*}

\begin{proposition}
L'algèbre $\Hh$ est une algèbre à division c'est-à-dire tout $q \in \Hh$ non nul est inversible dans $\Hh$. De plus, le centre de $\Hh$, c'est-à-dire l'ensemble des $x \in \Hh$ tels que  $xq=qx$ pour tout $q \in \Hh$, est $\Rr$.
\end{proposition}

\begin{proof}
Soit $0 \neq q =\left(\begin{array}{cc}
a & b\\
-\overline{b} & \overline{a}
\end{array}\right) \in \Hh$. Alors, \[q^{-1} = \frac{1}{|a|^2 + |b|^2}\left(\begin{array}{cc}
\overline{a} & -b\\
\overline{b} & {a}
\end{array}\right) \in \Hh.\]  

Soit $x = a + bI+cJ+dK \in \Hh$, alors si $x$ est dans le centre de $\Hh$, on a en particulier :
\[xI = Ix \text{ et } xJ =Jx\]
\[\implies b=c=d =0 .\]
La réciproque est clair : si $x \in \Rr$, alors $\Qq q \in \Hh,\, xq=qx$.
\end{proof}

\begin{remarque*}
Dans $\Hh$, l'équation $X^2 +1$ a une infinité de solutions, parmi lesquelles : $I,J,K$.
\end{remarque*}

\begin{exercicecours}
Vérifier que $Q_8 := \{\pm I_1,\pm I,\pm J,\pm K\}$ est un sous-groupe de $\Hh \setminus\{0\}$.
\end{exercicecours}

\subsection{Norme}

Pour tout $q \in \Hh$, on pose :
\[q^* := {}^t\!\overline{q} .\]

{\it Propriétés :}

\[\Qq q_1,q_2 \in \Hh,\, (q_1q_2)^* = q_2^*q_1^*\]
\[\Qq q \in \Hh,\, qq^* = q^*q = \det (q)I_2 \in \Rr_+I_2\]
\[\Qq a,b,c,d \in \Rr,\, (a+bI+cJ+dK)^*= a-bI-cJ-dK .\]
\begin{definition}
On pose pour tout $q \in \Hh$, $||q||:=\sqrt{qq^*}$. 
\end{definition}

\begin{proposition}
L'application :
\[\Hh \times \Hh \to \Rr_+ \; (q,q') \mapsto \langle q,q' \rangle := \frac{1}{2}(q{q'}^* +q'q^*)\]
est un produit scalaire.
\end{proposition}

\begin{proof}
Si $q = a+bI+cJ+dK, \, q'= a' + b'I + c'J +d'K$ avec $a,a',b,b',c,c',d,d'$, alors: $\langle q,q'\rangle = aa' +bb'+cc'+dd'$ ; c'est la formule du produit scalaire standard sur $\Rr^4$.
\end{proof}

On remarque :
\[\Qq q,q'\in \Hh, ||qq'|| = ||q||||q'||\]
donc :
\[G:=S^3:=SU_2:=\{q \in \Hh \mid ||q|| = 1\}\]
est un sous-groupe de $\Hh\setminus\{0\}$.

Ce groupe joue vis-à-vis des rotations de $SO_3(\Rr)$ le même rôle que le groupe $S^1$ vis-à-vis de $SO_2(\Rr)$.
\subsection{Lien avec les rotations}

Pour tout $0 \neq q \in \Hh $, on pose :
\[s_q : \Hh' \to \Hh' \; y \mapsto s_q(y):=qyq^{-1}\]
(vérifier que si $y \in \Hh'$, $s_q(y)\in\Hh' $).

Pour tout $0\neq q \in \Hh$, on notera $S_q$ la matrice de $s_q$ dans la base $I,J, K $ de $\Hh'$. 

\begin{remarque*}
La base $I,J,K$ de $\Hh'$ est orthonormale.
\end{remarque*}

\begin{lemme}
Si $s = s_1I + s_2 J + s_3K \in \Hh'$, avec $s_1,s_2,s_3 \in \Rr$ et $s_1^2+s_2^2 + s_3^2 =1$, alors il existe $g \in S^3$ tel que :
\[s_g(I) = s .\]
\end{lemme}

\begin{proof}
Il existe $\alpha, \beta \in \Rr$ tels que :
\[s_1 = \cos \alpha, \,s_2 = \sin \alpha \cos \beta,\, s_3= \sin \alpha \sin \beta .\]
On pose alors :
\[g = \cos \xi I + \sin \xi \cos \text{ et }a J +\sin \xi \sin \text{ et }a K\]
où $\xi := -\frac{\alpha}{2},\, \text{ et }a =\beta$. On a bien : $s_g(I) = s$.
\end{proof}
\begin{theoreme}
Pour tout $q \in S^3$, $S_q \in SO_3(\Rr)$. De plus l'application :
\[S^3 \to SO_3(\Rr) \; q \mapsto S_q\]
est un morphisme surjectif de groupes de noyau : $\pm 1$ c'est-à-dire :
\[\Qq q,q'\in S^3,\, S_{qq'} = S_qS_{q'} \]\[ S_q = S_{q' } \iff q' = \pm q \]
et  toute rotation $R\in SO_3(\Rr)$ est de la forme $R =S_q$ pour un certain $q\in S^3$.

\end{theoreme}

\begin{proof}
{\it On a bien un morphisme de groupes :}

Soient $q,q'\in S^3$. Alors :
\[\Qq y \in \Hh',\, s_qs_{q'}(y) = qq'y{q'}^{-1} q^{-1} = (qq')y(qq')^{-1} = s_{qq'}(y) .\]
Donc $s_qs_{q'} = s_{qq'}$ d'où : $S_qS_{q'}=S_{qq'}$.

{\it On arrive bien dans $O_3(\Rr)$ ... }

De plus :
\[\Qq y \in \Hh',\, ||s_q(y)|| = ||qyq^{-1}|| = ||q||||y|| ||q^{-1}|| = ||y||\]
donc $S_q \in O_3(\Rr)$ pour tout $q \in S^3$.

{\it ... plus précisément dans $SO_3(\Rr)$ :}

Nous allons voir que les $S_q$ sont en fait des rotations.

{\it On commence par un cas particulier :}

{\it Si $q =a + bI \in S^3$, avec $b \ge 0$}, alors on peut trouver $\theta \in \Rr$ tel que :
\[a = \cos \frac{\theta}{2} \text{ et } b = \sin \frac{\theta}{2} .\]
On vérifie alors que :
\[S_q = \left(\begin{array}{ccc}
1 & 0 & 0\\
0 & \cos \theta & -\sin \theta\\
0 & \sin \theta & \cos \theta
\end{array}\right) \in SO_3(\Rr).\]

{\it Si $q$ est quelconque ...}

Alors : $q =a +p$ où $p \in \Hh'$. On a alors : $1 = a^2 + ||p||^2$. Si $p=0$, alors $S_q=I_3 \in SO_3$. Sinon, $\frac{p}{||p||} \in \Hh '$ et d'après le lemme, il existe $g \in S^3$ tel que :
\[s_g(I) =gIg^{-1} = \frac{p}{||p||} .\]

On a alors :
\[s_g(a+||p||I) = q\]
\exoo. 

Soit $r:=a+||p||I$. On a donc : $grg^{-1} = q$ d'où :
\[S_q=S_gS_rS_g^{-1}\]
or $S_r \in SO_3(\Rr)$ d'après la première partie de la démonstration donc $S_q \in SO_3(\Rr)$ (car de déterminant $1$).

{\it Il reste à montrer la surjectivité }:

Soit $R \in SO_3(\Rr)$. Il existe un vecteur propre $v:=\left(\begin{array}{c}
v_1\\
v_2\\
v_3
\end{array}\right)\in \Rr^3$ de poids $1$ de $R$.D'après le lemme, il existe $g \in S^3$ tel que :
$s_g(I) = v_1I+v_2J+v_3K$. Mais alors :
\[S_g^{-1} RS_g \biggm( \left(\begin{array}{c}
1 \\
0\\
0
\end{array}\right)\biggm) = \left(\begin{array}{c}
1 \\
0\\
0
\end{array}\right) .\]

Donc, comme $S_g^{-1} RS_g \in SO_3(\Rr)$, on a :

\[S_g^{-1} R S_g = \left(\begin{array}{ccc}
1&0&0\\
0& \cos \theta & -\sin\theta\\
0 & \sin \theta & \cos \theta
\end{array}\right)\] 
pour un certain $\theta \in \Rr$. Donc si on pose :
\[r:= \cos \frac{\theta}{2} + \sin  \frac{\theta}{2} I\]
on a :
\[S_g^{-1} R S_g = S_r \iff R = S_gS_rS_g^{-1}\]
\[\iff R = S_{grg^{-1}}\]
d'où la surjectivité.

{\it Pour finir le noyau est $\{\pm 1\}$ :}

Si $S_q = I_3$ alors : 
\[\Qq y \in \Hh',\, s_q(y) = y\]
\[\iff \Qq y \in \Hh',\, qyq^{-1} = y \iff qy = yq\]
\[\iff \Qq y \in \Hh,\, qy = yq\]
\[\iff q \in \Rr.\]

Si $q \in S^3$, alors $||q|| = |q| = 1 \implies q = \pm 1$.



\end{proof}

\begin{exercicecours}
Soit $q \in S^3$. Si $q=\pm 1$, alors $S_q= I_3$. Sinon, $q= a+p$ avec $a \in \Rr$, $0 \neq p \in \Hh'$ et $a^2 + ||p||^2=1$. Mais alors il existe $\theta \in \Rr$ tel que :
\[a = \cos \frac{\theta}{2} \text{ et } ||p|| = \sin \frac{\theta}{2} .\]
Posons aussi $\vec{p}:=\left(\begin{array}{c}
p_1\\
p_2\\
p_3
\end{array}\right)$ si $p = p_1I+p_2J+p_3K$.
Avec ces notations, $S_q$ est la rotation d'axe $\Rr \vec{p}$ et d'angle $\theta$.
\end{exercicecours}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Invariants de similitude}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrices à coefficients polynomiaux}

\begin{lemme}
Soit $A \in \mathcal{M}_n(\Kk[X])$. La matrice $A$ est inversible dans $\mathcal{M}_n(\Kk[X])$ si et seulement si $\det A$ est une constante non nulle. Autrement dit :
\[\GL_n(\Kk[X]) = \left\{ A \in \mathcal{M}_n(\Kk[X]) \mid \det A \in \Kk^*\right\}.\]
\end{lemme}

\begin{proof}
Si $AB = I_n$ pour une matrice $B \in \mathcal{M}_n(\Kk[X])$, alors :
\[\det A \det B = 1\]
donc $\det A$ est un polynôme inversible. Donc $\det A \in \Kk\setminus \{0\}$. Réciproquement, si $\det A \in \Kk\setminus \{0\}$, alors :
\[A^{-1} = \frac{1}{\det A } {}^t\! \text{com}(A) \in \mathcal{M}_n(\Kk[X]) .\]
\end{proof}

\begin{definition}
On notera pour toute matrice non nulle  $A \in \mathcal{M}_n(\Kk[X])$ 
\[d_1(A) := \mbox{ le pgcd unitaire des coefficients de $A$}\]

c'est le polynôme unitaire de degré maximal qui divise tous les coefficients de $A$.
\end{definition}

\begin{proposition}
Si $P,Q \in \mathcal{M}_n(\Kk[X])$ sont des matrices inversibles (c'est-à-dire dont le déterminant est une constante non nulle), alors $d_1(PAQ) = d_1(A)$. 
\end{proposition}

\begin{proof}
Notons $c_{i,j}$ les coefficients de $PAQ$. Alors :
\[\Qq i,j\,,\, c_{i,j} = \sum_{k,l=1}^nP_{i,k}A_{k,l}Q_{l,j}\]
donc $d_1(A)$ divise $c_{i,j}$ pour tous $i,j$. Donc $d_1(A)$ divise $d_1(PAQ)$. De même $d_1(PAQ)$ divise $d_1(A)=d_1(P^{-1} (PAQ) Q^{-1})$. Ainsi, $d_1(A) = d_1(PAQ)$.
\end{proof}

\subsection{Matrices élémentaires}

Ce sont les  matrices de l'une des  formes suivantes :

 $T_{i,j}(\lambda)$ \og la matrice $I_n$ à laquelle on a ajouté un polynôme $\lambda \in \Kk[X]$ en position $i,j$ \fg\ 

\[\left(
\begin{array}{cccc}
1 & ...&\lambda(X) &\\
&\ddots &&\\
&&\ddots&\\
&&&1
\end{array}
\right)
\]

--- $\Sigma_{i}$ \og la matrice obtenue à partir de $I_n$ en permutant les colonnes $i$ et $i+1$ \fg\ :

[[ICI TABLEAU]]

%\[ \Sigma_{i}:=\left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{
%1\ar^{i-1\mathrm{\; fois}}@{-}[rrdd]&&&&&&&\\
%&&&&&&&\\
%&&1&&&&&\\
%&&&0&1&&&\\
%&&&1&0&&&\\
%&&&&&1\ar^{i-1\mathrm{\; fois}}@{-}[rrdd]&&\\
%&&&&&&&\\
%&&&&&&&1
%}
%}\right)
%\]

--- $D_i(\alpha)$ \og la matrice obtenue à partir de $I_n$ en rempla\c{c}ant le $i$ème coefficient diagonal par $\alpha \in \Kk^*$ :

\[\left(\begin{array}{ccccc}
1 &&&&\\
&\ddots &&&\\
&&\alpha &&\\
&&&\ddots &\\
&&&&1
\end{array}\right).\]

\begin{remarque*}
Ces matrices sont toutes inversibles dans $\mathcal{M}_n(\Kk[X])$.
\end{remarque*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Réduction des matrices à coefficients polynomiaux}

\begin{definition}
Soient $A,B \in \mathcal{M}_n(\Kk[X])$. On dira que $A$ est équivalente à $B$, notation : $A \sim B$ s'il existe $P,Q \in \GL_n(\Kk[X])$ telles que : $A = PBQ$.
\end{definition}

\begin{exercicecours}
C'est une relation d'équivalence c'est-à-dire :
\[\Qq A,B,C \in \mathcal{M}_n(\Kk[X]),\, A \sim A\;;\]
\[ A \sim B \implies B \sim A\;;\]
\[ A\sim B \sim C \implies A\sim C.\]
\end{exercicecours}

\begin{lemme}\label{lem:divelem}
Soit $A\in \mathcal{M}_n(\Kk[X])$ une matrice non nulle. Alors, $A$ est équivalente à une matrice de la forme :

[[ICI TABLEAU]]

%\[\left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{d_1(A) & 0\ar@{-}[rr]&&0\\
%0 \ar@{-}[dd]&\\
%&&*=<2cm,2cm>+[F]+\txt{$A'$}*\\
%0&
%}}\right)\]

pour une certaine matrice $A'\in\mathcal{M}_{n-1}(\Kk[X])$.
\end{lemme}

\begin{proof}
On utilise la multiplication à gauche et à droite par des matrices élémentaires. Dans le tableau suivant, on rappelle l'effet de la multiplication d'une matrice $A$ par  les matrices élémentaires :
\vskip 0.5cm

{\scriptsize
\begin{tabular}{|c|c|c|}
\hline
$\mbox{Matrices élémentaires  } \atop E$  & $\mbox{effet de la multiplication à gauche}\atop  EA$&$\mbox{effet de la multiplication à droite} \atop  AE$\\
\hline
$T_{i,j}(\lambda)$ & { \og ajoute $\lambda \times$ la  ligne $i$ à la  ligne j \fg} &{ \og ajoute $\lambda \times$ la  colonne $i$  à la  colonne $j$ \fg}\\
\hline
$D_i(\alpha)$ & {\og multiplie la ligne $i$ par $\alpha$\fg} & {\og multiplie la  colonne $i$ par $\alpha$\fg}\\
\hline
$\Sigma_i$ & \og échange les lignes $i$ et $i+1$ \fg & {\og échange les colonnes $i$ et $i+1$ \fg}\\
\hline
\end{tabular}
}

Soit $d$ le degré minimal d'un coefficient non nul $b_{i,j}$ d'une matrice $B$ équivalente à $A$. Quitte à permuter des lignes ou des colonnes de $B$, on peut supposer que $b_{1,1}$ est de degré $d$. Soit $2\le j\le n$, la division euclidienne de $b_{1,j}$ par $b_{1,1}$ donne :
\[b_{1,j} = qb_{1,1} +r_{1,j}\]
où $\deg r_{1,j} < \deg b_{1,1}$. Donc en retranchant $q \times$ la colonne $1$ à la colonne $j$ de $B$ on obtient une matrice équivalente à $B$ donc à $A$ dont la première ligne est de la forme :
\[b_{1,1}...r_{1,j}...\]
Si $r_{1,j}\neq 0$, on a contredit la minimalité de $d$. Donc $r_{1,j}=0$ et $b_{1,1}$ divise $b_{1,j}$. En raisonnant comme cela avec tous les colonnes $2\le j\le n$ et de même avec toutes les lignes $2 \le i \le n$, on s'aper\c{c}oit que l'on peut supposer que les coefficients $b_{1,j}$ et $b_{i,1}$ sont nuls si $2 \le i,j \le n$. Soit $b_{i,j}$ un coefficient de $B$ avec $i,j \ge 2$. En ajoutant la ligne $i$ à la ligne $1$, on trouve une matrice équivalente à $A$ dont la première ligne comprend les termes :
\[b_{1,1}...b_{i,j}...\]

On a alors vu que $b_{1,1}$ divise $b_{i,j}$.

On a donc montré que $A$ est équivalente à une matrice $B$ de la forme :

[[ICI TABLEAU]]

%\[\left(\raisebox{0.5\depth}{%
%       \xymatrixcolsep{1ex}%
%       \xymatrixrowsep{1ex}%
%       \xymatrix{b_{1,1} & 0\ar@{-}[rr]&&0\\
%0 \ar@{-}[dd]&\\
%&&*=<2cm,2cm>+[F]+\txt{$A'$}*\\
%0&
%}}\right)\]

où $b_{1,1}$ divise tous les coefficients de la matrice $A'$ et où l'on peut supposer que $b_{1,1}$ est unitaire (quitte à multiplier la ligne $1$ par un coefficient constant non nul) . Mais alors $d_1(B) = b_{1,1}$. Et comme $A$ est équivalente à $B$, $d_1(A) = b_{1,1}$.
\end{proof}


\begin{exemple}
\[\left(\begin{array}{cc}
X&-1\\
0& X
\end{array}\right) \sta{C_1\leftrightarrow C_2}{\sim} \left(\begin{array}{cc}
-1&X\\
X&0
\end{array}\right) \sta{-C_1}{\sim}\left(\begin{array}{cc}
1&X\\
-X&0
\end{array}\right) \]
\[\sta{L_2 \leftarrow L_2 + X L_1}{\sim}\left(\begin{array}{cc}
1&X\\
0&X^2
\end{array}\right)\sta{C_2\leftarrow C_2-X C_1}{\sim} \left(\begin{array}{cc}
1&0\\
0& X^2
\end{array}\right).\]
\end{exemple}

\begin{theoreme}
Soit $A \in \mathcal{M}_n(\Kk[X])$. Alors, il existe $r \ge 0$ et une suite $P_1,...,P_r$ de polynômes unitaires dans $\Kk[X]$ tels que : 
\[i)\;\;P_1 | P_2 |...|P_r \]

\[ii)\;\;A \sim \left(\begin{array}{cccccc}
P_1 &&&&&\\
& \ddots&&&&\\
&&P_r&&&\\
&&&0&&\\
&&&&\ddots&\\
&&&&&0
\end{array}
\right)\]
(\og $|$ \fg\ signifie divise).
De plus, si $s \ge 0$ et une suite $Q_1,...,Q_s$ de polynômes unitaires vérifient aussi $i) et ii)$, alors : $s=r$ et $Q_i = P_i$ pour tout $1 \le i \le r$.
\end{theoreme}

\begin{proof}
Pour l'existence des $P_1,...,P_r$, il suffit de raisonner par récurrence sur $n$, la taille de la matrice $A$, et d'utiliser le lemme \ref{lem:divelem}.

Pour l'unicité, on peut aussi raisonner par récurrence sur $n$.

Si on a :
\[\left(\begin{array}{cccccc}
P_1 &&&&&\\
& \ddots&&&&\\
&&P_r&&&\\
&&&0&&\\
&&&&\ddots&\\
&&&&&0
\end{array}
\right) \sim A \sim \left(\begin{array}{cccccc}
Q_1 &&&&&\\
& \ddots&&&&\\
&&Q_s&&&\\
&&&0&&\\
&&&&\ddots&\\
&&&&&0
\end{array}
\right)\]
et $P_1|...|P_r$, $Q_1|...|Q_s$
alors on peut supposer $A \neq 0$ et on a forcément $P_1 = Q_1 = d_1(A)$. Mais alors :
 
\[\left(\begin{array}{cccccc}
P_1 &&&&&\\
& \ddots&&&&\\
&&P_r&&&\\
&&&0&&\\
&&&&\ddots&\\
&&&&&0
\end{array}
\right)  \sim \left(\begin{array}{cccccc}
P_1 &&&&&\\
& \ddots&&&&\\
&&Q_s&&&\\
&&&0&&\\
&&&&\ddots&\\
&&&&&0
\end{array}
\right)\]
entra\^ine 
\[\left(\begin{array}{cccccc}
P_2 &&&&&\\
& \ddots&&&&\\
&&P_r&&&\\
&&&0&&\\
&&&&\ddots&\\
&&&&&0
\end{array}
\right)  \sim \left(\begin{array}{cccccc}
Q_2 &&&&&\\
& \ddots&&&&\\
&&Q_s&&&\\
&&&0&&\\
&&&&\ddots&\\
&&&&&0
\end{array}
\right)\]
\exoo
\end{proof}

\begin{remarque*}
Si $\det A \neq 0$, alors $r = n$ et $\det A = P_1...P_n$.
\end{remarque*}

\begin{definition}
Les $P_i$ différents de $1$ sont appelés les {\it diviseurs élémentaires}\index{diviseurs élémentaires} de $A$. Si $A \in \mathcal{M}_n(\Kk)$, les diviseurs élémentaires de $XI_n-A$ sont appelés les {invariants de similitude} \index{invariants de similitude} de $A$.
\end{definition}

\begin{exercicecours}
Si $A,B \in \mathcal{M}_n(\Kk)$ sont semblables, alors $A$ et $B$ ont les mêmes invariants de similitude. Pour la réciproque, voir la section suivante.
\end{exercicecours}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Invariants de similitude}

{\it Quels sont les invariants de similitude d'une matrice compagnon ?}

\begin{lemme}
Soient 
$P(X) := X^n+a_1X^{n-1}+...+a_n \in \Kk[X]$ et 


[[ICI TABLEAU]]

%\[C_P:=\left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%0\ar@{-}[rrr]\ar@{-}[rrrddd]&& &0\ar@{-}[ddd]&-c_{n}\ar@{.}[dddd]\\
%1\ar@{-}[rrrddd]&&&&\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&&\\
%&&&0&\\
%0\ar@{-}[rr]&&0&1&-c_{1}
%}
%}\right)  \in \mathcal{M}_n(\Kk)\]
la matrice compagnon associée. Alors la matrice $C_P$ a un seul invariant de similitude : le polynôme $P(X)$.
\end{lemme}

\begin{proof}

[[ICI TABLEAU]]

%\[
%XI_n-C_P:=\left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%X\ar@{-}[rrrddd]&0\ar@{-}[rrdd]\ar@{-}[rr]& &0\ar@{-}[dd]&a_n\ar@{.}[dddd]\\
%-1\ar@{-}[rrrddd]&&&&\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&0&\\
%&&&X&\\
%0\ar@{-}[rr]&&0&-1&X+a_{1}
%}
%}\right)  \]
%\[\sta{L_1\leftarrow L_1+XL_2+...+X^{n-1}L_n}{\sim} \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%0&0\ar@{-}[rrdd]\ar@{-}[rr]& &0\ar@{-}[dd]&P(X)\ar@{.}[dddd]\\
%-1\ar@{-}[rrrddd]&X\ar@{-}[rrdd]&&&\\
%0\ar@{-}[dd]\ar@{-}[rrdd]&&&0&\\
%&&&X&\\
%0\ar@{-}[rr]&&0&-1&X+a_{1}
%}
%}\right)  \]
%
%\[\sta{L_1\leftrightarrow L_n}{\sim} \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%-1\ar@{-}[rrrrdddd]&X\ar@{.}[rrrddd]&0\ar@{-}[rr]\ar@{-}[rrdd]& &0\ar@{-}[dd]&a_{n-1}\ar@{.}[ddd]\\
%0\ar@{-}[dddd]\ar@{-}[rrrrdddd]&&&&&\\
%&&&&0&\\
%&&&&X&a_2\\
%&&&&-1&X+a_1\\
%0\ar@{-}[rrrr]&&&&0&P(X)
%}
%}\right)\]
%\[\sim \left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%1\ar@{-}[rrdd]&&&\\
%&&&\\
%&&1&\\
%&&&P(X)
%}
%}\right)\]
\end{proof}

\begin{lemme}\label{lem:eqsem}
Soient $A,B \in \mathcal{M}_n(\Kk)$. alors :
\[XI_n-A \sim XI_n-B \iff \mbox{$A$ est semblable à $B$.}\]
\end{lemme}

\begin{proof}
Démontrons le sens difficile : $\implies $ : on suppose qu'il existe $P,Q\in\GL_n(\Kk[X])$ telles que :
\[XI_n-A = P(XI_n-B)Q.\]

{\it Il existe deux matrices $P_0,P_1 \in \mathcal{M}_n(\Kk)$ telles que $P=(XI_n-A)P_1+P_0$.}

En effet, comme $XI_n-A$ et $A$ commutent, on a pour tout $k \ge 1$ :
\[X^kI_n = ((XI_n-A)+A)^k = (XI_n-A)R_k +A^k\] 
pour une certaine matrice $R_k \in \mathcal{M}_n(\Kk[X])$ (il suffit de prendre $R_k = \sum_{j=0}^{k-1}{k \choose j}(XI_n-A)^{k-j-1}A^j$.)
. Or, $P = X^mC_m + ... + C_0$ pour certaines matrices $C_0,...,C_m \in \mathcal{M}_n(\Kk)$ (on a simplement décomposé les coefficients en somme de monômes et regroupé les monômes par degrés). Donc :
\[P=(XI_n-A) \underbrace{(R_m+...+R_1)}_{=:P_1} + \underbrace{(A^mC_m+...+C_0)}_{=:P_0}.\]

De même, il existe $Q_0,Q_1 \in \mathcal{M}_n(\Kk[X])$ telles que $Q=Q_1(XI_n-A)+Q_0$.

Mais alors :
\[XI_n-A = ((XI_n-A)P_1+P_0)(XI_n-B)(Q_1(XI_n-A)+Q_0) \]
\[ = P_0(XI_n-B)Q_0 + (XI_n-A)P_1(XI_n-B)Q_1(XI_n-A) \]
\[+P_0(XI_n-B)Q_1(XI_n-A)+(XI_n-A)P_1 (XI_n-B)Q_0 .\]
Or :
\[P_0(XI_n-B)Q_1(XI_n-A) = (P-(XI_n-A)P_1)(XI_n-B)Q_1(XI_n-A)\]
\[= (XI_n-A)\biggm( Q^{-1} Q_1 -P_1(XI_n-B)Q_1\biggm)(XI_n-A)\]
car $P(XI_n-B) = (XI_n-A)Q^{-1}$.

De même : \[(XI_n-A)P_1 (XI_n-B)Q_0 = (XI_n-A)\biggm(P_1P^{-1} -P_1(XI_n-B)Q_1\biggm)(XI_n-A) .\]

On a donc montré que :
\[XI_n-A = P_0(XI_n-B)Q_0 +(XI_n-A)S(XI_n-A)\]
où : 
\[S:= Q^{-1} Q_1 -P_1(XI_n-B)Q_1 + P_1P^{-1}  \in \mathcal{M}_n(\Kk[X]).\]

Finalement, on a obtenu :
\[XI_n-A - P_0(XI_n-B)Q_0 = (XI_n-A)S(XI_n-A) .\]
Si $S \neq 0$, le terme de droite est de degré au moins $2$ alors que le  terme de gauche est toujours de degré $\le 1$ : {\it contradiction !} 

Donc $S=0$ et :
\[XI_n-A = P_0(XI_n-B)Q_0\]
avec $P_0,Q_0 \in \mathcal{M}_n(\Kk)$. Enfin, on conclut :
\[XI_n-A = P_0(XI_n-B)Q_0\iff XI_n-A = XP_0Q_0 -P_0BQ_0\]
\[\iff P_0Q_0 = I_n \text{ et } A = P_0BQ_0\]
\[\implies P_0 \mbox{ inversible et }A=P_0BP_0^{-1}.\]

\end{proof}
Voici le théorème principal du chapitre (et même du cours) :
\begin{theoreme}\label{thm:pr}
Soit $A \in \mathcal{M}_n(\Kk)$. Il existe $1 \le r \le n$ et $P_1,...,P_r \in \Kk[X]$ des polynômes unitaires tels que :
\[i) \;\; P_1 | ... | P_r \;;\]
\[ii)\;\; XI_n-A \sim \left(\begin{array}{cccccc}
1 &&&&&\\
& \ddots&&&&\\
&&1&&&\\
&&&P_1&&\\
&&&&\ddots&\\
&&&&&P_r
\end{array}
\right) . \]

De plus, $A$ est semblable à la matrice diagonale par blocs :
\[\left(\begin{array}{c|c|c}
C_{P_1}&&\\\hline
&\ddots&\\\hline
&&C_{P_r}
\end{array}\right)\]
où les $C_{P_i}$ sont les matrices compagnons associées aux polynômes $P_i$. En particulier :

\[\chi_A(X) = P_1...P_r \text{ et } P_r \mbox{ est le polynôme minimal de $A$.}\]
\end{theoreme}

\begin{corollaire}
Si $A,B \in \mathcal{M}_n(\Kk)$ ont les mêmes invariants de similitude alors $A$ et $B$ sont semblables.
\end{corollaire}

\begin{proof}
$A$ et $B$ sont semblables à la même matrice diagonale par blocs \og compagnons\fg\ d'après le théorème. 
\end{proof}

\begin{proof}[du théorème principal]
Soient $P_1,...,P_r$ les invariants de similitude de $A$. Alors $i)$ et $ii)$ sont vérifiées.En particulier :
\[XI_n-A = P\left(\begin{array}{cccccc}
1 &&&&&\\
& \ddots&&&&\\
&&1&&&\\
&&&P_1&&\\
&&&&\ddots&\\
&&&&&P_r
\end{array}
\right)Q \]
pour certaines matrices $P,Q \in \GL_n(\Kk[X])$. En prenant le déterminant, on trouve :
\[\chi_A(X) = \det P (P_1...P_r) \det Q\]
\[\implies \chi_A(X) = cP_1...P_r\]
où $c :=\det P \det Q \in \Kk^*$. Comme $\chi_A(X)$ et $P_1...P_r$ sont unitaires, $c=1$.

En particulier, $\deg\chi_A(X) = n = \deg P_1+...+\deg P_r$. Donc
 dans la matrice $\left(\begin{array}{cccccc}
1 &&&&&\\
& \ddots&&&&\\
&&1&&&\\
&&&P_1&&\\
&&&&\ddots&\\
&&&&&P_r
\end{array}
\right)$, le nombre de \og$1$\fg\ sur la diagonal est :
\[n-r = (\deg P_1-1) +...+(\deg P_r-1).\]
On en déduit que :
\[\left(\begin{array}{c|c|c}
C_{P_1}&&\\\hline
&\ddots&\\\hline
&&C_{P_r}
\end{array}\right) \in \mathcal{M}_n(\Kk)\]
et que :

[[ICI TABLEAU]]

%\[\left(\begin{array}{c|c|c}
%C_{P_1}&&\\\hline
%&\ddots&\\\hline
%&&C_{P_r}
%\end{array}\right) \sim \left(\begin{array}{c|c|c}
%{\left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%1\ar@{-}[rrdd]&&&\\
%&&&\\
%&&1&\\
%&&&P_1
%}
%}\right)}&&\\\hline
%&\ddots&\\\hline
%&&{\left(\raisebox{0.5\depth}{%
%\xymatrixrowsep{1ex}%
%\xymatrixcolsep{1ex}%
%\xymatrix{
%1\ar@{-}[rrdd]&&&\\
%&&&\\
%&&1&\\
%&&&P_r
%}
%}\right)}
%\end{array}\right)\]

\[\sim \left(\begin{array}{cccccc}
1 &&&&&\\
& \ddots&&&&\\
&&1&&&\\
&&&P_1&&\\
&&&&\ddots&\\
&&&&&P_r
\end{array}
\right) \sim XI_n-A .\]

On applique alors le lemme \ref{lem:eqsem} aux matrices $A$ et $\left(\begin{array}{c|c|c}
C_{P_1}&&\\\hline
&\ddots&\\\hline
&&C_{P_r}
\end{array}\right)$. 

Il reste à montrer que $P_r$ est le polynôme minimal de $A$. Il suffit de vérifier que $P_r$ est le polynôme minimal de $\mathcal{C} := \left(\begin{array}{c|c|c}
C_{P_1}&&\\\hline
&\ddots&\\\hline
&&C_{P_r}
\end{array}\right)$. Or, un polynôme $p(X)$ annule $\mathcal{C}$ si et seulement si $p(C_{P_1})=...=p(C_{P_r}) =0$ c'est-à-dire $P_i | p(X)$ pour tout $i$ (car $C_{P_i}$ a pour polynôme minimal $P_i$). Donc :
\[p(\mathcal{C}) =0 \iff P_r | p(X).\]
Ainsi, $\mathcal{C}$, et donc $A$, ont pour polynôme minimal $P_r$. 
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Endomorphismes cycliques}
Soit $E$ un $\Kk-$espace vectoriel de dimension finie.


Soit $u$ un endomorphisme de $E$. 

Les facteurs invariants $P_1,...,P_r$ de la matrice de $u$ dans une base de $E$ ne dépendent pas de la base choisie ; on les appellera les facteurs invariants de $u$.

Pour tout $x$ de $E$, on note :
\[E_x:=\langle x, u(x),...,u^k(x),...\rangle .\]

{\it Remarque : } $E_x = \{P(u)(x) \mid P(X) \in \Kk[X]\}$. 

Le sous-espace $E_x$ est stable par $u$. De plus si $P(X) \in \Kk[X]$ est un polynôme annulateur de $u$ (par exemple le polynôme minimal de $u$), alors $E_x$ est engendré par les vecteurs $x, ...,u^{d-1}(x)$ où $d$ est le degré de $P(X)$ \exoo . 

\begin{definition}
On dit que $u$ est un endomorphisme cyclique\index{endomorphisme cyclique} s'il existe $x \in E$ tel que $E_x =E$.

\end{definition}

\begin{proposition}
Soit $u$ un endomorphisme de $E$ de polynôme minimal $m_u(X)$.

L'endomorphisme $u$ est  cyclique  si et seulement si $\deg m_u = \dim E$ (c'est-à-dire $m_u(X) =\chi_u(X)$ le polynôme caractéristique de $u$.
\end{proposition}

\begin{proof}
Soit $d:=\deg m_u(X)$. Soit $n:=\dim E$. Si $E_x =E$, alors il existe $k \le d$ tel que $x,...,u^{k-1}(x)$ forment une base de $E_x$. On a donc :
\[k = \dim E_x = \dim E = n \le d\]
Donc $\deg \chi_u(X) \le \deg m_u(X)$. Or, $m_u(X)$ divise $\chi_u(X)$ et les deux sont unitaires donc : $\chi_u = m_u$.

Pour la réciproque on utilise les facteurs invariants $P_1,...,P_r$ de $u$. On a $P_1 | ... | P_r$, $P_1...P_r = \chi_u(X)$ et $P_r = m_u(X)$. Si $m_u(X) = \chi_u(X)$, alors $r=1$ et il existe une base $e_1,...,e_n$ de $E$ où la matrice de $u$ est une matrice compagnon : $C_{P_1}$.

On a alors :
\[u(e_i) = e_{i+1}\]
si $1\le i\le n$. Donc :
\[E = \langle e_1,...,e_n\rangle = \langle e_1 ,..., u^{n-1}(e_1) \rangle\]
\[= E_{e_1}\]
et $u$ est cyclique.

\end{proof}

{\it Remarque :} si $u$ est cyclique et si $E=E_x$, alors pour tout $P \in \Kk[X]$, on a :
\[P(u)(x) = 0 \iff P(u) = 0 \iff m_u(X) | P(X) .\]
En effet, si $P(u)(x) =0$, alors $P(u)(u^k(x)) = u^k(P(u)(x)) = 0$ pour tout $k$, donc  $P(u)$ est nul sur $E_x=E$.

En particulier, si $E_x=E$, alors : $x, u(x),...,u^{n-1}(x)$ est une base de $E$.

Voici une version du théorème \ref{thm:pr} pour les endomorphismes :
\begin{theoreme}\label{thm:prend}
Si $u$ est un endomorphisme de $E$, alors il existe une suite de sous-espaces stables de $E$ : $E_1,...,E_r$ tels que :

i) $E = E_1 \oplus ...\oplus E_r$ ;

ii) pour tout $1 \le i \le r$, la restriction $u_i := u\Res{E_i}$ est un endomorphisme cyclique ;

iii) si pour tout $i$, $P_i$ est le polynôme minimal de $u_i$, alors $P_1 | ...|P_r$.

De plus la suite $P_1,...,P_r$ ne dépend pas de la décomposition choisie. Ce soont les facteurs invariants de $u$.
\end{theoreme}

{\it Remarque :} il existe une base $e_1,...,e_n$ de $E$ où la matrice de $u$ est de la forme :
\[\left(\begin{array}{c|c|c}
C_{P_1}&&\\\hline
&\ddots&\\\hline
&&C_{P_r}
\end{array}\right)\]
c'est la réduction de Frobenius.


\begin{theoreme}[Frobenius]
Soit $u$ un endomorphisme de $E$. Notons $P_1,...,P_r$ ses facteurs invariants. On note :
\[\mathcal{C}(u):= \{v \in \mathcal{L}(E) \mid uv=vu\}\]
l'espace des endomorphismes qui commutent à $u$. Alors :
\[\dim \mathcal{C}(u)=(2r-1)d_1+(2r-3)d_2+...+d_r\]
où $d_i := \deg P_i$ pour tout $i$.
\end{theoreme}

\begin{proof}
Soit $E =E_1 \oplus ... \oplus E_r$ une décomposition comme dans le théorème \ref{thm:prend}. On n ote $u_i:=u\Res{E_i}$. Pour tout $x \in E$ on pose $f_j(x$ la composante de $f(x) $ dans $E_j$ :
\[f(x) = f_1(x) +... +f_r(x)\]
avec $f_1 j(x) \in E_j$ pour tout $j$. Alors $f_j : E \to E_j$, $x \mapsto f_j(x)$ est linéaire. Pour tous  $1 \le i,j \le r$, on pose :
\[f_{i,j}:=f_j\Res{E_i} : E_i \to E_j .\]
L'application :
\[\mathcal{L}(E) \to \oplus_{1 \le i,j \le r}\mathcal{L}(E_i,E_j)\]
est un isomorphisme \exoo .

Pour tous $1 \le i,j \le r$, on pose 
\[\mathcal{C}_{i,j}:= \{F \in \mathcal{L}(E_i,E_j) \mid u_jF=Fu_i\}.\]

On a :
\[f \in \mathcal{C}(u) \iff fu=uf \iff \Qq 1 \le i\le r,\, \Qq x \in E_i,\, fu(x) uf(x)\]
\[ \Qq 1 \le i\le r,\, \Qq x \in E_i,\, f_1u(x)+...+f_ru(x) = uf_1(x)+...+uf_r(x)\]
\[\iff \Qq 1 \le i\le r,\, \Qq x \in E_i,\, \Qq 1 \le j\le r,\, f_ju(x) =uf_j(x)\]
\[\Qq 1 \le i,j \le r,\, f_ju_i = u_jf_j\Res{E_i}\]
\[\Qq 1 \le i,j \le r,\, f_{i,j}u_i = u_j f_{i,j}.\]

Donc $\mathcal{C}(u) \iso \oplus_{1 \le i,j\le r}\mathcal{C}_{i,j}$ et :
\[\dim \mathcal{C}(u)= \sum_{1 \le i,j \le r}\dim  \mathcal{C}_{i,j}.\]

Calculons $\dim\mathcal{C}_{i,j}$ : soit $x \in E_i$ tel que :
\[E_i = \langle x,...,u_i^{d_i-1}(x)\rangle = \langle x,...,u^{d_i-1}(x)\rangle \]
(en particulier, $x,...,u^{d_i-1}(x)$ est une base de $E_i$).

Soit $\Phi : \mathcal{C}_{i,j} \to E_j$, $F \mapsto F(x)$.

Alors $\Phi$ est injective :

en effet, si $F\in \mathcal{C}_{i,j}$ et si $\Phi(F) = 0$, alors $F(x)=0$ et pour tout $k \ge 0$, on a :
\[Fu_i^k(x) = u_j^kF(x) = 0\]
donc $F$ est nul sur $E_i$ c'est-à-dire $F =0$.

On a $\Im \Phi = \Ker P_i(u_j) \subset E_j$.

En effet, si $F \in \mathcal{C}_{i,j}$, alors  :
\[P_i(u_j)(F(x)) = (P_i(u_j) F) (x) \]
\[= (FP_i(u_i))(x)\] 
\[= 0.\]

Donc $F(x) \in \Ker P_i(u_j)$ pour tout $F\in \mathcal{C}_{i,j}$. ainsi : $\Im \Phi \subset \Ker P_i(u_j)$.  

Pour l'inclusion réciproque, soit $y \in \Ker P_i(u_j)$. On pose alors :
\[F(Q(u_i)(x)) := Q(u_j)(y)   \]
pour tout polynôme $Q$.
L'application $F: E_i \to E_j$ est bien définie en effet :
si $Q_1,Q_2 \in \Kk[X]$ sont des polynômes tels que $Q_1(u_i)(x) =Q_2(u_i)(x)$, alors \[(Q_1-Q_2)(u_i)(x) =0 \implies P_i | Q_1-Q_2\]
(car $P_i$ est le polynôme minimal de $u_i$)
\[\implies (Q_1-Q_2)(u_j)(y)=0\]
(car $y \in \Ker (P_i(u_j))$)
\[\implies Q_1(u_j)(y) =Q_2(u_j)(y).\]

L'application est linéaire et on a : $Fu_i(x) = u_j(y)$ et $F(x)=y$ donc :
\[Fu_i(x) = u_jF(x)\]
\[\implies Fu_i=u_jF\]
sur $E_i$.

Ainsi, $F \in \mathcal{C}_{i,j}$ et $\Phi(F)=F(x)=y$.

On a donc $\dim \mathcal{C}_{i,j}=\dim \Ker P_i(u_j)$.

Nous allons montrer que :
\[\dim \Ker P_i(u_j) =\left\{\begin{array}{l}
d_i \si i\le j\\
d_j \si i \ge j
\end{array}\right..\]

Si $i \ge j$, alors $P_j | P_i$ donc $P_i(u_j)=0$ et $\Ker P_i(u_j)= \Ker 0 =E_j$. Donc $\dim \Ker P_i(u_j) = \dim E_j = d_j$.

Si $i \le j$, alors $P_i|P_j$. Soit $Q \in \Kk[X]$ tel que $P_iQ=P_j$.

Soit $S\in \Kk[X]$. On a : $S(u_j)(x) \in E_j$ et :
\[S(u_j)(x) \in \Ker P_i(u_j) \iff P_i(u_j)S(u_j)(x)= 0\]
\[\iff P_j | P_iS\]
\[\iff P_i Q |P_iS\]
\[\iff Q |S .\]
Donc :
\[\Ker P_i(u_j) = \left\{S(u_j)(x) \mid Q |S\right\}\]
\[= \left\{ (QS_1)(u_j)(x)\mid S_1 \in\Kk[X]\right\}\]
\[= \left\langle Q(u_j)(u_j^k (x)) \mid k \ge 0\right\rangle\]
\[= \left\langle Q(u_j)(u_j^k (x)) \mid 0 \le k \le  d_i-1\right\rangle\]
\exoo .

Or les vecteurs $  Q(u_j)(u_j^k (x)) \mid 0 \le k \le  d_i-1$ sont indépendants donc $\dim \Ker P_i(u_j) = d_i$.

En conclusion, on a :
\[\dim \mathcal{C}(u) = \sum_{1 \le i , j\le r} \dim\mathcal{C}_{i,j} \]
\[= \sum_{1\le i < j \le r}d_i + \sum_{1 \le j < i \le r} d_j +\sum_{1\le i\le r} d_i\]
\[= 2 \sum_{1\le i < j \le r}d_i+\sum_{1\le i\le r} d_i\]
\[= 2\sum_{1 \le i \le r} (r-i) d_i +\sum_{1\le i\le r} d_i\]
\[= \sum_{1\le i\le r} (2r-2i+1)d_i.\]

\end{proof}

{\it Remarques :} 

--- si $u= \lambda\id_E$, alors $r=n$ et les facteurs invariants de $u$ sont $P_1=...=P_r=(X-\lambda)$ et on retrouve que :
\[\dim \mathcal{C}(u) = \sum_{1 \le i \le r }(2n-2i-1)\]
\[= 2n-1 + 2n -3 + ... + 1 \]
\[= n^2 = \dim \mathcal{L}(E).\]
D'où $\mathcal{C}(u) = \mathcal{L}(E)$.

--- si $u$ est cyclique, alors : $r=1$ et $P_1=\chi_u(X)$ donc :\[\dim \mathcal{C}(u) = n.\]
 
On en déduit dans ce cas que :
\[\mathcal{C}(u) = \Kk[u]\]
\[= \{P(u)\mid P \in \Kk[X]\}\]

\[= \langle \id_E,u,...,u^{n-1} \rangle.\]


\end{document}

\begin{proposition}
Toute permutation est un produit de transpositions.
\end{proposition}

\begin{proof}
Par récurrence descendante sur le nombre de points fixes de $\sigma \in \mathcal{S}_n$ (un point fixe $k$ de $\sigma$ est un entier $1 \le k \le n$ tel que $\sigma (k)=k$). Si $\sigma$ a $n$ points fixes, $\sigma$ est l'identité qui est produit de \og zéro \fg\ transposition. Supposons maintenant que $\sigma$ a $k$ points fixes, $k < n$. Il existe $i \neq j$ tels que $\sigma (i) =j$. Soit $\tau$ la transposition qui échange $i$ et $j$ (et laisse fixe tous les autres entiers). Alors :
\[\tau \sigma (i) = i \text{ et } \tau\sigma(k) = k\]
pour tout point fixe $k$ de $\sigma$. Donc $\tau \sigma$ a au moins un point fixe de plus que $\sigma$. Par hypothèse de récurrence, il existe des transpositions $\tau_1,...,\tau_N$ telles que : 
\[\tau \sigma = \tau_1...\tau_N\]
Or, $\tau ^{-1} = \tau$ donc $\sigma = \tau \tau_1...\tau_N$ est un produit de transpositions.
\end{proof}

\begin{definition}[Nombres d'inversion, signature]

Soit $\sigma \in \mathcal{S}_n$. Une {\it inversion}\index{inversion (d'une permutation)} de $\sigma$ est une paire $\{i,j\}$\infra{Rappelons que $\{i,j\} = \{j,i\}$.} telle que $1 \le i\neq j \le n$ et si $i<j$ $\sigma(i)> \sigma(j)$ (et si $i >j$, $\sigma(i) < \sigma(j)$) (\og $\sigma$ inverse l'ordre de la paire $\{i,j\}$ \fg\ ).

On note $I(\sigma)$\index{$I(\sigma)$, ensemble des inversions d'une permutation $\sigma$} l'ensemble des inversions de $\sigma$.

La {\it signature}\index{signature} de $\sigma$ est $\epsilon(\sigma) := 1$ si $I(\sigma)$\index{$\epsilon(\sigma)$, signature d'une permutation $\sigma$} a un nombre pair d'éléments et $\epsilon(\sigma)=-1
$ si $I(\sigma)$ a un nombre impair d'éléments.
\end{definition}

\begin{exemple}[s]

--- $\id$ a $0$ inversion, $0$ est pair, donc $\epsilon(\id) = 1$.

--- Si $\tau$ est la transposition :
\[\left\{ \begin{array}{l}
i \mapsto j\\
j \mapsto i\\
k \mapsto k \mbox{ si } k \neq i,j
\end{array}\right.
\] avec  $i < j$, alors \[I(\tau) = \{(i,j), (k,j), (i,l) \mid i < k < j, i <l < j\}\]
ce qui fait $2(j-i-1) +1$ éléments (impair) donc $\epsilon(\tau) = -1$.
\end{exemple}

%\newpage
\begin{exemple}
Voici les inversions et les signatures des $6$ permutations $\sigma \in \mathcal{S}_3$ :

\[\begin{array}{|c|c|c|} 
\hline
\sigma & I(\sigma)\atop \# I(\sigma) & \epsilon (\sigma)\\
\hline
\id : \left\{\raisebox{0.5\depth}{%
\xymatrixcolsep{3ex}%
       \xymatrixrowsep{1ex}% 
\xymatrix{
1 \ar@{|->}[r] & 1\\
2 \ar@{|->}[r]& 2\\
3 \ar@{|->}[r] & 3
}
}\right.&\varnothing \atop 0& 1\\
\hline

s_1 : \left\{\raisebox{0.5\depth}{%
\xymatrixcolsep{3ex}%
       \xymatrixrowsep{1ex}% 
\xymatrix{
1 \ar@{|->}[rd] & 1\\
2 \ar@{|->}[ru]& 2\\
3 \ar@{|->}[r] & 3
}
}\right.&\{\{1,2\}\}\atop 1& -1 \\
\hline
s_2 : \left\{\raisebox{0.5\depth}{%
\xymatrixcolsep{3ex}%
       \xymatrixrowsep{1ex}% 
\xymatrix{
1 \ar@{|->}[r] & 1\\
2 \ar@{|->}[rd]& 2\\
3 \ar@{|->}[ru] & 3
}
}\right.& \{\{2,3\}\} \atop 1 & -1 \\
\hline
s_1s_2 : \left\{\raisebox{0.5\depth}{%
\xymatrixcolsep{3ex}%
       \xymatrixrowsep{1ex}% 
\xymatrix{
1 \ar@{|->}[rd] & 1\\
2 \ar@{|->}[rd]& 2\\
3 \ar@{|->}[ruu] & 3
}
}\right.&\{\{2,3\},\{1,3\}\}\atop 2&1\\
\hline
s_2s_1 : \left\{\raisebox{0.5\depth}{%
\xymatrixcolsep{3ex}%
       \xymatrixrowsep{1ex}% 
\xymatrix{
1 \ar@{|->}[rdd] & 1\\
2 \ar@{|->}[ru]& 2\\
3 \ar@{|->}[ru] & 3
}
}\right.&\{\{1,2\},\{1,3\}\} \atop 2&1\\
\hline
{s_1s_2s_1 \atop (= s_2s_1s_2) }: \left\{\raisebox{0.5\depth}{%
\xymatrixcolsep{4ex}%
       \xymatrixrowsep{1ex}% 
\xymatrix{
1 \ar@{|->}@/_/[rdd] & 1\\
2 \ar@{|->}[r]& 2\\
3 \ar@{|->}[ruu] & 3
}
}\right.&\{\{1,2\},\{2,3\},\{1,3\}\} \atop 3& -1 \\
\hline
\end{array}\]

{\it Le nombre d'inversions d'une permutation est aussi le nombre de \og croisements \fg\ dans le diagramme qui la représente}.
\end{exemple}

\begin{definition}
Soit $A=(a_{i,j})_{1\le i,j\le n}$ une matrice. On note :
\[\det A := |A| := \sum_{\sigma \in \mathcal{S}_n}\epsilon(\sigma) a_{\sigma(1),1}....a_{\sigma(n),n}\] le déterminant de $A$.
\end{definition}

\begin{exercicecours}
Pour $n=2,3$ on retrouve la définition usuelle.
\end{exercicecours}


\begin{proposition}[déterminant d'une matrice triangulaire]
Soit $T = (t_{i,j})_{1 \le i,j \le n}$ une matrice triangulaire supérieure c'est-à-dire $t_{i,j}=0$ si $i >j$. Alors :

\[\left|\raisebox{0.5\depth}{%
       \xymatrixcolsep{1ex}%
       \xymatrixrowsep{1ex}%
       \xymatrix{
t_{1,1}\ar@{.}[rrr]\ar@{.}[rrrddd] & && t_{1,n}\ar@{.}[ddd]\\
0\ar@{.}[dd]\ar@{.}[rrdd]&&&\\
& &&\\
0 \ar@{.}[rr]&& 0& t_{n,n}}}\right| 
 = t_{1,1} ... t_{n,n}\]
le produit des coefficients diagonaux. En particulier, \[|I_n| = 1\]. 
\end{proposition}

\begin{proof}
Par définition :
\[|T| = \sum_{\sigma \in \mathcal{S}_n} \epsilon(\sigma)t_{\sigma(1),1}...t_{\sigma(n),n}\]
or, le produit $t_{\sigma(1),1}...t_{\sigma(n),n}$ est nul sauf si, éventuellement, $\sigma(1) \le 1 ,...,\sigma(n) \le n$. Cela n'arrive que si $\sigma(1) = 1,...,\sigma(n) = n$ c'est-à-dire si $\sigma = \id$. Donc :

\[|T| = \epsilon(\id)t_{1,1}...t_{n,n}= t_{1,1}...t_{n,n} .\]
\end{proof}

\begin{proposition}[déterminant de la transposée]
\[\Qq A \in \mathcal{M}_n(\Kk), \, |{}^tA| = |A| .\]
\end{proposition}

\begin{proof}
Soit $A=(a_{i,j})_{1 \le i,j \le n}$. 

\[|{}^tA| = \sum_{\sigma \in \mathcal{S}_n}\epsilon(\sigma)a_{1,\sigma(1)}...a_{n,\sigma(n)}\]

puisque le produit dans $\Kk$ est commutatif :
\[a_{1,\sigma(1)}...a_{n,\sigma(n)} = a_{\sigma^{-1}(1),1}...a_{\sigma^{-1}(n),n} .\]

Or, on a une bijection :
\[\mathcal{S}_n \stackrel{1:1}{\to} \mathcal{S}_n\]
\[\sigma \mapsto \sigma^{-1}\]
donc :
\[ | {}^t A | = \sum_{\sigma \in \mathcal{S}_n}\epsilon(\sigma^{-1}) a_{\sigma(1),1}...a_{\sigma(n),n} \]
Or, \[\epsilon( \sigma^{-1}) = \epsilon (\sigma)\]
(en fait, $\sigma^{-1} $ et $\sigma$ ont le même nombre d'inversions car l'application :
\[I(\sigma^{-1}) \to I(\sigma)\]
\[\{i,j\} \mapsto \{\sigma^{-1}(i),\sigma^{-1}(j)\}\]
est bijective \exoo.)

donc : $|{}^tA| = |A|$.


\end{proof}

\begin{proposition}

Le déterminant est $n-$linéaire alterné en les colonnes (et en les lignes).

\end{proposition}


{\bf traduction : }

--- \og $n-$linéaire en les colonnes \fg\ signifie que si les colonnes \[\left(\begin{array}{c}
a_{1,1}\\
\vdots\\
a_{n,1}
\end{array}\right),...,\left(\begin{array}{c}
a_{1,j-1}\\
\vdots\\
a_{n,j-1}
\end{array}\right),\left(\begin{array}{c}
a_{1,j+1}\\
\vdots\\
a_{n,j+1}
\end{array}\right),...,\left(\begin{array}{c}
a_{1,n}\\
\vdots\\
a_{n,n}
\end{array}\right)\]

sont fixées, alors l'application :


\[\left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array}\right) \mapsto \bordermatrix[{||}]{&&&j\atop|&&\cr
&a_{1,1}& ... & x_1&...&a_{1,n}\cr
&\vdots & &\vdots & &\vdots\cr
&a_{n,1}&...& x_n& ...& a_{n,n}\cr}
\]
est linéaire.

--- \og alternée en les colonnes \fg\ signifie que si la matrice $A$ a deux colonnes identiques, alors $|A| = 0$.

\begin{proof}

Pour le caractère $n-$linéaire :

 il suffit de remarquer que (pour tout $1 \le j \le n$) :
\[
\left|\begin{array}{ccccc}
a_{1,1} & ...& x_1&...&a_{1,n}\\
\vdots & &\vdots & & \vdots\\
a_{n,1} & ...& x_n &... & a_{n,n}
\end{array}\right| = \sum_{\sigma \in \mathcal{S}_n} (\epsilon(\sigma)a_{\sigma(1),1} ...a_{\sigma(j-1),j-1} a_{\sigma(j+1),j+1}...a_{\sigma(n),n}) x_{\sigma(j)}
\]
est une expression linéaire en les $x_k$, $1 \le k \le n$.

\vskip 1cm

Pour le caractère alterné :

Supposons que les colonnes $i$ et $j$ d'une  matrice $A$ sont égales, $1 \le i < j \le n$ (c'est-à-dire $\Qq 1 \le k \le n,\, a_{k,i}=a_{k,j}$) . Soit $\tau$ la transposition qui envoie $i$ sur $j$ (et $j$ sur $i$  et qui laisse fixe les $1 \le k\neq i,j\le n$).

\[|A| = \sum_{\sigma \mid \epsilon(\sigma) = 1} a_{\sigma(1),1}...a_{\sigma(n),n} - \sum_{\sigma \mid \epsilon(\sigma) = -1} a_{\sigma(1),1}...a_{\sigma(n),n}\]

Or on a :

\begin{lemme}
\[\epsilon( \sigma \tau) = - \epsilon (\sigma)\]
si $\tau$ est une transposition et $\sigma$ une permutation
 \end{lemme}

Admettons ...

On a donc une bijection :
\[\left\{\sigma \in \mathcal{S}_n \mid \epsilon(\sigma) = 1 \right\} \sta{1:1}{\to } \left\{\sigma \in \mathcal{S}_n \mid \epsilon(\sigma) = - 1 \right\}\]
\[\sigma \mapsto \sigma \tau\]
d'où :
\[|A| = \sum_{\sigma\in \mathcal{S}_n \atop \epsilon(\sigma) = 1} a_{\sigma(1),1}...a_{\sigma(n),n} - a_{\sigma\tau(1),1}...a_{\sigma\tau(n),n}\]
or :
\[a_{\sigma\tau(1),1}...a_{\sigma\tau(n),n} = a_{\sigma(1),\tau(1)}...a_{\sigma(n),\tau(n)}\]
(en réordonnant les termes)
\[= a_{\sigma(1),1}...a_{\sigma(n),n}\]
car les colonnes $i$ et $j$ sont égales.

Donc $|A|=0.$

\begin{proof}[du lemme]


Soient $I(\sigma)$ l'ensemble des inversions de $\sigma$. On a une bijection :

\[\left(I( \sigma\tau) \setminus I(\sigma) \right) \cup  \left(I(\sigma) \setminus I(\sigma\tau) \right)\stackrel{1:1}{\longleftrightarrow} I(\sigma \tau \sigma^{-1})\]

\[\{i,j\} \mapsto \{\sigma(i),\sigma(j)\}\]
\exoo.

Or, $\sigma\tau \sigma^{-1}$ est une transposition : c'est la transposition qui échange $\sigma(i)$ et $\sigma(j)$. Donc $|I(\sigma\tau\sigma^{-1})|$ est impair. 
Or :

\[|I(\sigma)| = |I(\sigma) \cap I(\sigma\tau)| + |I(\sigma) \setminus I(\sigma\tau)|\]
\[|I(\sigma\tau)| = |I(\sigma) \cap I(\sigma\tau)| + |I(\sigma\tau) \setminus I(\sigma)|\]
\[\implies |I(\sigma)| + |I(\sigma\tau)| = 2 |I(\sigma) \cap I(\sigma\tau)| + |I(\sigma) \setminus I(\sigma\tau)| + |I(\sigma\tau) \setminus I(\sigma)|\]
\[\implies |I(\sigma)| + |I(\sigma\tau)| = 2 |I(\sigma) \cap I(\sigma\tau)|  + |I(\sigma\tau\sigma^{-1})|\]
et $|I(\sigma)| + |I(\sigma\tau)|$ est impair. Donc $\epsilon(\sigma) = - \epsilon(\sigma\tau)$.
\end{proof}

\begin{corollaire}[du lemme]
Si $\sigma \in \mathcal{S}_n$ est un produit de $N$ transpositions :
\[\sigma = \tau_1...\tau_N \;,\]
alors $\epsilon(\sigma) = (-1)^N$. En conséquence, on a :
\[\Qq \sigma,\sigma' \in \mathcal{S}_n,\, \epsilon (\sigma \sigma') = \epsilon(\sigma)\epsilon(\sigma') .\]
\end{corollaire}

\end{proof}

{\bf Remarque importante :} Comme 
\[\Qq A \in \mathcal{M}_n(\Kk), \, |{}^tA| = |A| \]
le déterminant est aussi $n-$linéaire alterné en les lignes.

%\newpage

{\bf Caractérisation du déterminant} : le théorème qui suit dit que le déterminant est l'unique fonction telle que ...

\begin{theoreme}
Soit $F : \mathcal{M}_n(\Kk) \to \Kk$ une application $n-$linéaire alternée en les colonnes. Alors :

\[\Qq A \in \mathcal{M}_n(\Kk), \, F(A) = \det A F(I_n) .\]

\end{theoreme}

\begin{remarque*}
Même énoncé si on remplace les colonnes par des lignes.
\end{remarque*}

\begin{proof}
Soit $A=(a_{i,j})_{1 \le i,j \le n} \in \mathcal{M}_n(\Kk)$. Notons $E_i$ le vecteur colonne :
\[\left(\begin{array}{c}
0 \\
\vdots\\
1\\
\vdots\\
0
\end{array}\right)\]
($1$ en position $i$, $0$ ailleurs).

On utilise la $n-$linéarité et on développe : 
\[A = \left(\begin{array}{c|c|c}
\sum_{i=1}^n a_{i,1}E_i & ... & \sum_{i=1}^na_{i,n}E_i
\end{array}\right)\]
donc :
\[F(A) = \sum_{1 \le i_1,...,i_n \le n} a_{i_1,1}...a_{i_n,n}F(E_{i_1}|...|E_{i_n}) .\]
 Gr\^ace au caractère alterné, tous les termes :

\[F(E_{i_1}|...|E_{i_n})\]
sont nuls sauf éventuellement si les indices $i_1,...,i_n$ sont $2$ à $2$ distincts c'est-à-dire si \[(i_1,...,i_n) = (\sigma(1),...,\sigma(n))\]
pour une certaine permutation $\sigma \in \mathcal{S}_n$.

Donc :
\[F(A) = \sum_{\sigma \in \mathcal{S}_n} a_{\sigma(1),1}...a_{\sigma(n),n}F(E_{\sigma(1)}|...|E_{\sigma(n)}) .\]

Or, \[ F(E_{\sigma(1)}|...|E_{\sigma(n)} ) = \epsilon(\sigma) F(E_1|...|E_n) = F(I_n)\]

en effet, supposons d'abord que $\sigma$ est la transposition qui échange $i$ et $j$, $1 \le i < j \le n$ fixés. Alors, dans ce cas :
\[F(E_{\sigma(1)}|...|E_{\sigma(n)} ) = F(E_1|...|E_j|...|E_i|...|E_n)\]
Or, $F$ est alternée donc :
\[F(E_1|...|E_{i}+E_j|...|E_{i}+E_j|...|E_n) = 0\]
or, $F$ est $n-$linéaire donc :
\[0 = F(E_1|...|E_{i}+E_j|...|E_{i}+E_j|...|E_n)\]
\[= \underbrace{F(E_1|...|E_{i}|...|E_{i}|...|E_n)}_{=0} +   F(E_1|...|E_i|...|E_j|...|E_n) \]\[+  F(E_1|...|E_j|...|E_i|...|E_n) + \underbrace{ F(E_1|...|E_j|...|E_j|...|E_n)}_{=0}\]
\[=   F(E_1|...|E_i|...|E_j|...|E_n) +  F(E_1|...|E_j|...|E_i|...|E_n)\]
d'où : \[  F(E_1|...|E_i|...|E_j|...|E_n) = - F(E_1|...|E_j|...|E_i|...|E_n) .\]
Si maintenant, $\sigma$ est quelconque, $\sigma$ est un produit transpositions :

\[\sigma = \tau_1...\tau_N\]
alors :
\[F(E_{\sigma(1)}|...|E_{\sigma(n)} ) = F(E_{\tau_1...\tau_N(1)}|...|E_{\tau_1...\tau_N(n)} )\]
\[= -  F(E_{\tau_2...\tau_N(1)}|...|E_{\tau_2...\tau_N(n)} )\]
\[...\]
\[= (-1)^N F(E_1|...|E_n)\]
\[ = \epsilon(\sigma) F(E_1|...|E_n) .\]

\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Produit Scalaire}

Soit $E$ un $\Rr-$espace vectoriel réel. 

\begin{definition}
Un produit scalaire \index{produit scalaire} sur $E$ est une application $\langle \cdot,\cdot \rangle : E \times E \to \Rr$ telle que :

\[ i) \; \Qq x,y,z \in E,\, \Qq t \in \Rr, \, \langle x+ty,z\rangle = \langle x,z\rangle + t \langle y,z\rangle \; , \; \langle x,y+tz \rangle = \langle x, y \rangle + \langle x , z \rangle \;,\]
\[ii) \; \Qq x,y \in E, \; \langle x, y \rangle = \langle y, x \rangle \;,\]
\[iii) \Qq x \in E,\, \langle x, x \rangle \ge 0 \text{ et } \langle x , x \rangle =0 \implies x =0.\]
\end{definition}

Un $\Rr-$espace vectoriel de dimension finie muni d'un produit scalaire, est un {\it espace euclidien} \index{espace euclidien}.

\begin{exemple}[standard]
L'espace $E= \Rr^n$ muni du produit scalaire suivant :\[
\Qq X=\left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array}\right),\,\Qq Y=\left(\begin{array}{c}
y_1\\
\vdots\\
y_n
\end{array}\right) \in \Rr^n\,,
\]\[ \langle X ,Y \rangle := {}^t\!XY= x_1y_1+...+x_ny_n. \]
\end{exemple}




\begin{theoreme}[Cauchy-Schwarz]
Si $\langle \cdot,\cdot\rangle$ est un produit scalaire sur $E$, alors :

\[\Qq x,y \in E\,, \langle x,y\rangle^2 \le \langle x , x \rangle\langle y,y\rangle .\]

\end{theoreme}

\begin{proof}
Soient $x,y \in E$. Alors d'après (iii) de la définition d'un produit scalaire, on a :

\[\left\langle \langle x , x\rangle y - \langle x ,y\rangle x \,,\,\langle x , x\rangle y - \langle x ,y\rangle x \right\rangle \ge 0\]
\[\iff \langle x,x \rangle^2 \langle y,y \rangle - \langle x,y \rangle^2 \langle x,x \rangle \ge 0 \]
\[\iff \langle x,x\rangle \left( \langle x,x\rangle \langle y, y\rangle -\langle x,y\rangle^2 \right)\ge 0\]
\[\implies \langle x,y\rangle^2 \le \langle x , x \rangle\langle y,y\rangle\]
si $x \neq 0$. C'est aussi vrai si $x =0$.
\end{proof}

\begin{definition}
On pose : $||x|| := \sqrt{\langle x, x\rangle}$ pour tout $x \in E$.
\end{definition}

En particulier, pour le produit scalaire standard sur $\Rr^n$, on a :
\[\Qq X =
\left(\begin{array}{c}
x_1\\
\vdots\\
x_n
\end{array}\right)\in \Rr^n\,,\, ||X|| = x_1^2+...+x_n^2\p
\]

\begin{corollaire}[Inégalité de Minkowski]
Soit $E$ un espace vectoriel réel muni d'un produit scalaire. Alors pour tous $x,y\in E$, \[||x+ y || \le ||x || + ||y|| .\]
\end{corollaire}

\begin{proof}
\[(||x || + ||y||)^2 - ||x+ y ||^2 = 2(||x||||y|| -\langle x, y\rangle) \ge 0.\]
\end{proof}

\begin{proposition}
L'application $|| \cdot || : E \to \Rr_{\ge 0}$ est une norme.
\end{proposition}
\subsection{Bases orthogonales}

Soit $(E,\langle \cdot,\cdot \rangle)$ un espace euclidien. 

\begin{definition}
Une base orthogonale \index{base orthogonale} de $E$ est une base $e_1,...,e_n$ telle que $\langle e_i,e_j\rangle = 0$ si $i\neq j$. Si de plus tous les $e_i$ sont de norme $1$ (c'est-à-dire $\Qq i,\, \langle e_i , e_i \rangle =1$), on dit que la base est orthonormale\index{orthonormale}.
\end{definition}

\begin{exemple}
La base canonique de $\Rr^n$ est une base orthonormale pour le produit scalaire standard de $\Rr^n$.
\end{exemple}

\begin{remarque*}
Si $e_1,...,e_n$ est une base orthonormale de $E$, alors $||x_1e_1 + ... +x_ne_n||^2 = x_1^2+...+x_n^2$ pour tous $x_1,...,x_n \in \Rr$.
\end{remarque*}

Nous verrons tout à l'heure que les matrices de passage d'une base orthonormale dans une autre base orthonormale sont les matrices orthogonales.

\begin{proposition}
Il existe des bases orthonormales.
\end{proposition}

\begin{proof}
En particulier, si $F$ est un sous-espace de $E$, comme la restriction du produit scalaire à $F$ :
\[\langle \cdot, \cdot \rangle : F \times F \to \Rr\]
est encore un produit scalaire, alors $F$ admet un base orthonormale.

On démontre la proposition : soit  $n:=\dim E$,
soit $e_1,...,e_n$ une base de $E$. Alors on pose :

\[e'_1 := \frac{e_1}{||e_1||}\]
\[e'_k = \frac{e_k - \sum_{i=1}^{k-1}\langle e_k,e'_i\rangle e'_i}{||e_k - \sum_{i=1}^{k-1}\langle e_k,e'_i\rangle e'_i||}\] 
si $2 \le k \le n$. (Il est facile de voir que $e'i \in \langle e_1,...,e_ile$ pour tout $i$ et donc que les $e'_k$ sont bien définis et que $\langle e'_k ,e'_l\rangle = 0$ si $k <l$).
\end{document}